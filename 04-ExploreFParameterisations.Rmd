# Exploring alternative fishing mortality parameterisations (F) {#Fexplore}

The current Alaskan sablefish stock assessment (Chapter \@ref(modeldescription)) estimates annual fishing mortality values for each gear \(g\) denoted by \(F^g_{y}\). This parametersation poses to potential problems when considering future assessment models and spatial models. The first is, the number of parameters will increase as the number of gears increase. The fishery is currently going through a transformation whereby there is a switch from longline to pots. The second consideration is how to set this up in a spatially explicit model where catch have an added spatial dimension. There are two alternative approaches to the current approach which treat \(F\) as a derived quantity rather than an estimable parameter. The first is to use Newton Raphson method to solve for \(F^g_{y}\). This is the recommended approach in Stock Synthesis [@methot2013stock], termed the "hybrid" approach. The previous two methods assume the Baranov catch equation for mortality [@baranov1918question]. An alternative is to assume Popes discrete formulation [@pope1972investigation] which uses exploitation proportions (sometimes called harvest rates or fishing pressure) and has a closed form solution.

A good general overview on these methods can be found in @branch2009differences. They describe and compared the continuous Baranov catch equation [@baranov1918question] with Pope's discrete formulation [@pope1972investigation]. Arguments for using the continuous case is that $M$ and $F$ occur simultaneously, also with the continuous case, $F$ allows for multiple event encounters, this is assuming a fleet has the same selectivity and availability, that a fish that escapes one net can be caught in another. In contrast, the discrete formulation only allows a fish to be caught or escape from an instantaneous event. I have tried to summarize the benefits of the continuous equation in the following list,

*  Allows the entire population to be caught (not sure this is that relevant)
*  Allows simultaneous $M$ and $F$, no need to worry about order of operations. From a coding/practical perspective this is quite attractive. Once you have an F and M you can easily derive all mid-mortality quantities. Where as using a \(U\) approach you need save the population before and after to interpolate to derive mid-mortality quantities.
*  The magnitude of $F$ effects composition data, where as in the discrete case, composition is independent of the magniture of $U$.
*  Allows for multiple catch events of an individual
*  Can fit to catch observations thus allows for uncertainty in catches. In practice the uncertainty/variance on catch is very small i.e., coeffecient of variations ranging from 0.01 to 0.1. This essentially states catch is observed with high confidence and in my opinion isn't that much different to saying catch is known exactly. Note often this high precision on observed catch is needed in order to make the \(F\)'s identifiable. This high precision also muddies the "degrees of freedom" for the model. Although the \(F\)'s look like independent and free parameters they are heavily constrained by the assumptiosn on observed catch variance.


The arguments for the discrete approximation is that there is an analytical solution for \(U\) and so is fast to calculate expected catch, where as $F$ has to be either solved numerically or estimated as a free parameter (as mentioned earlier). 
 
Chris Francis's wrote a response to this paper [@francisCommentBranch] where he argues the discrete formulation does not preclude the multiple encounters and that only the data can truly tell us which catch equation is the best one to use.

- Need to make a point about how there may be Automatic differentiation issues with the \(U\) approach. Because there is an `if(U > 0.99)` which can cause a fork in the chain rule which can equal a coding nightmare.

The relationship between $F$ (Instantaneous fishing mortality) and $U$ exploitation rate for a simple scenario (single fishery) is illustrated in the following R code.


```{r illustrate_F_vs_U_sim}
exploitation_rates = seq(0,0.8,by = 0.02)
## calculate F given a U
fishing_mortalites = -log(1 - exploitation_rates)
## back calculate U given a F
# 1 - exp(-fishing_mortalites) 
```



The objective of this simulation

- Is the method efficient i.e., no loss of speed.
- Is the method numerically stable (No NaNs during optimization), particularly under high fishing pressure




```{r illustrateFvsU, echo = F, eval = T, fig.fullwidth=T, fig.cap="Illustration of how mortality is applied to an age cohort continuously over time (y)."}
## the application throught time.
N_1 = 100
par(mfrow = c(2,2), mar = c(2,2,2,1), oma = c(3,2,2,0))
Fs = c(0.2,0.6,1,1.4)
for(i in 1:length(Fs)) {
  F_t = Fs[i]
  U_t = 1 - exp(-F_t)
  M = 0.5
  time_ = seq(0,1, by = 0.001)
  change_over_time = N_1 * exp(-(F_t+M)*time_)
  change_over_time_alt = N_1 * exp(-M*time_[time_<0.5]) 
  change_over_time_alt = c(change_over_time_alt, change_over_time_alt[length(change_over_time_alt)] * (1 - U_t))
  change_over_time_alt = c(change_over_time_alt, change_over_time_alt[length(change_over_time_alt)] * exp(-M*time_[time_ < 0.5]) )
  
  plot(1,1, type = "n", xlab = "", ylab = "", ylim = c(0,100), xlim = c(0,1), xaxt = "n", yaxt = "n", cex.main = 1.5,cex.lab = 1.5, main = substitute(paste(F[y], " = ", this_F, " M = ", M), list(this_F = F_t, M= M)))#paste0(, " = ", F_t))
  lines(time_, change_over_time, lwd = 4)
  lines(time_, change_over_time_alt, lwd = 4, col = "red")
  if (i > 2)
    axis(side = 1, tick = T, at = c(0,1), labels = c("y", "y+1"), cex.axis = 2)
  if (i == 1)
    legend('bottomleft', legend = c("F","U"), lwd = 3, col = c("black","red"), cex = 0.8)
}
mtext(side = 1, text = "Time", outer = T, line = 0.7, cex = 1.3)
mtext(side = 2, text = "N", outer = T, line = -1, cex = 1.3)
```



## Set up a simulation {-}
To explore the three methods described above a simple simulation was conducted for a simple age-structured stock assessment. The purpose was to check that the derived methods were reliable so that we can implement them in the spatial model that is development.


```{r setupfirstsim}
bio_params = list(
  ages = 1:20,
  L_inf = 58,
  K = 0.133,
  t0 = 0,
  M = 0.15,
  a = 2.08e-9, ## tonnes
  b = 3.5,
  m_a50 = 6.3,
  m_ato95 = 1.2,
  sigma = 0.6,
  h = 0.85,
  sigma_r = 0.6,
  R0 = 8234132,
  plus_group = 1 # 0 = No, 1 = Yes
)

other_params = list(
  s_a50 = 3.6,
  s_ato95 = 2,
  s_q = 0.2,
  f_a50 = 5,
  f_ato95 = 2,
  ssb_prop_Z = 0.5,
  survey_prop_Z = 0.5,
  survey_age_error = c(0.5, 0.4),  ## sd, rho (ignored if iid)
  fishery_age_error = c(0.5, 0.4),  ## sd, rho (ignored if iid)
  survey_bio_cv = c(0.1)
)

ages = bio_params$ages
max_age = max(bio_params$ages)
n_years = 30
years = (2020 - n_years + 1):2020
n_ages = length(ages)
## annual fishing mortality
start_F = c(rlnorm(10, log(seq(from = 0.05, to = 0.2, length = 10)), 0.1), rlnorm(10, log(0.13), 0.1), rlnorm(10, log(0.07), 0.1))
recruit_devs = log(rlnorm(n_years, -0.5 * bio_params$sigma_r * bio_params$sigma_r, bio_params$sigma_r))

length_at_age = vonbert(bio_params$ages, bio_params$K, bio_params$L_inf, bio_params$t0)
fishery_ogive = logis(bio_params$ages, other_params$f_a50, other_params$f_ato95)
survey_ogive = logis(bio_params$ages, other_params$s_a50, other_params$s_ato95)
mat_age = logis(bio_params$ages, bio_params$m_a50, bio_params$m_ato95)
weight_at_age = bio_params$a * length_at_age^bio_params$b

## observation temporal frequency
survey_year_obs = years
survey_ages = 1:20
fishery_year_obs = years
fishery_ages = 1:20
```

```{r poptmbobjects, echo = F, eval = T, results = 'hide', warning=FALSE, message=FALSE, error = FALSE}

TMB_data = list()
TMB_data$ages = ages
TMB_data$maxAgePlusGroup = bio_params$plus_group
TMB_data$years = years
TMB_data$nyears = length(TMB_data$years)
TMB_data$nages = length(TMB_data$ages)

## No ageing error
TMB_data$ageing_error_matrix = matrix(0, nrow = TMB_data$nages, ncol = TMB_data$nages)
diag(TMB_data$ageing_error_matrix) = 1;

TMB_data$survey_year_indicator = as.integer(TMB_data$years %in% survey_year_obs)
TMB_data$survey_obs = rnorm(sum(TMB_data$survey_year_indicator), 100, 4)
TMB_data$survey_cv = rep(0.05,sum(TMB_data$survey_year_indicator))
TMB_data$survey_min_age = min(survey_ages)
TMB_data$survey_max_age = max(survey_ages)
TMB_data$survey_comp_type = 0 ## 0 = prop at age, 1 = numbers at age
TMB_data$survey_age_plus_group = 1
TMB_data$survey_comp_likelihood = 1 ## 0 = MVN (can be applied to both comp_type), 1 = Multinomial, 2 = dirichlet-multinomial
TMB_data$survey_sample_time = rep(0.5,sum(TMB_data$survey_year_indicator))
TMB_data$survey_covar_structure = 1 # if survey_comp_likelihood == 1 (MVN); 1 = iid, 2 = us_covstruct, 3 = cs_covstruct Compound Symmetry, 5 = ar1
TMB_data$survey_trans_comp = matrix(5, nrow = length(TMB_data$survey_min_age:TMB_data$survey_max_age) - ifelse(TMB_data$survey_comp_likelihood == 0, 1, 0), ncol = sum(TMB_data$survey_year_indicator))

TMB_data$fishery_year_indicator = as.integer(TMB_data$years %in% fishery_year_obs)
TMB_data$fishery_min_age = min(fishery_ages)
TMB_data$fishery_max_age = max(fishery_ages)
TMB_data$fishery_comp_type = 0 ## 0 = prop at age, 1 = numbers
TMB_data$fishery_age_plus_group = 1
TMB_data$fishery_covar_structure = 1 # if survey_comp_likelihood == 1 (MVN); 1 = iid, 2 = us_covstruct, 3 = cs_covstruct Compound Symmetry, 5 = ar1
TMB_data$fishery_comp_likelihood = 1 ## 0 = MVN (can be applied to both comp_type), 1 = Multinomial, 2 = dirichlet-multinomial
TMB_data$fishery_trans_comp = matrix(5, nrow = length(TMB_data$fishery_min_age:TMB_data$fishery_max_age) - ifelse(TMB_data$fishery_comp_likelihood == 0, 1, 0), ncol = sum(TMB_data$fishery_year_indicator))

TMB_data$catches = rep(1000, n_years)# this will be overriden in the simulate() call
TMB_data$ycs_estimated = rep(1, n_years)
TMB_data$standardise_ycs = 0;

TMB_data$catchMeanLength = TMB_data$stockMeanLength = matrix(length_at_age, byrow = F, ncol = TMB_data$nyears, nrow = TMB_data$nages)
TMB_data$propMat = matrix(mat_age, byrow = F, ncol = TMB_data$nyears, nrow = TMB_data$nages)
TMB_data$natMor = bio_params$M
TMB_data$steepness = bio_params$h
TMB_data$stockRecruitmentModelCode = 2 ## BH
TMB_data$propZ_ssb = rep(other_params$ssb_prop_Z, TMB_data$nyears)
TMB_data$propZ_survey = rep(other_params$survey_prop_Z, TMB_data$nyears)
TMB_data$sel_ato95_bounds = c(0.1,20)
TMB_data$sel_a50_bounds = c(0.1,20)
TMB_data$mean_weight_a = bio_params$a
TMB_data$mean_weight_b = bio_params$b
## The same parameters as OM, to check for consistency
true_pars = list(
  ln_R0 = log(bio_params$R0),
  ln_ycs_est =  log(exp(recruit_devs[TMB_data$ycs_estimated == 1] - 0.5*bio_params$sigma_r^2)),
  ln_sigma_r = log( bio_params$sigma_r),
  ln_extra_survey_cv = log(0.0001),
  trans_survey_error = c(log(0.29), inv_bound_unit(0.55)),
  trans_fishery_error = c(log(0.38), inv_bound_unit(0.25)),
  logit_f_a50 = logit_general(other_params$f_a50, TMB_data$sel_a50_bounds[1], TMB_data$sel_a50_bounds[2]),
  logit_f_ato95 = logit_general(other_params$f_ato95, TMB_data$sel_ato95_bounds[1], TMB_data$sel_ato95_bounds[2]),
  logit_survey_a50 = logit_general(other_params$s_a50, TMB_data$sel_a50_bounds[1], TMB_data$sel_a50_bounds[2]),
  logit_survey_ato95 = logit_general(other_params$s_ato95, TMB_data$sel_ato95_bounds[1], TMB_data$sel_ato95_bounds[2]),
  logit_surveyQ = qlogis(other_params$s_q),
  ln_F = log(start_F),
  ln_catch_sd = log(0.02)
)

ran_start_vals = function(covar = "iid") {
  start_params = list()
  start_params$ln_R0 = ran_start(n = 1, LB = log(bio_params$R0 * 0.2), UB = log(bio_params$R0 * 2))
  start_params$ln_ycs_est = ran_start(n = sum(TMB_data$ycs_estimated), LB = -1, UB = 1)
  start_params$ln_sigma_r = log(ran_start(n = 1,LB = 0.2, UB = 1))
  start_params$ln_extra_survey_cv = log(0.01)
  start_params$trans_survey_error = c(log(ran_start(n = 1, LB = 0.1, UB = 0.6)))
  start_params$trans_fishery_error = c(log(ran_start(n = 1, LB = 0.1, UB = 0.6)))
  if (covar == "ar1") {
    start_params$trans_survey_error  = c(start_params$trans_survey_error, inv_bound_unit(ran_start(n = 1, LB = -0.5, UB = 0.5)))
    start_params$trans_fishery_error  = c(start_params$trans_fishery_error, inv_bound_unit(ran_start(n = 1, LB = -0.5, UB = 0.5)))
  }
  start_params$logit_f_a50 = logit_general(ran_start(n = 1, LB = 3, UB = 8),TMB_data$sel_a50_bounds[1], TMB_data$sel_a50_bounds[2])
  start_params$logit_f_ato95 = logit_general(ran_start(n = 1, LB = 3, UB = 8), TMB_data$sel_ato95_bounds[1], TMB_data$sel_ato95_bounds[2])
  start_params$logit_survey_a50 = logit_general(ran_start(n = 1, LB = 3 , UB = 8),TMB_data$sel_a50_bounds[1], TMB_data$sel_a50_bounds[2])
  start_params$logit_survey_ato95 = logit_general(ran_start(n = 1, LB = 3, UB = 8), TMB_data$sel_ato95_bounds[1], TMB_data$sel_ato95_bounds[2])
  start_params$logit_surveyQ = qlogis(ran_start(n = 1, LB = 0.04, UB = 0.3))
  start_params$ln_F = log(ran_start(n = TMB_data$nyears, LB = 0.02, UB = 0.4))
  start_params$ln_catch_sd = log(0.02)
  return(start_params)
}
##################################
### Build TMB OM with Multinomial
##################################
#dyn.unload(dynlib(file.path("TMB","SimpleAgeStructuredModel")))
compile(file.path("TMB","SimpleAgeStructuredModel.cpp"), flags = "",DLLFLAGS="");
dyn.load(dynlib(file.path("TMB","SimpleAgeStructuredModel")))
## tolerance form model convergence, all gradients need to be less than this.
grad_tol = 0.001
# these parameters we are not estimating.
na_map = fix_pars(par_list = true_pars, pars_to_exclude = c("ln_catch_sd", "trans_survey_error", "trans_fishery_error","ln_extra_survey_cv","ln_sigma_r"))
ASM_obj <- MakeADFun(TMB_data, true_pars, DLL= "SimpleAgeStructuredModel", map = na_map)
true_report = ASM_obj$report()
```

```{r self_test, echo = T, eval = T}
##################################
### Self Test with estimable F's
##################################
n_sims = 10

est_pars = NULL
ssbs = NULL
for(sim_iter in 1:n_sims) {
  if(sim_iter %% 10 == 0)
    cat("iter = ", sim_iter, "\n")
  sim_data = ASM_obj$simulate(complete = T)
  start_pars = ran_start_vals(covar = "ar1")
  est_model = MakeADFun(sim_data, start_pars, DLL= "SimpleAgeStructuredModel", map = na_map, silent = T)
  opt_modelc = nlminb(est_model$par, est_model$fn, est_model$gr, control = list(iter.max = 10000, eval.max = 10000))
  est_pars = rbind(est_pars, opt_modelc$par)
  est_rep = est_model$report(opt_modelc$par)
  ssbs = rbind(ssbs, est_rep$ssb)
}
molten_ssbs = melt(ssbs)
colnames(molten_ssbs) = c("sim", "year", "value")
ggplot() +
  geom_line(data =molten_ssbs, aes(x = year, y = value, group = sim)) +
  geom_line(data = data.frame(year = 1:31, value = true_report$ssb),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "SSB (t)") +
  theme_bw()
```


## Other things to consider {-}

## Appendix - Hybrid approach {-}
The hybrid fishing mortality process uses the methods and algorithms applied in Stock Synthesis [@methot2013stock]. The descriptions below are heavily based on the text describing this approach in the Appendix of [@methot2013stock].

This process begins by calculating popes discrete approximation, and then converts this to Baranov fishing mortality coefficients. A tuning algorithm is then done to tune these coefficients to match input catch nearly exactly, rather than the full Baranov approach.

Total mortality, denoted by \(Z_{a,y,s,r}\) for sex \(s\), region \(r\), age \(a\) and year \(y\) will hereby be denoted by \(Z_{a,y,s}\) i.e., drop the region index. This is because mortality rates are calculated independently (in isolation) among regions (**NOTE** consider parallelising this in the model).

\begin{equation*}
	Z_{a,y,s} = M_{a,y,s} + \sum\limits_{f} S^g_{y,s} F^g_y
\end{equation*}

where, \( M_{a,s}\) is the natural mortality rate, \( F^g_y\) is fishing mortality and \(S^g_{a,s}\) is the selectivity.

The hybrid fishing mortality method allows the \(F\) values to be "tuned" to match input catch nearly exactly, rather than estimating them as free model parameters. The process begins by calculating mid year exploitation rate using Pope’s approximation. This exploitation rate is then converted to an approximation of the Baranov continuous \(F\). The \(F\) values for all fisheries operating in that year and region are then tuned over a set number of iterations (`f_iterations`) to match the observed catch for each fishery with its corresponding \(F\). Differentiability is achieved by the use of Pope's approximation to obtain the starting value for each \(F\) and then the use of a fixed number of tuning iterations, typically 4. Tests from Stock Synthesis have shown that modelling \(F\) as hybrid versus \(F\) as a parameter has trivial impact on the estimates of the variances of other model derived quantities. 

The hybrid method calculates the harvest rate using the Pope's approximation then converts to an approximation of the corresponding F as:

\begin{align}
	V^g_{y} &= \sum\limits_s\sum\limits_a N_{a,y,s} \exp\left(-\delta_t M_{a,s}\right) \nonumber \\
	\tilde{U}^g_{y} &= \frac{C^g_{y}}{V^g_{y} + 0.1 C^g_{y}}\\
	j^g_{y} &= \left(1 + \exp \left(30 (\tilde{U}^g_{y} - 0.95) \right)\right)^{-1}\\
	U^g_{y} &= 	j_{t,f} \tilde{U}^g_{y} + 0.95 (1 - j^g_{y} )\\
	\tilde{F}^g_{y} &= \frac{-\log\left(1 - U^g_{y}\right)}{\delta_t}
\end{align}

where, \(C^{g}_y\) is the observed catch, \(\delta_t\) is the duration of the period of observation within the year. In most situations where the entire catch has been observed in a time-step. This should be one. \(V^g_{y}\) is partway vulnerable biomass and \(\tilde{F}^g_{y}\) is the initial \(F\).

The formulation above is designed so that high exploitation rates (above 0.95) are converted into an F that corresponds to a harvest rate of close to 0.95, thus providing a more robust starting point for subsequent iterative adjustment of this F. The logistic joiner, \(j\), is used at other places in Stock Synthesis to link across discontinuities.

The tuning algorithm begins by setting \(F^g_{y} = \tilde{F}^g_{y}\) and repeating the following algorithm `f_iteration` times.

\[
\widehat{C}_{a,y,s}  = \sum\limits_g {F}^g_{y}\left(S^g_{a,s} N_{a,y,s}\right) \lambda^*_{a,y,s}
\]

where, \( \lambda^*_{a,y,s}\) denotes the survivorship and is calculated as:

\begin{equation}
 \lambda^*_{a,y,s} = \frac{1 - \exp\left(-\delta_t Z_{a,y,s}  \right) }{Z_{a,y,s}}
   (\#eq:survival)
\end{equation}

Total fishing mortality is then adjusted over several fixed number of iterations (typically four, but more in high F and multiple fishery situations). The first step is to calculate the ratio of the total observed catch over all fleets to the predicted total catch according to the current F estimates. This ratio provides an overall adjustment factor to bring the total mortality closer to what it will be after adjusting the individual \(F\) values.

\[
\widehat{C}_{y}  =  \sum\limits_g \sum\limits_s\sum\limits_a {F}^g_{y}\left(S^g_{a,s} N_{a,y,s}\right) \lambda^*_{a,y,s}
\]

This is different from Equation~A.1.25 in the Appendix of [@methot2013stock]. They include \(Z_{a,y,s}\) in the denominator of \({F}^g_{y}\), which is a type error because \(Z_{a,y,s}\) is already included in \(\lambda^*_{a,y,s}\), see Equation~\ref{eq:survival}.

\[
Z^{adj}_t = \frac{\sum\limits_g C^g_{y}}{\widehat{C}_{y}}
\]

The total mortality if this adjuster was applied to all the Fs is then calculated:

\[
Z^*_{a,y,s} = M_{a,s} + Z^{adj}_t \left(Z_{a,y,s} -M_{a,s} \right)
\]

\[
\lambda^*_{a,y,s} = \frac{1 - \exp\left(-\delta_t Z^*_{a,y,s}  \right) }{Z^*_{a,y,s}}
\]

The adjusted mortality rate is used to calculate the total removals for each fishery, and then the new \(F\) estimate is calculated by the ratio of observed catch to total removals, with a constraint to prevent unreasonably high \(F\) calculations (`max_f`):

\begin{align*}
	\tilde{V}^g_{y} &= \sum\limits_s\sum\limits_a \left(N_{a,y,s} \bar{w}_{a,y,s}S^g_{a,s} \right)\lambda^*_{a,y,s} \\
	F^{g*}_{y} &= \frac{C^g_{y}}{\tilde{V}^g_{y} + 0.0001}\\
	j^{g*}_{y} &= \left(1 + \exp \left(30 (F^{g*}_{y} - 0.95 F_{max}) \right)\right)^{-1}\\
\end{align*}

where, \(F_{max}\) is a user defined maximum fishing mortality `f_max`. The \(F\) at the end of each tuning iteration follows: 

\[
F^g_{y} = j^{g*}_{y} F^{g*}_{y} + \left(1 - j^{g*}_{y}\right)F_{max}
\]

After the tuning algorithm removals at age, and other derived quantities are recorded. The final total mortality is updated
\[
	Z_{a,y,s} = M_{a,s} + \sum\limits_{g} S^g_{a,s} F^g_{y}
\]

This process generates catch at age and sex for each year (and region) (\(\widehat{C}^g_{a,y,s}\)) which can be accessed by `process_removals` observations. Numbers at age are calculated as

\[
\widehat{C}^g_{a,y,s} = \frac{F^g_{a,s,y}}{Z_{a,y,s}} N_{a,y,s} \exp\left(-Z_{a,y,s}\right)
\]

Total catch is the summed over all sexes and age 

\[
\widehat{C}^g_{y} = \sum\limits_s\sum\limits_a \widehat{C}^g_{a,y,s}  \bar{w}_{a,y,s}
\]

where \(\bar{w}_{a,y,s}\) is the mean weight


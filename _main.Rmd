--- 
title: "Alaskan sablefish research"
author: "C.Marsh"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
includes:
 before_body: preamble-mathjax.tex
#url: https:///r4Casal2/
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This Gitbook documents my post-doctoral research relating to Alaskan sablefish.
link-citations: yes

---

# Overview

This Gitbook documents my/our research on Alaskan sablefish (*Anoplopoma fimbria*). The objective is to develop and explore a spatially explicit stock assessment model for the Alaskan sablefish stock. However, there will be many topics that I will touch on. This book outlines my research and thinking along with some rabbit holes I entered (and hopefully returned) during my post-doctoral research. The following section outlines chapters contained in this document.


## Gitbook outline  {-}
- Chapter \@ref(objectives) outlines a list of objectives that we have set or accomplished during this research project. Some of our research cannot be presented in this document due to the confidentiality reasons. However, this checklist should include all tasks during the project

- Chapter \@ref(modeldescription) documents the current stock assessment model and assumptions (work in progress). This is the first step of the project, with the purpose of helping me understand the data and important process dynamics assumed in the current assessment. Most models used in this research is written in TMB [@kristensen2015tmb], many of these are available in the [`SpatialSablefishAssessment` R package](https://github.com/Craig44/SpatialSablefishAssessment).

- Chapter \@ref(spatialmodeldescription) documents a generalized spatial assessment model used as both an OM and EM during simulations.


- Chapter \@ref(surveydata) describes, characterizes and explores the survey longline data available for the the assessment.


- Chapter \@ref(observerdata) describes, characterizes and explores fishery dependent data, which includes reported catch (log-book data) and observer records.


- Chapter \@ref(tagdata) describes the tagging data and how it can be used within a spatial stock assessment model but also used in a panmictic model for informing population dynamics such as growth.


- Chapter \@ref(sexratios) explores how to include sex disaggregated composition observations that can include sex ratio information. It conducts a simple simulation to investigate two different approaches.


- Chapter \@ref(Fexplore) explores how to parameterise fishing mortality. The current approach is to estimate an annual fishing mortality parameter for each gear as a free parameter. I feel slightly uncomfortable about this approach moving towards a spatial model as the number of estimated parameters will explode when a spatial dimension is explored. This chapter looks at two alternative approaches that either derive fishing mortality estimates using a Newton Raphson solver which is heavily borrowed from the Stock Synthesis "hybrid" approach [@methot2013stock] and Pope's discrete approach [@pope1972investigation] which uses exploitation rates. Both these methods have been applied in the literature for decades. The aim of this chapter is to do a simulation and make sure the considered approaches are efficient and numerically stability for the purposes of our research.


- Chapter \@ref(spatialInit) explores how to calculate the plus group in spatially explicit age-structured models that assume markovian movement during initialization.







## Future things to consider {-}
 

- When is the latest year that we can start the model at? Currently the assessment model starts in 1960 because there is early survey and catch info along with some of the largest catches recorded. I want to explore starting the model at a later period i.e., the 1980's when there is consistent surveys and age data. This is thought to reduce the number of estimable parameters that have low information (early recruitment deviations). However, the downfall is we may loose information on stock production because the 60's and 70's have some of the largest recorded catches.




```{r install_packages, results = 'hide', message=FALSE, warning=FALSE}
library(TMB)
#library(stockassessmenthelper)
library(ggplot2)
library(dplyr)
library(reshape2)
library(gridExtra)
library(knitr)
library(RColorBrewer)
```

```{r auxillary_functions,echo = F,eval =T, results = 'hide', message=FALSE, warning=FALSE}
#' vonbert applies the Von Bertalanffy age-length relationship
#' @param age take an age look in globe scope for the rest of parameters
#' @param L_inf asympototic length
#' @param K growth rate parameter
#' @param t0 age for length = 0
#' @export
#' @return mean length at age
vonbert <- function(age,K,L_inf,t0) {
  return(L_inf * (1-exp(-K*(age -t0))))
}
#'
#' richards_growth provides the mean length at a give age
#' @param age true age
#' @param p is the shape parameter
#' @param k is the growth coefficient
#' @param l_inf is the asymptotic length
#' @param t_0 corresponds to an inflexion point on the curve.
#' @return mean length at age
#'
richards_growth = function(age, p, k, t_0, l_inf) {
  mean_length = l_inf * (1 + 1 / p * exp(-k * (age - t_0)))^-p
}
#' create a function for simulating starting values for model robustness
#' @param n <integer> number of draws you want
#' @param dist <string> distribution to use to draw starting values, allowed "unif", "norm" 
#' @param LB <scalar> Lower bound of starting value, shouldn't use lower bound of parameter
#' @param UB <scalar> Upper Bound of starting balues.
#' @param dist_pars <vector> has distributional parameters, i.e for norm = c(mu, sigma)
#' @return vector of starting values
ran_start = function(n = 100, dist = "unif", LB = -Inf, UB = Inf, dist_pars = NULL) {
  if (!any(dist %in% c("unif", "norm")))
    stop("parameter 'dist', needs to be either unif or norm")
  start_vals = vector();
  if (dist == "unif") {
    start_vals = runif(n, LB, UB)
  } else if (dist == "norm") {
    start_vals = rnorm(n, dist_pars[1],dist_pars[2])
    start_vals[start_vals < LB] = LB
    start_vals[start_vals > UB] = UB
    
  } else {
    stop("something went wrong")
  }
  return(start_vals)
}

#' Logistic selectivity
#' @export
logis<- function(X,a50,a95)
{
  1/(1+19^((a50-X)/a95)) 
}

#' bound_unit constrains Y from -inf -> inf to be between -1 -> 1 
#' @param Y scalar range [-inf, inf]
#' @return X to be between [-1,1] 
bound_unit = function(Y) {
  return(Y / sqrt(1.0 + Y * Y))
}

#' inv_bound_unit constrains Y from -inf -> inf to be between -1 -> 1 
#' @param X scalar range [-1,1] 
#' @return Y to be between [-inf, inf]
inv_bound_unit = function(X) {
  return(sqrt((X*X) / (1 - X*X)) * ifelse(X < 0, -1, 1))
}
#' logit bounds X which is between 0-1 to -inf -> inf based on the logit transformation
#' equivalent to qlogis(X)
#' @param X scalar range [0,1]
#' @return Y to be between [-inf, inf]
logit = function(X) {
  log(X / (1 - X)) 
}
#' invlogit Inverse logit transformation, equivalent to plogis(Y)
#' @param Y scalar between [-inf, inf] 
#' @return X between [0,1]
invlogit<- function(Y) {
  1/(1 + exp(-Y))
}
#' logit_general bounds X which is between [lb,ub] to -inf -> inf based on the logit transformation
#' @param X scalar range [lb,ub]
#' @param ub upper bound for X
#' @param lb lower bound for X
#' @return Y to be between [-inf, inf]
logit_general = function(X, lb, ub) {
  X1 = (X - lb) / (ub - lb)
  log(X1/(1 - X1))
}


simplex <- function (xk, sum_to_one = TRUE)  {
  zk = vector()
  if (!sum_to_one) {
    xk = xk/sum(xk)
  }
  else {
    if (abs(sum(xk) - 1) > 0.001) 
      stop("xk needs to sum = 1, otherwise speify sum_to_one = TRUE")
  }
  K = length(xk)
  zk[1] = xk[1]/(1)
  for (k in 2:(K - 1)) {
    zk[k] = xk[k]/(1 - sum(xk[1:(k - 1)]))
  }
  yk = stats::qlogis(zk) - log(1/(K - 1:(K - 1)))
  return(yk)
}
restoresimplex <- function (yk) {
  K = length(yk) + 1
  zk = stats::plogis(yk + log(1/(K - 1:(K - 1))))
  xk = vector()
  xk[1] = zk[1]
  for (k in 2:(K - 1)) {
    xk[k] = (1 - sum(xk[1:(k - 1)])) * zk[k]
  }
  xk[K] = 1 - sum(xk)
  return(xk)
}


#' invlogit_general bounds X which is between -inf -> inf to [lb,ub] based on the logit transformation
#' @param Y scalar range [-inf, inf]
#' @param ub upper bound for X
#' @param lb lower bound for X
#' @return X to be between [lb,ub]
invlogit_general = function(Y, lb, ub) {
  Y1 = 1 / (1 + exp(-Y))
  lb + (ub - lb)*Y1
}
#' fix_pars 
#' @author C.Marsh
#' @description TMB helper function this function returns a list of factors used in the map argument of the MakeADFun function
#' values with <NA> will not be estimated.
#' @param par_list a named list that you give to the par argument in the MakeADFun
#' @param pars_to_exclude a vector of strings with names of parameters you want to FIX in the objective object.
#' @param vec_elements_to_exclude a named list (names %in% pars_to_exclude) with number of elements = length(vec_pars_to_adjust). each list element 
#' @param array_elements_to_exclude a named list (names %in% pars_to_exclude) with a matrix each row corresponds to an element with the first column being the array row index and second column being the array column index to fix

#' contains a vector of elements that we want to exclude from estimation.
#' @return a list of factors used in the MakeADFun function
#' @export
fix_pars <- function(par_list, pars_to_exclude, vec_elements_to_exclude = NULL, array_elements_to_exclude = NULL) {
  if (!any(pars_to_exclude %in% names(par_list))) {
    stop(paste0("The parameters ", paste(pars_to_exclude[!pars_to_exclude %in% names(par_list)],collapse = " ")," in exclusion parameters could not be found in the 'par_list', please sort this out"))
  }
  pars = names(par_list)
  mapped_pars = list();
  if (!is.null(vec_elements_to_exclude)) {
    if (!all(names(vec_elements_to_exclude) %in% pars_to_exclude))
      stop("parameters names in vec_elements_to_exclude, need to also be in pars_to_exclude")
  }
  if (!is.null(array_elements_to_exclude)) {
    if (!all(names(array_elements_to_exclude) %in% pars_to_exclude))
      stop("parameters names in array_elements_to_exclude, need to also be in pars_to_exclude")
  }
  param_factor = 1;
  for(i in 1:length(pars)) {
    if (pars[i] %in% pars_to_exclude) {
      params_in_this_par = par_list[[pars[i]]];
      if (pars[i] %in% names(vec_elements_to_exclude)) {
        include_element_index = c(1:length(params_in_this_par))[-vec_elements_to_exclude[[pars[i]]]]
        params_vals = factor(rep(NA, length(params_in_this_par)), levels = factor(param_factor:(param_factor + length(include_element_index) - 1)))
        params_vals[include_element_index] = factor(param_factor:(param_factor + length(include_element_index) - 1))#, levels = factor(include_element_index))
        param_factor = param_factor + length(include_element_index)
        mapped_pars[[pars[i]]] = params_vals;
      } else if(pars[i] %in% names(array_elements_to_exclude)) {
        elements_to_drop = array_elements_to_exclude[[pars[i]]]
        mapped_vector = rep(NA, length(params_in_this_par))
        first_param_factor = param_factor
        vec_ndx = 1;
        ## TMB converts arrays to vectors down columns (not by rows)
        for(col_ndx in 1:ncol(params_in_this_par)) {
          for(row_ndx in 1:nrow(params_in_this_par)) {
            ## check if we need to drop this value
            for(drop_ndx in 1:nrow(elements_to_drop)) {
              if(!((row_ndx == elements_to_drop[drop_ndx, 1]) &  (col_ndx == elements_to_drop[drop_ndx, 2]))) {
                mapped_vector[vec_ndx] = param_factor
                param_factor = param_factor + 1
              }
            }
            vec_ndx = vec_ndx + 1;
          }
        }
        mapped_vector = factor(mapped_vector, levels = first_param_factor:max(mapped_vector, na.rm = T))
        mapped_pars[[pars[i]]] = mapped_vector;
      } else {
        ## exclude entire parameters
        mapped_pars[[pars[i]]] = rep(factor(NA),length(params_in_this_par));
        n_params_to_exclude = nrow(vec_elements_to_exclude[[pars[i]]])
      }
    } else {
      params_in_this_par = par_list[[pars[i]]];
      params_vals = factor(param_factor:(param_factor + length(params_in_this_par) - 1))
      param_factor = param_factor + length(params_in_this_par)
      mapped_pars[[pars[i]]] = params_vals
    }
  }
  return(mapped_pars);
}


#' set_pars_to_be_the_same 
#' @author C.Marsh
#' @description TMB helper function this function returns a list of factors used in the map argument of the MakeADFun function
#' values with the same factor level will be estimated as the same value
#' @details TMB will estimate parameters based on the index specified in by the map argument in MakeADFun
#' so parameters with the same factor in map will be estimated as the same value.
#' NOTE: this only works for within the same parameter. It doesn't work across parameters.
#' @param par_list a named list that you give to the par argument in the MakeADFun
#' @param map a list of factors that has been created by fix_pars(). parameters that you want fixed to other values should be set to <NA> in this object
#' @param base_parameters a named list (names) each element contains one index that will be used to set the value in copy_parameters
#' @param copy_parameters a named list (names) each element contains one index that will be set equal to the corresponding base_parameters
#' @return a list of factors used in the MakeADFun function
#' @export
set_pars_to_be_the_same <- function(par_list, map, base_parameters, copy_parameters) {
  if(length(base_parameters) != length(copy_parameters))
    stop("the number of elements in base_parameters must be the same as copy_parameters. Please check these")
  if(!class(map) == "list")
    stop("map needs to be a list")
  if(!any(names(base_parameters) %in% names(par_list)))
    stop(!paste0("The parameters in base_parameters ", paste(names(base_parameters)[!names(base_parameters) %in% names(par_list)],collapse = " ")," could not be found in the 'par_list', please sort this out"))
  if(!any(names(copy_parameters) %in% names(par_list)))
    stop(!paste0("The parameters in copy_parameters ", paste(names(copy_parameters)[!names(copy_parameters) %in% names(par_list)],collapse = " ")," could not be found in the 'par_list', please sort this out"))
  pars = names(par_list)
  
  for(i in 1:length(base_parameters)) {
    if(is.na(map[[names(base_parameters)[i]]][base_parameters[[i]]]))
      stop(paste0("In base_parameters for parameter ", names(base_parameters)[i], " at ndx ", base_parameters[[i]], ". We found an NA. This cannot be, please check"))
    if(!is.na(map[[names(copy_parameters)[i]]][copy_parameters[[i]]]))
      stop(paste0("In copy_parameters for parameter ", names(base_parameters)[i], " at ndx ", base_parameters[[i]], ". Was not an NA. This must be an NA value in 'map', please check"))

    temp_copy_parm = map[[names(copy_parameters)[i]]]
    temp_copy_parm = as.numeric(as.character(temp_copy_parm))
    base_value = as.numeric(as.character(map[[names(base_parameters)[i]]][base_parameters[[i]]]))
    temp_copy_parm[copy_parameters[[i]]] = base_value
    lvls = unique(temp_copy_parm[!is.na(temp_copy_parm)])
    map[[names(copy_parameters)[i]]] = factor(temp_copy_parm, levels = lvls)
  }
  
  return(map);
}

#' get_list_obj gets names objects out of simulated TMB reports.
#' @param est_ls a list of length n_sims
#' @param object_label a charachter that points to a name of an element of the TMB report
#' @return a matrix 

get_list_obj = function(est_ls, object_label) {
  val = Reduce(rbind, lapply(X = est_ls, FUN = function(x){
    temp = x[[object_label]]
    temp
    }))
  elements = nrow(val) / length(est_ls)
  
  cbind(sort(rep(1:length(est_ls), elements)), val)
}

```

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# A list of objectives/milestones that we have set along the project life {#objectives}


- [x]  Translate current stock assessment (Chapter \@ref(modeldescription)) from ADMB to TMB. Planned date of completion is December 1 2022
- [ ]  ~~Conduct self test using TMB stock assessment model. Planned date of completion is December 1 2022~~ I decided not to do this because the assessment had too many bespoke likelihoods
- [x]  Consider improvements i.e., sex disaggregated composition data or sex ratio observations (look at the rock lobster assessment) including age-length observations or tag-increment observations to estimate growth internally. (Chapter \@ref(sexratios))
- [x]  Characterize both fishery and survey data to get an idea of data limitations when considering spatially explicit stock assessment model.
- [x]  Develop a spatially explicit estimation model in TMB that generalizes the current assessment model. This requires a lot of thought, especially how we want to integrate the tagging data (Chapter \@ref(tagdata)). Self test has been complete without tagging data.
- [x]  Generate observation error values for composition data. either boostrap or use SE methods
- [ ]  ~~Calculate mean length at age for each year - perhaps use the growth model, but this may be double using some of the datasets? This is an interesting idea cause if we take the tag data for example it would be used to estimate mean length at age just using increment and time at liberty, where as when it is internally used in the model it would be using age-frequency to estimate ontogenetic movement.~~ I decided to default to the current assessment length at age and mean length assumptions
- [ ]  Outline key model decisions based on data exploration (Chapter \@ref(InitialModelSetup))
- [ ]  Display initial model runs
- [ ]  Consider descibe data-weighting methods



<!--chapter:end:01-Objectives.Rmd-->

# Current Alaskan sablefish stock assessment model {#modeldescription}
The latest published stock assessment [@goethel2021assessment] is a single area model sexually disaggregated integrated age-structured model. Let \(\boldsymbol{N_{y,s}}\) denote a vector of ages in year \(y\) for sex \(s\) (the partition) i.e., \(\boldsymbol{N_{y,s}} = (N_{1,y,s}, N_{2,y,s}, \dots, N_{a_+,y,s})^T\). The general process model is sequential and follows the general Equation \@ref(eq:processmodel),

\begin{equation} 
  \boldsymbol{N_{y,s}} = 
  \begin{cases}
  g\left(\boldsymbol{\theta}\right), & y = 1959 \text{ Initial model year}\\
  f\left(\boldsymbol{N_{y-1,s}}|\boldsymbol{\theta}\right), & y > 1959 \\
  \end{cases}
  (\#eq:processmodel)
\end{equation} 
where, \(g(.)\) is the function describing initial conditions for the partition and \(f(.)\) is the function that applies populations dynamics each year i.e., birth, death, growth and migration. See later sections for a detailed description of \(g(.)\) and \(f(.)\) and \(\boldsymbol{\theta}\) is the set of estimable (not all are estimated) parameters.


Maximum Likelihood Estimates (MLE) for estimated parameters \(\widehat{\boldsymbol{\theta}}_{MLE}\) are evaluated,
\begin{equation}
	\widehat{\boldsymbol{\theta}}_{MLE} = \underset{\boldsymbol{\theta}}{\arg\max} \left( L\left(\boldsymbol{\theta} | \boldsymbol{y^{obs}}\right) \right)
	  (\#eq:observationmodel)
\end{equation}
where, \(\boldsymbol{y^{obs}}\) is a set of observations and \(L\left( . \right)\) is an objective function that is made up of priors/penalties and densities. All the models in this research estimate \(\widehat{\boldsymbol{\theta}}_{MLE}\) by minimising the negative log of \(ll\left( . \right) = -\log(L\left( . \right))\). See the observation section for descriptions of \(ll\left( . \right)\).


All symbols used in the following equations are defined in the table at the end of this chapter.

## Process equations {-}

### Initialisation \left(\(g\left(.\right)\)\right) {-}
\begin{align*}
N_{a,1,s} = 
\begin{cases}
R_1, & a = a_1\\
\exp\bigg( \mu_r + \tau_{a_1 - a + 1}\bigg) \exp-\bigg( a - a_1\bigg) \bigg( M + F_{hist} * \mu_{LL} * S^{LL}_{a,1}\bigg), & a_0 < a < a_+\\
\exp( \mu_r) \exp-( a - 1) ( M + F_{hist} * \mu_{LL} * S^{LL}_{a - 1,1})(1 - \exp( M + F_{hist} * \mu_{LL} * S^{LL}_{a - 1,1}))^{-1}, &  a = a_+
\end{cases}
\end{align*}

### Population dynamics \left(\(f\left(.\right)\)\right) {-}
The assessment assumes a closed population that is only effected by mortality (natural and fishing), recruitment and growth. Mortality is applied assuming 

\[
Z_{a,y,s} = M + \sum_g F^g_{y} S^g_{a,y,s}
\]
where, \(S^g_{a,y,s}\) is the fishery selectivity and \(F^g_{y}\) is the annual estimated fishing mortality. The annual cycle follows,


\begin{align*}
N_{a,y,s} = 
\begin{cases}
p^s_{y} R_y, & a = a_1\\
N_{a - 1,y - 1,s} \times 
\exp\bigg( -Z_{a - 1,y - 1,s} \bigg), & a_0 < a < a_+\\
\exp\bigg( -Z_{a - 1,y - 1,s} \bigg) + \exp\bigg( -Z_{a,y - 1,s} \bigg), &  a = a_+
\end{cases}
\end{align*}
where,
\[
R_y = \exp\{\mu_r + \tau_y + 0.5\sigma_R^2\}
\]
and \(p^s_{y}\) is the proportion of recruits in year \(y\) for sex \(s\).


## Observation equations (\(ll\left( . \right)\)) {-}
Three are three observation types used in the current Sablefish stock assessment
- Relative abundance indices
- Age composition (aggregated over sex)
- Length composition (disaggregated by sex)

These three observation types come from both fishery dependent i.e., observer programs and fishery independent i.e., research surveys.


### Catch at age {-}
Fishery dependent catch at age observations for gear type \(g\) denoted by \({C^g}_{a,y,s}\) are calculated as follows

\begin{equation} 
  {C^g}_{a,y,s} = \frac{F^g_{a,y,s}}{Z_{a,y,s}}   N_{a,y,s} \left(1 - S_{a,y,s} \right)
  (\#eq:catchatage)
\end{equation} 
Currently all age observations are sex aggregated which means the model expected values before applying ageing error is
\[
  {C^g}_{a,y} = 0.5 \sum_s \frac{{C^g}_{a,y,s}}{\sum_a{C^g}_{a,y,s}}
\]
why the 0.5? should be omitted going forward. Ageing error is then incorporated and the values are normalized so that they are proportions, before being passed to the multinomial log-likelihood function.



Survey age composition is similar but instead of being a function of \(F\) it is calculated at the beginning of the year. For survey \(k\) the numbers at age are denoted by \({C^k}_{a,y}\) and calculated following
\begin{equation} 
  {C^k}_{a,y} = \sum_s p^s N_{a,y,s} S^k_{y,a,s}
  (\#eq:surveyage)
\end{equation} 

I am not sure exactly what the timing of these surveys are, but do we need to account for some mid-year mortality? or changes in timing of the survey? if so we could easily replace 
\[
N_{a,y,s}
\]
with
\[
N_{a,y,s} \left(1 - \exp\{-p^k_y Z_{a,y,s}\} \right)
\]
where,\(p^k_y\) is the proportion of mortality that we want to account for in year \(y\) for survey \(k\).

The survey numbers at age \({C^k}_{a,y}\) are then adjusted for ageing error and normalised so they sum to one for each year.


### Relative abundance indices {-}
\begin{equation} 
  \widehat{I}^g_{y} = \sum_s\sum_a p^s N_{a,y,s} \exp \{-0.5 Z_{a,y,s}\} S^g_{y,a,s} \bar{w}_{a,y,s}
  (\#eq:relativeindex)
\end{equation} 
where, \(\bar{w}_{a,y,s}\) is mean weight at age, this can be omitted if the observation is in numbers i.e., abundance instead of biomass and \(p^s\) is the proportion for each sex. This is currently a user input, but should be dealt within the model either as having different sex selectivities or through the sex ratio of recruitment.

A list of slight improvements

- change how sex ratio is handled in age and length composition (Chapter \@ref(sexratios))
- fishery dependent abundance indices i.e., CPUE change \(N_{a,y,s} \exp \{-0.5 Z_{a,y,s}\}\) with \(\boldsymbol{C^g}_{a,y,s}\) which is calculated in the catch at age observations.

### Catch at length {-}

For each year that has a length frequency observation, numbers at length denoted by \(\boldsymbol{C^l}_{y,s} = (C^l_{1,y,s}, \dots, C^l_{n_l,y,s})^T\) (dimension of \(\boldsymbol{C^l}_{y,s}\) is \(n_l \ \times \ 1\)) were calculated for each sex. This involved multiplying the catch at age (see above for how it is calculated) through a sex specific age-length transition matrix denoted by \(\boldsymbol{A}^l_{y,s}\) (dimensions of \(\boldsymbol{A}^l_{y,s}\) are \(n_a \ \times \ n_l\) and its rows must sum to 1). The calculation follows Equation \@ref(eq:agelengthtransition),

\begin{equation} 
  \boldsymbol{C^l}_{y,s} =  \left(\boldsymbol{A}^l_{y,s} \right)^T \ \times \ \boldsymbol{C_{y,s}}
  (\#eq:agelengthtransition)
\end{equation} 
where, \(\boldsymbol{C_{y,s}}\) is a column vector of numbers at age (dimension \(n_a \ \times \ 1\)) at the beginning of year \(y\) for sex \(s\).



<!-- ### Determining selectivities for fisheries and surveys {-} -->
<!-- The ADMB model has a hard coded number of selectivities. Some of them relate to changes in the fishery and so represent time-varying blocks. We want to spell these out and simplify for the TMB model. There are nine selectivities labelled `fish1`, `fish2`, `fish3`, `fish4`, `fish5`, `srv1`, `srv2` and `srv10`. -->


<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | label                      | Selectivity description                             -->
<!-- +============================+=============================================================================+ -->
<!-- | `fish1`                    | fixed gear selectivity from 1960-1994 -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `fish2`                    | Not sure if this is used, maybe this is used for `srv6`? -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `fish3`                    | Trawl selectivity from 1960 - \(T\) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `fish4`                    | fixed gear selectivity from 1995 -\( \ IFQ_y\) (\(IFQ_y\) can = \(T\) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `fish5`                    | fixed gear selectivity from \(IFQ_y \ - \ T\) if there is post IFQ block -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `srv1`                     | Domestic Longline survey selectivity -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `srv2`                     | Japanese Longline survey selectivity -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->

<!-- ### Observations for fisheries and surveys {-} -->

<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | label                      | Relative Abundance description                             -->
<!-- +============================+=============================================================================+ -->
<!-- | `srv1`                     | Biomass domestic longline survey uses both `srv1_sel` and `srv10_sel` -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `srv3`                     | Abundance domestic longline survey uses both `srv1_sel` and `srv10_sel` -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `srv2`                     | Biomass survey uses both `srv2_sel` and `srv9_sel` -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `srv4`                     | Abundance survey uses both `srv2_sel` and `srv9_sel` -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `srv5`                     | fixed gear fishery CPUE `fish1_sel`, `fish4_sel`, `fish5_sel` -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `srv6`                     | Japanese LL fishery CPUE -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `srv7`                     | NMFS bottom trawl survey (currently GOA only; fit in model) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->



<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | label                      | Composition description                             -->
<!-- +============================+=============================================================================+ -->
<!-- | `ac_fish1`                 | fixed gear Fishery Age Comp (sex aggregated) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `sc_fish1`                 | fixed gear Fishery LF (sex dis-aggregrated) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `sc_fish3`                 | Trawl Fishery LF (sex disaggregrated) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `sc_fish2`                 | LF for japaneses Longline fishery (sex aggregrated) (basically a survey now) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `sc_fish4`                 | LF for japaneses Longline fishery (sex aggregrated) (basically a survey now) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `fish_size`                | LF From japaneses trawl survey? (sex aggregrated) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `ac_srv1`                  | Domestic Longline Survey AF (sex aggregated) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `sc_srv1`                  | Domestic Longline Survey LF (sex disaggregrated) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `ac_srv2`                  | Japanese Longline Survey AF (sex aggregated) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `sc_srv2`                  | Japanese Longline Survey LF (sex disaggregrated) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `ac_srv7`                  | NMFS bottom trawl survey AF (sex aggregated) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | `sc_srv7`                  | NMFS bottom trawl survey LF (sex disaggregrated) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->

## Symbol Notation {-}
+----------------------------+-----------------------------------------------------------------------------+
| Symbol                     | Description                            
+============================+=============================================================================+
| \(y\)                      | Year, \(y = 1960, \dots, T\) 
+----------------------------+-----------------------------------------------------------------------------+
| \(T\)                      | Terminal year of the model
+----------------------------+-----------------------------------------------------------------------------+
| \(s\)                      | Sex index \(s \in \{1,2\}\)
+----------------------------+-----------------------------------------------------------------------------+
| \(a\)                      | Model age cohort, i.e., \(a = a_0, a_0 + 1, \dots\)
+----------------------------+-----------------------------------------------------------------------------+
| \(a_{1}\)                  | Recruitment age to the model = 2 
+----------------------------+-----------------------------------------------------------------------------+
| \(a_+\)                    | Plus-group age class (oldest age considered plus all older ages)
+----------------------------+-----------------------------------------------------------------------------+
| \(n_a\)                    | Number of age classes modeled \(a_+ \ - a_1\)
+----------------------------+-----------------------------------------------------------------------------+
| \(l\)                      | length class
+----------------------------+-----------------------------------------------------------------------------+
| \(n_l\)                    | Number of length classes
+----------------------------+-----------------------------------------------------------------------------+
| \(g\)                      | gear type index, i.e. longline survey, fixed gear fishery, trawl fishery
+----------------------------+-----------------------------------------------------------------------------+
| \(x\)                      | log-likelihoos index
+----------------------------+-----------------------------------------------------------------------------+
| \(\bar{w}_{a,y, s}\)       | Average weight at age \(a\), year \(y\) and sex \(s\)
+----------------------------+-----------------------------------------------------------------------------+
| \(\phi_{a,y}\)             | Proportion of female mature by age and year
+----------------------------+-----------------------------------------------------------------------------+
| \(p^s_{y}\)                | Proportion of recruits for sex \(s\). Often assumed = 0.5
+----------------------------+-----------------------------------------------------------------------------+
| \(\ln \mu_{r}\)            | Average log-recruitment
+----------------------------+-----------------------------------------------------------------------------+
| \(\ln \mu_{f}\)            | Average log-fishing mortality
+----------------------------+-----------------------------------------------------------------------------+
| \(\phi_{y,g}\)             | annual fishing mortality deviation by gear (log space)
+----------------------------+-----------------------------------------------------------------------------+
| \(\tau_{y}\)               | annual recruitment deviation \(\sim LogNormal\left(0,\sigma_r\right)\)
+----------------------------+-----------------------------------------------------------------------------+
| \(\sigma_r\)               | Recruitment standard deviation
+----------------------------+-----------------------------------------------------------------------------+
| \(N_{a,y,s}\)              | Numbers of fish at age \(a\) in year \(y\) of sex \(s\)
+----------------------------+-----------------------------------------------------------------------------+
| \(M\)                      | Natural mortality
+----------------------------+-----------------------------------------------------------------------------+
| \(F^g_{a,y}\)              | Fishing mortality for year \(y\), age \(a\) and gear \(g\)
+----------------------------+-----------------------------------------------------------------------------+
| \(F_{hist}\)               | Historical proportion of Fishing mortality 
+----------------------------+-----------------------------------------------------------------------------+
| \(Z_{a,y}\)                | Total mortality for year \(y\), age \(a\) \(=\sum\limits_g F^g_{a,y} + M\)
+----------------------------+-----------------------------------------------------------------------------+
| \(R_{y}\)                  | Annual recruitment
+----------------------------+-----------------------------------------------------------------------------+
| \(B_{y}\)                  | Spawning biomass in year \(y\)
+----------------------------+-----------------------------------------------------------------------------+
| \(S^g_{a,y,s}\)            | Selectivity at age \(a\) for gear type \(g\) and sex \(s\)
+----------------------------+-----------------------------------------------------------------------------+
| \(a_50\)                   | age at 50\% selection for ascending limb 
+----------------------------+-----------------------------------------------------------------------------+
| \(d_50\)                   | age at 50\% selection for descending limb 
+----------------------------+-----------------------------------------------------------------------------+
| \(\delta\)                 | slope/shape parameters for different logistic curves
+----------------------------+-----------------------------------------------------------------------------+
| \(\boldsymbol{A}\)         | ageing-error matrix dimensions \(n_a \ \times \ n_a\)
+----------------------------+-----------------------------------------------------------------------------+
| \(\boldsymbol{A}^l_s\)     | age to length conversion matrix by sex. dimensions  \(n_a \ \times \ n_l\)
+----------------------------+-----------------------------------------------------------------------------+
| \(q_g\)                    | abundance index catchability coeffecient by gear
+----------------------------+-----------------------------------------------------------------------------+
| \(\lambda_x\)              | Statistical weight (penalty) for component \(x\)
+----------------------------+-----------------------------------------------------------------------------+
| \(P^g_{l,y,s}\)            | Observed proportions at length for gear \(g\) in year \(y\) and sex \(s\)
+----------------------------+-----------------------------------------------------------------------------+
| \(P^g_{a,y,s}\)            | Observed proportions at age for gear \(g\) in year \(y\) and sex \(s\)
+----------------------------+-----------------------------------------------------------------------------+


### Inference {-}
If random effects are considered the joint probability model follows,
	\begin{equation}
	Pr\left[ \boldsymbol{y^{obs}}, \boldsymbol{u}| \boldsymbol{\theta} \right]  = Pr\left[\boldsymbol{y^{obs}} |\boldsymbol{\theta^f}, \boldsymbol{u} \right] Pr\left[\boldsymbol{u} |\boldsymbol{\theta^h} \right] 
	\end{equation}

where, \(\boldsymbol{\theta}\) denotes "fixed-effect" parameters which are furthur seperated by \(\boldsymbol{\theta} = (\boldsymbol{\theta^f},\boldsymbol{\theta^h}) \), with \(\boldsymbol{\theta^h}\) denoting fixed effects that are hyperprior parameters for the "random-effect" variables denoted by \(\boldsymbol{u}\) and \(\boldsymbol{\theta^f}\) are all other fixed-effect parameters.

Inference is conducted by maximising the marginal likelihood noting \( L\left(\boldsymbol{\theta} | \boldsymbol{y^{obs}} \right) \propto Pr\left[ \boldsymbol{y^{obs}} | \boldsymbol{\theta} \right]\)
	\begin{equation}\label{eq:marginal_ll}
	L\left(\boldsymbol{\theta} | \boldsymbol{y^{obs}}\right) = \int \left(Pr\left[\boldsymbol{y^{obs}} |\boldsymbol{\theta^f}, \boldsymbol{\theta^g}, \boldsymbol{u} \right] Pr\left[\boldsymbol{u} |\boldsymbol{\theta^h} \right] \right) \boldsymbol{du}
	\end{equation}
In general this integral is not tractable, and so approximations are necessary. The software used here implement the Laplace approximation, which relies on Gaussian assumptions.

Maximum Likelihood Estimates (MLE) for fixed effect parameters \(\widehat{\boldsymbol{\theta}}_{MLE}\) are evaluated,
\begin{equation}
	\widehat{\boldsymbol{\theta}}_{MLE} = \underset{\boldsymbol{\theta}}{\arg\max} \left( L\left(\boldsymbol{\theta} | \boldsymbol{y^{obs}}\right) \right)
\end{equation}

and Empirical Bayes estimates are evaluated for \(\widehat{\boldsymbol{u}}\), which are used model diagnostics and other model quantities,
\begin{equation}
\widehat{\boldsymbol{u}} = \underset{\boldsymbol{u}}{\arg\max} \left( Pr\left[ \boldsymbol{y^{obs}}, \boldsymbol{u}| \widehat{\boldsymbol{\theta}}_{MLE} \right] \right)
\end{equation}


### TODO {-}
- Build a validate function to help catch users setting up parameters or data structures that will cause a crash once supplied to TMB.
- Self test
- Change array column casting from `vector<Type>(array.col(i))` to `array.col(i).vec()`

<!--chapter:end:02-CurrentAssessmentModel.Rmd-->

# Spatial stock assessment model for Alaskan sablefish {#spatialmodeldescription}
The spatial model used for this research is available as an R package and is best described by the R package documentation [found here](https://craig44.github.io/SpatialSablefishAssessment/TagIntegratedEq.html). I have described the general model, but the package documentation is the best resource for specific equations.


## Process equations {-}
### Population dynamics {-}
The order of processes in an annual cycle follow

1. Recruitment and release of tagged fish (we apply initial tag induced mortality here)
2. Total mortality and ageing
3. Markovian movement
4. Annual tag shedding (applied as a mortality process)


Before applying movement, the partition is updated following 
\begin{align*}
N_{a,r,y,s} = 
\begin{cases}
R_{r,y} 0.5, & a = a_1\\
N_{a - 1,r,y - 1,s} \exp\bigg( -Z_{a - 1,r, y - 1,s} \bigg), & a_1 < a < a_+\\
N_{a - 1,r,y - 1,s} \exp\bigg( -Z_{a - 1,r, y - 1,s} \bigg) + N_{a,r,y - 1,s} \exp\bigg( -Z_{a,r, y - 1,s} \bigg), &  a = a_+
\end{cases}
\end{align*}
where, \(N_{a,r,y,s}\) is the numbers at age \(a\) in region \(r\), year \(y\) for sex \(s\), \(Z_{a,r,y,s} = M + \sum_g F_{a,r,y,s}^g\) is total mortality and \(R_{r,y}\) is annual recruitment for region \(r\).

Once ageing and mortality have taken place movement is then applied as

\begin{equation*}
\boldsymbol{N}'_{a,y,s} = \boldsymbol{N}_{a,y,s} \boldsymbol{M} \ \ \forall \ a
\end{equation*}

where, \(\boldsymbol{N}'_{a,y,s} = (N'_{a,1,y,s}, \dots, N'_{a,n_r,y,s})\) denotes the numbers for age \(a\) across all regions after movement and \(\boldsymbol{M}\) is an \(n_r \times n_r\) movement matrix, which will move age cohort \(a\) among the regions based on the movement matrix.


### Initialisation {-}
An equilibrium age structure is derived following the method described in Chapter \@ref(spatialInit), but for completeness we will briefly describe it here. The annual cycle is run \(n_a - 1\) times to populate all age cohorts prior to the plus group. Then, iterate the annual cycle one more time and calculate the number of individuals that moved into each regions plus age cohort, denoted by \(c^r_{a+}\). This will be the result of ageing, mortality and movement. The equilibrium plus group for region \(r\) is then calculated as

\[
N_{a+, r} = N_{a+ - 1, r} \frac{1}{1 - c^r_{a+}}  \  .
\]

After the equilibrium age-structure is calculated, there is an option to estimate age specific deviations to allow the model to start with a non-equilibrium age-structure denoted by \(e^{\epsilon_a}\)

\[
N_{a, r} = N_{a, r} e^{\epsilon_a} \ \ \forall \ r \ a \in [a_2..(a_+ - 1)] .
\]
To help with estimation there is a penalty on \(\epsilon_a\) that assumes a central tendancy of zero with an estimable variance parameter (\(\sigma_{\epsilon}^2\)). 
\[
\epsilon_a \sim \mathcal{N}(0, \sigma_{\epsilon})
\]

Future model generalizations should consider making this inital age-deviations regional specific.


### Growth {-}

Empirical age-length matrices are supplied for all years by sex, these are the same matricies used in the current single area sex. We did not consider spatially varying growth because when the data was visually inspected, there did not seem to be obvious differences. The other reason was when spatially varying growth is included in the model, we would need to track length in the partition (add an extra dimension) to avoid fish drastically changing length as they move between areas and different growth curves.

Mean weight at age was calculated using an allometric length weight relationship with time and space invariant parameters \(\alpha\) and \(\beta\)

\[
\bar{w}_{a,y} = \alpha \bar{l}_{a,y}^{\beta} 
\]

### Recruitment {-}
There are two options in the model for recruitment, regional mean recruitment with global recruitment deviations and regional mean recruitment with region recruitment deviations.


TODO
- Add regional stock recruitment relationships as a model option
- Add a global stock recruitment relationship

### Fishing mortality {-}
When the hybrid \(F\) method is assumed (this is my default), tagged fish were not included when internally solving the fishing mortality nuisance parameters (Chapter \@ref(Fexplore)). The ratio of tagged to untagged numbers of fish in the partition in any year was assumed to be small enough not to effect \(F\) estimates. However, tagged fish were included when the model calculates predicted catch-at-age/length and catch for a fleet. This decision was made to reduce the computational overhead this would require to implement. When \(F\) parameters are estimated as free parameters, then this is not a problem and \(F\) will be applied to both tagged and untagged fish.


### Tag release events {-}

Tags release event denoted by the index \(k\) have an implied region \(r\) and year \(y\) dimension. Each tag release event has known sex and age frequency at release. This is derived using the survey age-length key. This is reasonable given the survey is responsible for releasing most tags. A downside of using the age-length key approach to convert unsexed lengths at release into tag releases by age and sex is a tagged fish that would have an exact age, sex and length will be represented in the model as a fraction of a fish across multiple ages and sexes. For release years where there were no age length keys, we used age-length keys pooled over years 1981, 1985 and 1987 for earlier perios (prior to 1981) and the closest age-length key for later periods. (**How can we explore uncertainty in this input?** re-run the model with numbers at age and length from bootstrapping age-length keys?)


Tagged fish from release event \(k\) are denoted in the partition as \(T^k_{a,r,y,s}\), and are tracked for \(n_{T}\) years before migrating into a pooled tag group, at which point we loose release-year information but do maintain its region of release. At present, tagged fish are assumed to take on the exact same population processes as the untagged elements of the partition (instantaneous mixing).


Other considerations for tag-releases

- Do we want to include the inshore (Chatham Strait and Clarence Strait) tag-releases? there are alot of fish tagged in these regions. 
- Do we care about the amount of tags that leave stock area? See Figure \@ref(fig:TagRecoveriesOutsideRegion)


## Observation equations {-}
There are four observation types in the model

- Relative indices of abundance
- Age composition disaggregated by sex for the fixed gear fishery and longline survey
- Length composition disaggregated by sex for the trawl and early period of the fixed gear fishery 
- Tag-recovery observations 


### Catch at age {-}
Fishery dependent catch at age observations are available for the fixed gear fishery, but are also needed to calculate catch at length observations for the trawl fishery. Catch at age for fishery \(g\) is denoted by \({C}^g_{a,r,y,s}\) and model fitted values are calculated following

\begin{equation} 
  {C}^g_{a,r,y,s} = \frac{F^g_{a,r,y,s}}{Z_{a,r,y,s}}   N_{a,r,y,s} \left(1 - S_{a,r,y,s} \right)
\end{equation} 

Observed values were proportions with respect to age and sex, final model fitted proportions were
\[
{P}^g_{a,r,y,s} = \frac{{C}^g_{a,r,y,s}}{\sum_a \sum_s {C}^g_{a,r,y,s}},
\]
and initially the multinomial likelihood was assumed

\[
\boldsymbol{X}^g_{r,y} \sim \text{Multinomial}\left(\boldsymbol{\widehat{P}}^g_{r,y}\right)
\]
where, \(\boldsymbol{X}^g_{r,y} = \boldsymbol{P}^g_{r,y}N^{eff}_{r,y}\) and \(\boldsymbol{P}^g_{r,y}\) is the observed proportions, \(N^{eff}_{r,y}\) is the effective sample size and \(\boldsymbol{P}^g_{r,y} = (P^g_{1,r,y,1}, \dots, P^g_{a_+,r,y,1}, P^g_{1,r,y,2}, \dots, P^g_{a_+,r,y,2})\) is the vector of observed proportions across all ages and sexs in year \(y\) and region \(r\), and \(\boldsymbol{\widehat{P}}^g_{r,y}\) is the model fitted values which have the same dimension (\(\sum_a \sum_s \widehat{P}^g_{a,r,y,s} = 1\)).


### Catch at length {-}
Catch at length observations are available for the trawl fishery. Model fitted values are derived by multiplying the catch at age (see above) by an age-length transition matrix denoted by \(\boldsymbol{A}^l_{y,s}\) (dimensions of \(\boldsymbol{A}^l_{y,s}\) are \(n_a \ \times \ n_l\) and its rows must sum to 1),

\begin{equation} 
  \widehat{\boldsymbol{Cl}}^g_{r,y,s} =  \left(\boldsymbol{A}^l_{y,s} \right)^T \ \times \ \widehat{\boldsymbol{C}}^g_{r,y,s}
\end{equation} 
where, \(\widehat{\boldsymbol{C}}^g_{r,y,s}\) is a column vector of catch at age (dimension \(n_a \ \times \ 1\)) at the beginning of year \(y\) in region \(r\) for sex \(s\), and \(\widehat{\boldsymbol{Cl}}^g_{r,y,s} \) is a column vector of catch at length (dimension \(n_l \ \times \ 1\)).


### Proportions at age {-}
Survey age composition data denoted by \({N}^s_{a,r,y,s}\) is available for the longline survey where model fitted numbers are derived as,

\begin{equation} 
  \widehat{N}^s_{a,r,y,s} = N_{a,r,y,s} \left(1 - \exp^{\delta_y Z_{a,r,y,s}}\right)S^s_{y,r,a,s}
\end{equation} 
where, \(\delta_y \in (0,1)\) is the proportion of time in the year that the observation occurs during year \(y\) and \(S^s_{y,r,a,s}\) is the survey selectivity.


### Relative abundance indices {-}
\begin{equation} 
  \widehat{I}^s_{r,y} = \sum_s\sum_a \widehat{N}^s_{a,r,y,s} \bar{w}_{a,y,s}
\end{equation} 
where, \(\bar{w}_{a,y,s}\) is mean weight at age, this can be omitted if the observation is in numbers i.e., abundance instead of biomass.

### Tag recovery observations {-}

When tagged fish are released in to the model partition they are tracked by the tag release event \(k\) for \(n_{T}\) years. The model expects tag-recovery observations to be known by age and sex. These are derived for each recovery event (also year and region specific) and tag-release event by using the same age-length key that was used to release each recovered fish to obtain an sex specific age-frequency. This age-frequency is then aged by the number of years this tag-recovery event was at liberty to derive observed tag-recoveries by age and sex.

This assumes the age-length conversion between releases and recoveries is consistent and allows us to use recovered fish with no length or sex recorded, but has the down side as mentioned earlier in the tag release section of smearing a single recovered fish across multiple age bins and sexes. However, it does mitigate the problem highlighted in chapter \@ref(tagdata) (tag release section) of going backwards and forwards through the age-length transition matrix, which appears to be more problematic.

All tag recoveries are from the fixed gear (longline and pot gear) fishery, due to the recent trend in recoveries by gear method (Figure \@ref(fig:recoveriebygear)), we only considered tags recovered before 2018. The switch from longline to pot fishing in the fixed gear fishery has not seen anywhere near the same recoveries to be reported from the pot gear compared to the longling gear. To avoid this complication we will only consider recoveries from a period when the longline method was the dominant method of fixed gear catch 

Model fitted values for tag event \(k\) recovered in region \(r\) and year \(y\) denoted by the set index \(m = \{r,y\}\) are denoted by \(\widehat{N}^k_{m}\), and are derived as


\[
\widehat{N}^k_{m} =  \sum_a \sum_s T^k_{a,r,y,s} \frac{F^{LL}_{a,r,y,s}}{Z_{a,r,y,s}} (1 - e^{-Z_{a,r,y,s}})\delta_{r,y} \quad m = \{r,y\}
\]
where, \(T^k_{a,r,y,s}\) are the number of tagged fish from tag-release event \(k\), and \(\delta_{r,y}\) is the reporting rate in year \(y\). Initial model runs used the Poisson 

\[
\widehat{N}^k_{m}  \sim \mathcal{Poisson}(\widehat{N}^k_{m})
\]

However, the negative binmoial and multinomial are in development. See Chapter \@ref(taglike) for simple exploration of different tag-likelihoods for estimating movement parameters.

<!--chapter:end:03-SpatialAssessmentModel.Rmd-->

# Survey data {#surveydata}

This chapter explores and describes survey age, length and catch rate data to inform the following spatial model decisions

- Is there spatial differences in age or length composition?
- Is there spatial differences in relative abundance?
- Is there spatial differences growth?




## Survey design {-}

The longline survey in the Gulf of Alaska and Bering Sea began in 1978-79 and is fixed station systematic design [@sasaki1985studies]. "Survey stations are distributed as uniformely as possible in each geographic area. One station per day is fished, and normally 160 hachis/skates are fished at each station. The longline is set at right angles to the isobars in a manner to cover the depth range of 101- 1,000 m. However, the distance between 101 and 1,000 m varied at each survey station. Thus, this complete depth range could not be covered at stations where this distance exceeded the 16 km length of the longline gear. The longline was usually set from shallow to deep waters and was retrieved in the same direction. Hauling the longline started 2 hours after the set was completed." pg 68 @sasaki1985studies


Each hachi is assigned to a depth stratum which is treated as an independent observation by stratified estimators to generate population estimates and associated variance (**Is this right?** seems inappropriate given all these hachi are essentially a cluster from a single fishing event)


## Abundance (catch rate) data {-}
A geostatistical model-based estimator was used to estimate spatial estimates of abundance. `sdmTMB` [@sdmTMB]  was the package used for this exercise. Geostatistical models require spatial locations to be known for all observations. A limitation of using hachi level data is latitude and longitude is only known for the whole longline event (e.g., all hachi on the same line have the same spatial coordinates). For each longline event we summed over all hachi. Survey abundnace pooled over 50km x 50km grid cells is shown in Figure \@ref(fig:surveycatchgif).


```{r surveycatchgif, fig.cap="A time-series of sablefish catch from the long line survey pooled over 50km by 50km grided cells", eval = T, echo = F}
knitr::include_graphics("Figures/catch.gif")
```



<!-- One of the aims is to a geostatistical model-based estimator [@sdmTMB] with the current estimator. The survey is a systematic/fixed design, but the current estimator for the population total and variance is based on a stratified random survey design. This is often done because there is no design based estimator for systematic surveys that have a single primary sampling unit i.e., the only random event would be for the first station then the rest are systematically placed based on the first sampling unit. I wonder if a stratified random variance estimator will under state the variance (see @fewster2011variance for more detail on this issue).  -->


<!-- A consideration when using the survey data within a geostatistical model-based estimator is what to define an observation as. The current estimator treats each skate/hachi as an "observation" (need to ask about this). Due to the fishing of gear being perpendicular to land a single fishing event can cover many depth strata. Each skate/hachi is attached to a depth strata where it is used in the current estimator (does this inflate the sample size? should skate within a fishing event be treated as a cluster?). Although it would be interesting to use the skate/hachi level data, only the start and end latitude and longitudes are recorded. This means we would have to interpolate a coordinate for each skate or pool data over each fishing event for use in the geostatistical model? -->


A range of covariates, spatio-temporal and likelihood assumptions were considered in the geostatistical analysis. Two covariates that were forced in the model whether they were statistically significant or not was Year and country. There has been documented difference between Japanese and NMFS sampling relating to hook and ganion construction [@kimura1997standardizing]. 


Given the unusual shape of the coast for the region, an boundary was created to avoid spatial regions in the Bearing sea sharing information with CGOA and EGOA. The mesh assumed in models that has spatial random effects but **not** spatio-temporal random effects is shown in Figure \@ref(fig:meshCPUE).

```{r meshCPUE, fig.cap="Mesh for spatial random effects, red dots indicate station location", eval = T, echo = F, fig.width=5, fig.height= 5}
knitr::include_graphics("Figures/mesh.png")
```

The first model was configured using code chunk \@ref(fig:firstModel).
```{r firstModel, echo = T, eval = F}
fit_spatial_tweedie <- sdmTMB(
  catch ~ 0  + as.factor(Year) + Country,
  family = tweedie(link = "log"),
  spatial = "on",
  time = "Year",
  spatiotemporal = "off",
)
```

This model structure was repeated assuming the Gamma and Lognormal distribtion because there were only 5 records of 4000 that did not catch sablefish strictly positive distributions were only considered. Residuals shown in Figure \@ref(fig:meshCPUE)


```{r spatialResid, fig.cap="Boxplot comparing overall residuals between distributions", eval = T, echo = F, fig.width=5, fig.height= 5}
knitr::include_graphics("Figures/spatial_residuals.png")
```


+----------------------------+-----------------------------------------------------------------------------+
| Symbol                     | Description                            
+============================+=============================================================================+
| \(\mathcal{M}_1\)          | `y ~ year + country + omega`
+----------------------------+-----------------------------------------------------------------------------+
| \(\mathcal{M}_2\)          | `y ~ year + country + FMP_region + omega`
+----------------------------+-----------------------------------------------------------------------------+
| \(\mathcal{M}_3\)          | `y ~ year + country + epsilon`
+----------------------------+-----------------------------------------------------------------------------+

where `omega` is time-invariant Gaussian Field (GF) and `epsilon` is time-varying Gaussian Field


```{r spatialprediction, fig.cap="Spatial predition M1 with the tweedie distribution.", eval = T, echo = F, fig.width=5, fig.height= 5}
knitr::include_graphics("Figures/spatial_tweedie_predict.png")
```



```{r indexComparison, fig.cap="Comparison of indicies for the Tweedie distribution and the three model structures.", eval = T, echo = F, fig.width=5, fig.height= 5}
knitr::include_graphics("Figures/indicies.png")
```




```{r surveyProjection, fig.cap="Spatial predictions of log abundance from model M3 (spatio-temporal GF)", eval = T, echo = F,  out.width = '100%', fig.height= 13}
knitr::include_graphics("Figures/spatial_ar1_tweedie_predict_all.png")
```




## Age data {-}
A big challenge of any spatial stock assessment is deriving spatially disaggregated age-frequency observations from small age sample sizes. For both the long-line survey and fixed gear fishery we have both age and length data. This means there are multiple estimators available, including; direct ageing and age-length key [@ailloud2019general;@hoenig2002generalizing] estimators. The forward age-length key method at face value looks to be the most attractive, as it reduces data sparsity when compared to direct ageing estimators. However, we will generate AF's for both methods and see if they effect any model outputs.


A note on sablefish otolith sampling from the survey. "Otolith collections were length-stratified from 1979-94 and random thereafter" pg 9 of @sigler2001alaska.

```{r agesamples, out.width = '100%', fig.height= 5, fig.cap="Number of aged fish by sex, region and year", echo = F, eval = T}
include_graphics(file.path("Figures", "age_samples_by_year_region_sex.png"))
```


## Length frequencies {-}
Often, each haul is subsampled for LF measurements. For each haul we calculate the sampling fraction of samples for LF vs the entire haul catch
\[
\pi_h = \frac{n_h}{N_h}
\]
where, \(n_h\) is the number of fish measured for LF and \(N_h\) are the total number of fish caught in haul \(h\). Each length measurement is then scaled by this sampling fraction to derive the length frequency for the entire haul,

\[
N^h_l = \frac{\pi_h}{n^h_l}
\]
where, \(n^h_l\) is the number of fish in length bin \(l\) for haul \(h\), and \(N^h_l\) is the scaled number of fish.

To generate region wide LF's from the survey data, we used the mean estimator
\[
N^r_l = \frac{1}{\sum\limits_{h \in r}1} \sum\limits_{h \in r} N^h_l
\]
where, \(h \in r\) denotes only hauls within region \(r\) and \(\sum\limits_{h \in r}1\) indicates the number of hauls in that region.



```{r lengthsamples, out.width = '100%', fig.height= 5, fig.cap="Number of fish measured for length by sex, region and year", echo = F, eval = T}
include_graphics(file.path("Figures", "length_samples_by_year_region_sex.png"))
```

## Age frequencies {-}

Two estimators were explored for deriving spatially disaggregated age-frequencies from survey data. These were direct ageing estimators and age-length key estimators.

### Direct ageing estimators

The direct ageing estimator we applied assumed age-samples within a region and year are the result of simple random sampling, with the estimator.

\[
P_{a,r,y} = \frac{n_{a,r, y}}{\sum\limits_a n_{a,r, y}}
\]
where, \(P_{a,r,y}\) is the proportion in age \(a\) and \(n_{a,r, y}\) are the number of fish aged in age \(a\) region  \(r\) and year \(y\).

To aggregate age frequencies across areas we use an abundance weighted approach, where the abundance estimated from the survey in region \(r\) and year \(y\) denoted by \(I_{r,y}\) is used,

\[
\tilde{P}_{a,r,y} = P_{a,r,y} \frac{I_{r,y}}{\sum\limits_r I_{r,y}}  \  .
\]
If we are going to aggregate proportions at age over multiple regions we would use \(\tilde{P}_{a,r,y}\) such as,

\[
{P}_{a,R,y} = \sum\limits_{r \in R} \tilde{P}_{a,r,y}
\]
where, \(R\) indicates the set of areas that we are aggregating and this is normalized so that it sums to one, which leads to area aggregated estimates

\[
{P}_{a,R,y} = \frac{{P}_{a,R,y}}{\sum\limits_a{P}_{a,R,y}}  \ .
\]


Length bins that did not have age samples were imputed with the global age-length key values. This was to include length samples in the length frequency to avoid odd age-frequencies.

### Age length key estimators
The Age length key (ALK) estimator uses paired age-lengths to calculate an age-length key which is used to derive age-frequencies by multiplying the ALK by scaled length frequencies.

Constructing an age-length key requires paired age and length measurements which calculate the probability of age given a length bin

\[
\hat{p}(a|l) = \frac{n_{a,l}}{n_{+,l}}
\]
where, \(n_{+,l}\) is the number of ages with corresponding lenghts that fall within length bin \(l\).

The probabilities of age given length using the forward ALK method are then simply multiplied by the marginal probabilities by the scaled length frequencies

\[
P_{a,r} = \hat{p}(a|l) N^r_l
\]

where, \(N^r_l\) are regional specific scaled length frequencies (described in the prior section). The ALK can also be regional specific where only paired age-lengths are used from that region. This is not considered in this work because of the lack of samples when building ALKs at this resolution.

### Continuation ratio logits (CRL) {-}

**This method was not explored in detail**  due to the sex dimension for sablefish adding a complexity we didn't have time to get in to. There is R code that applies the CRL model ignoring sex in the data and wanted to partially document it as it seems quite promising.


An alternative to the ALK is to use a continuation ratio model (CRL) to model the distribution of ages \(\boldsymbol{p}_a(\boldsymbol{x}) = (p_J, \dots, p_A)\), which has been proposed and explored in recent studies [@berg2012spatial;@correa2020improved]. Using a \(J\) minus \(A\) conditional model for describing the probability of being age \(a\) given it as at least \(a\), that is

\begin{equation}
\pi_a = P(Y = a| Y \geq a) = \frac{p_a}{p_a + p_{a+1} + \dots + p_A} , \ \ a = J, \dots, A-1
\end{equation}


Given the set of A − J models, the estimated unconditional probabilities \(p_a\) from the conditional probabilities \(\pi_a\) follows

\begin{align}
\hat{p}_a =&  \hat{\pi}_a , \ a = R\\
\hat{p}_a =& \hat{\pi}_a \bigg( 1 - \sum\limits_{k = J}^{a - 1} \hat{p}_a \bigg) = \hat{\pi}_a \prod\limits_{k = J}^{a - 1} (1 - \hat{\pi}_k) , \ a > J
\end{align}





### Observation error estimates

Bootstrapping is used to estimate standard errors for \(P_{a,R,y}\). Data used to calculate age-length keys are re sampled with replacement to calculate a SE and CV. This produces a CV for each age, region and year denoted by \(CV_{a,r, y}\). Proportions at age are assumed to be distributed according to the multinomial distribution. To translate the observation error expressed as a CV to an initial effective sample denoted by \(N^{eff}_y\) we apply a non-linear least squares solver to estimate \(N^{eff}_y\) by minimsing the following expression

\[
\log CV_{a,r, y} = \log \sqrt{\frac{P_{a,R,y}\widehat{N}^{eff}_y \left(1 -  P_{a,R,y}\right)}{P_{a,R,y}\widehat{N}^{eff}_y}}
\]



## Age-length data {-}



```{r agelengthbysex, echo=FALSE,out.width="61%", fig.height=6,fig.cap="Age-length by sex",fig.show='hold',fig.align='center'}
knitr::include_graphics(c(file.path("Figures","raw_growth_by_sex.png")))
``` 



```{r malegrowth, echo=FALSE,out.width="49%", fig.height=5,fig.cap="Male age-length by region (left panel) and decade (right panel)",fig.show='hold',fig.align='center'}
knitr::include_graphics(c(file.path("Figures","raw_male_growth_by_region.png"),file.path("Figures","raw_male_growth_by_decade.png")))
``` 


```{r femalegrowth, echo=FALSE,out.width="49%", fig.height=5,fig.cap="Female age-length by region (left panel) and decade (right panel)",fig.show='hold',fig.align='center'}
knitr::include_graphics(c(file.path("Figures","raw_female_growth_by_region.png"),file.path("Figures","raw_female_growth_by_decade.png")))
``` 


## Appendix {-}


<!-- Sampling notation -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | Symbol                     | Description                             -->
<!-- +============================+=============================================================================+ -->
<!-- | \(h\)                      | haul index -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | \(i\)                      | an individual fish index -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | \(a_i\)                    | age of individual \(i\) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | \(l_i\)                    | length of individual \(i\)  -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | \(C^h\)                    | Catch for haul \(h\) can be numbers or biomass -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | \(n^h_l\)                  | number of fish measured for length from haul \(h\)  -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->
<!-- | \(n^h_a\)                  | number age samples in haul \(h\). Assumed to be a subsample of \(n^h_l\) -->
<!-- +----------------------------+-----------------------------------------------------------------------------+ -->




```{r femaleCDFagesamples, out.width = '100%', fig.height= 10, fig.cap="CDF of age frequencies for females", echo = F, eval = T}
include_graphics(file.path("Figures", "raw_ecdf_AFs_by_year_region_female.png"))
```

```{r maleCDFagesamples, out.width = '100%', fig.height= 10, fig.cap="CDF of age frequencies for males", echo = F, eval = T}
include_graphics(file.path("Figures", "raw_ecdf_AFs_by_year_region_male.png"))
```


```{r femaleCDFlengthsamples, out.width = '100%', fig.height= 10, fig.cap="CDF of length frequencies for females", echo = F, eval = T}
include_graphics(file.path("Figures", "raw_ecdf_LFs_by_year_region_female.png"))
```

```{r maleCDFlengthsamples, out.width = '100%', fig.height= 10, fig.cap="CDF of length frequencies for males", echo = F, eval = T}
include_graphics(file.path("Figures", "raw_ecdf_LFs_by_year_region_male.png"))
```

<!--chapter:end:04-SurveyData.Rmd-->

# Fishery dependent data {#observerdata}

This chapter describes an exploratory analysis using the observer and other fishery reported data. The aim is to highlight trends and signals in the data that would lead to an assessment decision or consideration e.g. spatial and temporal structures. All data analysed in this section has been extracted from the AKFIN database (North Pacific Observer Program). It included extracts from the length, age and catch reports. Some of the data cannot be displayed here due to confidentiality reasons. Locations shown in this work have been generalized to generic center locations of a 20 x 20 sq. km grid if there were 3 or more unique vessels, as per NOAA/NMFS regulations.

This data will result in two important assessment inputs: observed catch (which will be assumed known with a high level of precision) and composition by fishery, either age or length. 


Define what a fishery is? Currently a "fishery" is defined by a gear type i.e., trawl vs fixed-gear (line and pot).

is there evidence for changing selectivity? i.e., time-blocks

Auxiliary information see Table 3.3 @goethel2021assessment for management actions which describe spatial closures by gear.


```{r CatchHistoryByRegion, out.width = '80%',fig.height= 4, fig.cap = "Catch history by region", echo = F, eval = T}
include_graphics(file.path("Figures", "observed_catch_by_area.png"))
```

## Catch data {-}

Deriving annual estimates of catch by year and region should be fairly straight forward. Fishers generally have a legal requirement to record catches and we will probably just assume that reported catches are accurate. Unfortunately catch by gear type and region is not known prior to 1977 (pers comms Kari Fenske).

The following bubble plots display the proportion of catch sampled for lengths and ages by observers relative to the catch by gear and area from 1990 when observer data available. When bubbles and crosses have the same size then the observer samples are proportional to catch for a given year. When crosses are larger then observers sampled more relative to the catch and vice versa. These are meant to give some impression of "representative" sampling i.e., are there regions or gears that are under or over sampled.


```{r observerRepresentativeSamplesByGear, out.width = '100%', fig.cap = "Observer representative figures by method. When the cross and circle are the same size this indicates observers sample proportional to the catch for a year.", echo = F, eval = T}
include_graphics(file.path("Figures", "proportion_observed_catch_by_method_2.png"))
include_graphics(file.path("Figures", "proportion_observed_catch_by_method_1.png"))
```


```{r observerRepresentativeSamplesByArea, out.width = '100%', fig.cap = "Observer representative figures by method. When the cross and circle are the same size this indicates observers sample proportional to the catch for a year.", echo = F, eval = T}
include_graphics(file.path("Figures", "proportion_observed_catch_by_area_2.png"))
include_graphics(file.path("Figures", "proportion_observed_catch_by_area_1.png"))
```


## Length data {-}

### Catch at length estimator {-}
The aim of catch at length is to obtain representative length frequencies (LF) of fish removed by a specific fishery within a specific area and year. These estimates are derived from observer collected data of which only a subsample of fishing trips are observed. Observers are tasked with subsampling observed catch to provide length and age samples. Thus these samples need to be scaled to either the stock wide level for inputs into a single area stock assessment or to some region level for use in a spatially disaggregated stock assessment. This is sampling structure has a natural hierarchy which is illustrated in Figure \@ref(fig:fishdependentsampling).


```{r fishdependentsampling, out.width = '100%',fig.height= 7, fig.cap = "Sampling hierachy for fishery dependent catch at age and catch at length estimates", echo = F, eval = T}
include_graphics(file.path("Figures", "fishery_dependent_sampling.png"))
```

Each node in the sampling hierarchy is denoted by the index \(m\). When a node is nested within another node, we use \(m \in m'\) to define that \(m'\) is nested within \(m\).

+----------------------------+-----------------------------------------------------------------------------+
| Symbol                     | Description                            
+============================+=============================================================================+
| \(m,l,a,s\)                | index for node, length bin, age bin, sex
+----------------------------+-----------------------------------------------------------------------------+
| \(n_{m,l,s},N_{m,l,s}\)    | Number of fish in unscaled and scaled LF for sex $s$, at node $m$
+----------------------------+-----------------------------------------------------------------------------+
| \(n_{m,a,s},N_{m,a,s}\)    | Number of fish in unscaled and scaled AF for sex $s$, at node $m$
+----------------------------+-----------------------------------------------------------------------------+
| \(q_m\)                    | Number of child nodes for node $m$
+----------------------------+-----------------------------------------------------------------------------+
|\(n_{m,l,a,s},N_{m,l,a,s}\) | Number of fish in unscaled and scaled LF \& AF for sex $s$, at node $m$
+----------------------------+-----------------------------------------------------------------------------+
|\(\pi_m\)                   | Scaling factor at node $m$
+----------------------------+-----------------------------------------------------------------------------+
|\(W_m,M_m,P_m,A_m\)         | scaling variables for each node (weight, numbers, proportion, area)
+----------------------------+-----------------------------------------------------------------------------+
|\(w_{l,s}\)                 | mean weight for length bin $l$ and sex $s$
+----------------------------+-----------------------------------------------------------------------------+
|\(w_{a,s}\)                 | mean weight for age bin $a$ and sex $s$
+----------------------------+-----------------------------------------------------------------------------+
|\(K_{m,l,a,s}\)             | Age-length key for node \(m\) by sex
+----------------------------+-----------------------------------------------------------------------------+

We require information the numbers at length for each node ( $n_{m,l,s}$) which make up the unscaled length frequency. Length weight parameters that describe the allometric relationship of weight from length ($w_{l,s} = a_s L_{l,s}^{b_s}$). The last piece is how we want to scale the frequencies ($W_m,M_m,P_m,A_m$). Length frequencies are iteratively calculated up the sampling hierarchy

\[
n_{m,l,s} = \sum_{m' \in m} N_{m',l,s}
\]
where
\[
N_{m',l,s} = \pi_{m'} n_{m',l,s}
\]

There are multiple approaches for scaling unscaled numbers to scaled numbers such as scaling by area etc, scaling by weight. 

<!-- \[ -->
<!-- \pi_m = \left\{\begin{array}{lr} -->
<!-- W_m / \sum_{l,s} w_{l,s}n_{m,l,s}, & \text{if scaling is by weight}\\ -->
<!-- M_m / \sum_{l,s}n_{m,l,s} , & \text{if scaling is by weight}\\ -->
<!-- 1 / P_m, & \text{if scaling is by proportion}\\ -->
<!-- \frac{A_m\sum_m W_m}{q_m \sum_m W_m}, & \text{if scaling is by area}\\ -->
<!-- 1, & \text{if scaling is none}\\ -->
<!-- \end{array}\right\} -->
<!-- \] -->


If an individual haul is subsampled i.e., only 50 fish are measured for length of 1000 caught in a single haul, then we use the sampling fraction denoted by \(\pi^{h}\) to scale the subsample to a haul level LF. Region wide LFs are calculated by summing the LFs over all sampled hauls within that region. Due to each haul having a scaled LF hauls, hauls with larger catches will naturally contribute more samples to a region wide LF, thus making this a catch weighted estimator.

If \(n^h_l\) fish are measured for lengths from a haul that had \(N^h_l\) fish, then the scaler for the subsample follows,
\[
\pi^{h} = \frac{N^h_l}{n^h_l}
\]
each length sample in haul \(h\) is divided by this sampling fraction. 

The estimator for region wide LFs denoted by \(\widehat{N}^r_{l,s}\) follow,

\[
\widehat{N}^r_{l,s} = \sum_{h\in a} n^h_{l,s} \pi^{h} \pi^{r}
\]
where \(\pi^{r}\), is the region scalar which weights each observed haul by the catch from hauls that had LFs collected and total catch in the region.
\[
\pi^{r} = \frac{C_r}{\sum_{h \in h'} C_h}
\]
where, \( C_h\) is the catch from haul \(h\), \(h'\) denotes the set of hauls that were observed and had LFs taken, and \(C_r\) is the region wide catch, which is a legal requirement from fishers to report.


Region wide LFs are aggregated across multiple regions denoted by the set \(r'\) using an another catch weighted approach

\[
\widehat{N}^{r'}_{l,s} = \sum_{r\in r'} \widehat{N}^r_{l,s} \pi^{r}
\]
where,

\[
\pi^{r} = \frac{C_r}{\sum_r C_r}
\]
where, \(A_r\) is the catch in region \(C_r\) and \(\sum_r C_r\) denotes catch over all regions under investigation.


```{r ScaledLFs, out.width = '100%', fig.cap = "This figure shows how each of the scaling factors change the raw sampled LFs (\\(n^h_{l,s}\\)) to obtain regional scaled length frequencies (\\(\\widehat{N}^{r'}_{l,s}\\)) for the fixed gear fishery.", echo = F, eval = T}
include_graphics(file.path("Figures", "LF_scaling.png"))
```

A gear and region had to have at least 100 fish measured for length in order to generate catch at length estimates.

## Age data {-}

Only the fixed gear fishery has age-samples to estimate age-frequencies. The same direct and age-length key estimators as the survey was used for the fixed gear survey.


## Age-length data {-}



```{r obsagelengthbysex, echo=FALSE,out.width="61%", fig.height=6,fig.cap="Age-length by sex",fig.show='hold',fig.align='center'}
knitr::include_graphics(c(file.path("Figures","obs_raw_growth_by_sex.png")))
``` 



```{r obsmalegrowth, echo=FALSE,out.width="49%", fig.height=5,fig.cap="Male age-length by region (left panel) and decade (right panel)",fig.show='hold',fig.align='center'}
knitr::include_graphics(c(file.path("Figures","obs_raw_male_growth_by_region.png"),file.path("Figures","raw_male_growth_by_decade.png")))
``` 


```{r obsfemalegrowth, echo=FALSE,out.width="49%", fig.height=5,fig.cap="Female age-length by region (left panel) and decade (right panel)",fig.show='hold',fig.align='center'}
knitr::include_graphics(c(file.path("Figures","obs_raw_female_growth_by_region.png"),file.path("Figures","raw_female_growth_by_decade.png")))
``` 



<!--chapter:end:05-FisheryDependentData.Rmd-->

# Tagging data exploration {#tagdata}
Since 1972 there have been approximately 400 000 sablefish tagged in Alaska waters, of which over 38 500 have been recovered. Although there is extensive and long term tagging data, this information is not currently directly included in the stock assessment [@goethel2021assessment].


Historical publications investigating movement of Alaskan sablefish include @heifetz1991movement, @hanselman2015move

## Exploratory analysis of the tag data {-}

Figure \@ref(fig:TagReleases) shows the spatial distribution of both releases and recaptures, which both have fairly broad spatial distributions which is a good attribute. Figure \@ref(fig:recoveriebygear) shows the number of recoveries by gear method and year, this highlights a major drop off in recoveries from the Longline gear with no other gear has picked up in. This will have to be discussed with the wider team. In particular, what years to consider this data to be informative. 


```{r TagReleases, out.width = '100%',fig.height= 5, fig.cap = "Tag releases and recoveries pooled over all years", echo = F, eval = T}
include_graphics(file.path("Figures", "Overall_tag_spatial.png"))
```



```{r recoveriebygear, out.width = '60%',fig.height= 3.4, fig.cap = "Recovered fish by gear type and year", echo = F, eval = T}
include_graphics(file.path("Figures", "recovery_by_gear_and_method.png"))
```



We applied a simple Lincoln-Peterson estimators to view changes in abundance over the area of interest using a subset of the tag recoveries. The Lincoln-Peterson estimator follows

\[
\widehat{N} = \frac{n e^{-(\kappa + M)}K\tau}{k} \ .
\]

+----------------------------+-----------------------------------------------------------------------------+
| Symbol                     | Description                            
+============================+=============================================================================+
| \(N\)                      | Number of fish in the population 
+----------------------------+-----------------------------------------------------------------------------+
| \(n\)                      | Number of fish released with a tag
+----------------------------+-----------------------------------------------------------------------------+
| \(K\)                      | Number of fish scanned for tags (fishery catch over the period of recoveries)
+----------------------------+-----------------------------------------------------------------------------+
| \(k\)                      | Number of tagged fish recovered 
+----------------------------+-----------------------------------------------------------------------------+
| \(\tau\)                   | Reporting/detection rate = 0.276 from @heifetz2001estimation
+----------------------------+-----------------------------------------------------------------------------+
| \(\kappa\)                 | Annual tag loss or mortality = 0.1 @beamish_88
+----------------------------+-----------------------------------------------------------------------------+
| \(M\)                      | Natural mortality = 0.1 based on @goethel2021assessment
+----------------------------+-----------------------------------------------------------------------------+


Most of the tags are released during the summer survey (Figure \@ref(fig:TagDataByMonth)) but recoveries are more spread out within a year.

```{r TagDataByMonth, echo=FALSE,out.width="49%", fig.height=5,fig.cap="Tag recovery and release distributions by month",fig.show='hold',fig.align='center'}
knitr::include_graphics(c(file.path("Figures","Releases_by_month.png"),file.path("Figures","Recoveries_by_month.png")))
``` 


A Lincoln-Peterson estimator based on long line tag recoveries that were at liberty for a year (300 - 420 days) was explored. This time-at-liberty period was chosen so we could calculate annual estimates of abundance and thus derive annual estimates of exploitation rates to compare with the assessment. Due to the large distance traveled by sablefish over this time-period (Figure \@ref(fig:KMtravelled)), the spatial extent considered was the entire stock region.


```{r KMtravelled, out.width= '60%',fig.height= 3.4, fig.cap = "Distance (km) between release location and recovery location for fish at liberty for a year.", echo = F, eval = T}
include_graphics(file.path("Figures", "distance_travelled_after_a_year.png"))
```


A few calculations/approximations are included in the Lincoln-Peterson estimator, these include tag loss and natural mortality for tagged fish after a year at liberaty, reporting rates from the commercial fishery, and changing reported weights to numbers. The period for calculating catch that was scanned during tag recoveries was the 15 of May to 15 of September. This was chosen as it brackets the monthly peak of releases (Figure \@ref(fig:TagDataByMonth)). The other adjustment was converting reported weight into numbers. For this I calculated the mean numbers of fish per tonne based on the observer data. I then used this multiplier for the catch reported in tonnes during the period of recoveries to extract the numbers scanned by the fishery each year.


```{r popestimateslincolnpeterson, out.width = '60%',fig.height= 3.4, fig.cap = "Recovered fish by gear type and year", echo = F, eval = T}
include_graphics(file.path("Figures", "LincolPetersonEstimates.png"))
```

Once we have annual population abundance estimates we can derive a rough annual exploitation rate (\(U_y \)) denoted by 

\[
U_y  = \frac{C_y}{\widehat{N}}
\]
where, \(C_y\) is the annual catch in numbers for the Longline fishery shown in Figure \@ref(fig:roughExploitationRate).

```{r roughExploitationRate, out.width = '60%',fig.height= 3.4, fig.cap = "Exploitation rate for the Longline fishery based on Lincoln-Peterson estimates", echo = F, eval = T}
include_graphics(file.path("Figures", "rough_exploitation_rate.png"))
```



## Integrating tagging observations in spatial age-structured models {-}

This project intends to explore a range of methods for utilizing tag-recovery observations in spatially disaggregated age-structured stock assessments.

## Tagging things to consider with relevant references {-}

- Years to retain tagged fish in the partition "After approximately 9 yr the number of recaptures was small and contributed more to the variance associated with the trends in movement than an improved understanding of these trends" @beamish_88
- Reporting rates [@heifetz2001estimation]
- Scan detection rates. Is this not a factor of reporting rates?
- Mixing time and how to deal with it?
- Tag loss "tag loss in the fist year was approximately 10% and after that approximately 2% per year." @beamish_88
- Release conditioning vs recapture conditioning [@vincent2020parameter;@mcgarvey2002estimating]
- likelihood choice? [@hanselman2015move]


## Releasing tags {-}
Tag release events involve releasing a tag-cohort at the beginning of a year within a specific area. A tag cohort is indexed by \(k\) and has an implied year \(y\) and region \(r\) index. \(\boldsymbol{N}^k\) is used to denote a vector of lengths or ages for tag-cohort \(k\). In general, only the length frequency is known at time of release for each tag-cohort becasue ageing is a fatal process. We consider two different approaches for seeding a tag cohort within spatial age-structured models. The two methods are essentially the same but differ in whether the length are converted to age outside of the model ("External") or done within the model ("Internal"). The internal method requires users to supply length frequency for each tag-cohort and the model will use the assumed growth assumptions and sex ratios to convert the lengths to ages. The external approach will use an age-length key outside of the model to derive an age frequency that can then be supplied to the model. 

A frequent assumption of age-structured tagging models in the literature [@maunder1998integration;@vincent2020parameter] is that the age-frequency of each tag-cohort is known. Due to the fact that ageing is a fatal process, we assume they have used the external approach. If the age-length key is representative, then this method is expected to be very similar and have better computational performance. Factors to consider at time of release are; gear method used to select releases, where releases occur, and time of releases. If growth is estimated within the assessment model, then the internal method may be prefered to keep the growth assumptions consistent between LF observations and other model quantities.


One down fall of the internal approach is tag releases and recoveries are both length-based inputs. Due to age-structured modelling growth as length conditional on age. Moving individuals back and forwards through the age-length transition matrix (length $\rightarrow$ age $\rightarrow$ length) will cause "smearing" of length frequencies. This is demonstrated in Figures \@(ref:addagelength) and \@(ref:showagelengthtransition_problem), and needs to be considered when considering model fitted values for observations and corresponding likelihood assumptions. Given this phenomenon, we make the argument that the external age-length key approach should be used. The external method means we assume (there actually is error in this) the ages of released fish and time at liberty, thus we know the age at recovery.

```{r addagelength, out.width = '70%', echo = F, eval = T, fig.cap='An example of theoretical length at age, with overlapping length bins used to describe the effect of going back and fourth through the age-length transition matrix.'}
ages = c(1:30) # as.numeric(names(ibm_out$init_1$values))
lens_bins = c(seq(from = 4, to = 10, by = 2), 11:81) # ibm_out$model_attributes$length_mid_points
lens = lens_bins[1:(length(lens_bins) - 1)] + diff(lens_bins) / 2

## simple illustrations
example_ages = 1:3
mean_length = c(13,19,24)
len_bins = seq(10,35,by = 5)
len_midpoints = len_bins[1:5] + diff(len_bins)/2
## randomly get some ages and lengths
Num_age = #rowSums(prob_length_given_age_matrix[c(6,9,13),c(32,35,42,45,50)]) / 100
  c(1757.704, 4320.569, 3129.675)
cv = 0.13
## generate age-length probability transition matrix used in age-based models
prob_length_given_age = matrix(0.0, nrow = length(example_ages), ncol = length(len_bins) - 1)
for(age_ndx in 1:length(example_ages)) {
  for(len_ndx in 2:length(len_bins)) {
    if (len_ndx == 2) {
      prob_length_given_age[age_ndx, len_ndx - 1] = pnorm(len_bins[2], mean_length[age_ndx], mean_length[age_ndx] * cv)
    } else if (len_ndx == length(len_bins)) {
      prob_length_given_age[age_ndx, len_ndx - 1] = 1 - pnorm(len_bins[length(len_bins) - 1], mean_length[age_ndx], mean_length[age_ndx] * cv)
    } else {
      prob_length_given_age[age_ndx, len_ndx - 1] = pnorm(len_bins[len_ndx], mean_length[age_ndx], mean_length[age_ndx] * cv) -  pnorm(len_bins[len_ndx - 1], mean_length[age_ndx], mean_length[age_ndx] * cv)
    }
  }
}

Cols = colorRampPalette(brewer.pal(n = 7, name = "Blues"))(5)[2:5]
plot(1:40, 1:40, type = "n", xlab = "Length", yaxt = "n", ylab = "Frequency", xlim = c(7,35), ylim = c(0,800), main = "Example length at age distribution")
temp_lens = 1:40
probs = seq(0.01,0.99, length = length(temp_lens))
for(i in 1:length(example_ages)) {
  lines(y = Num_age[i] * dnorm(temp_lens, mean_length[i], cv *  mean_length[i]) / sum(dnorm(temp_lens, mean_length[i], cv *  mean_length[i])), x = temp_lens, lwd = 3, col = Cols[i])
}
text(x = mean_length, y = 238, labels = paste0("a = ",example_ages), cex = 1.1, col= Cols)
text(x = len_midpoints, y =  773, labels = c(expression(l[1]),expression(l[2]),expression(l[3]),expression(l[4]),expression(l[5])), cex = 1.5, col= "red")

abline(v = len_bins, lty = 2, col = "red", lwd = 2)
group_name =  c(expression(l[1]),expression(l[2]),expression(l[3]),expression(l[4]),expression(l[5]))
```


```{r showagelengthtransitionproblem, out.width = '100%', fig.height= 4, fig.cap="A visualisation of the effect of going back and forth through an age-length transition matrix. This can happen when tag releases and recaptures are input as length in an age-structured model. The model converts lengths to ages, then reconverts the age to length for observations. This is assuming the same age-length relationship in Figure \\@ref(fig:addagelength)", echo = F, eval = T}
#############
# A fish in length bin l2
#############
# probability of length given age = prob_length_given_age
#rowSums(prob_length_given_age)
# What about probability of age given length? re-scale
prob_age_given_length = sweep(prob_length_given_age, STATS = colSums(prob_length_given_age), MARGIN = 2, FUN = "/")

dimnames(prob_length_given_age) = dimnames(prob_age_given_length)  = list(example_ages, len_midpoints)
prob_length_given_age_long = melt(prob_length_given_age)
colnames(prob_length_given_age_long) = c("age", "length","prop")
prob_length_given_age_long$age = factor(prob_length_given_age_long$age)
prob_length_given_age_long$length = factor(prob_length_given_age_long$length)

prob_length_given_length_long = melt(prob_age_given_length)
colnames(prob_length_given_length_long) = c("age", "length","prop")
prob_length_given_length_long$age = factor(prob_length_given_length_long$age)
prob_length_given_length_long$length = factor(prob_length_given_length_long$length)

test = ggplot(prob_length_given_length_long, aes(fill=age, y=prop, x=length)) + 
  geom_bar(position="stack", stat="identity") +
  xlab("Numbers") +
  ggtitle("Age composition conditioned on length bin")+
  scale_x_discrete(labels=group_name)  # Ad

test_alt = ggplot(prob_length_given_age_long, aes(fill=length, y=prop, x=age)) + 
  geom_bar(position="stack", stat="identity") +
  xlab("") +
  ggtitle("Length composition conditioned on age")+
  scale_fill_discrete(labels=group_name)  # Ad

## In an age based model
## A single Fish in length bin = 2, equates to the following ages
age_from_l2 = prob_age_given_length[,2]
#age_from_l2
## The model derived lenght distribution of this
model_length = age_from_l2 %*% prob_length_given_age
#model_length

P1 = ggplot(data = data.frame(length = factor(len_midpoints), result = c(0,1,0,0,0)), aes(y=result, x=length)) + 
  geom_bar(position="dodge", stat="identity") +
  ylab("Numbers") +
  xlab("Length bin") +
  ylim(0,1) +
  ggtitle(substitute(paste("Single fish in length bin = ", l[len_ndx]), list(len_ndx = 2))) +
  theme(axis.text.x=element_text(size=15),axis.text.y=element_text(size=15),
        axis.title=element_text(size=15,face="bold"),
        title =element_text(size=13)) +
  scale_x_discrete(labels=group_name) +
  theme_bw()

P2 = ggplot(data = data.frame(age = factor(example_ages), result = age_from_l2), aes(x = age, y = result)) + 
  geom_bar(position="dodge", stat="identity") +
  ylab("") +
  ylim(0,1) +
  ggtitle(substitute(paste("Age disribution of fish in length bin = ", l[len_ndx]), list(len_ndx = 2))) +
  theme(axis.text.x=element_text(size=15),axis.text.y=element_text(size=15),
        axis.title=element_text(size=15,face="bold"),
        title =element_text(size=13)) +
  theme_bw()



P3 = ggplot(data = data.frame(length = factor(len_midpoints), result = as.numeric(model_length)), aes(y=result, x=length)) + 
  geom_bar(position="dodge", stat="identity") +
  ylab("") +
  ylim(0,1) +
  xlab("Length bin") +
  ggtitle(substitute(paste("Length distribution of initial fish = ", l[len_ndx]), list(len_ndx = 2))) +
  theme(axis.text.x=element_text(size=15),axis.text.y=element_text(size=15),
        axis.title=element_text(size=15,face="bold"),
        title =element_text(size=13)) +
  scale_x_discrete(labels=group_name) +
  theme_bw()# Ad


P1 = P1 + geom_segment(aes(x = 4, y = 0.5, xend =5, yend = 0.5, size = 1),
               arrow = arrow(length = unit(1, "cm")), col = "red") +
  guides(size = "none")
P2 = P2 + geom_segment(aes(x = 3.8, y = 0.5, xend =5, yend = 0.5, size = 1),
                 arrow = arrow(length = unit(1, "cm")), col = "red") +
  guides(size = "none")
grid.arrange(grobs = list(P1, P2, P3), nrow = 1)
```

#### Internal method {-}
Age-structured stock assessment models contain growth models, which describe length conditioned on age. This requires assumptions on the distribution and associated parameters. The default is often the normal distribution with mean length at age denoted by \(\bar{l}_a\) with standard deviation parameterised as a coefficient of variation (\(\sigma_a = cv*\mu_a\)). This information enables the model to derive a growth transition matrix \(P_{l|a}\). Given the lower and upper limits for each length bin denoted as \(\boldsymbol{b} = (b_1, b_2, \dots, b_{max})'\), the probability of being in length bin \(l\) given age \(a\)

\begin{equation}
P_{l|a} = 
\begin{cases}
\Phi(b_{l + 1}|\mu_a,\sigma_a)\quad &\text{for } l = 1\\
\Phi(b_{l + 1}|\mu_a,\sigma_a) - \Phi(b_l|\mu_a,\sigma_a)\quad &\text{for } 1 < l < n_l\\
1 - \Phi(b_{l}|\mu_a,\sigma_a)\quad &\text{for } l = n_l
\end{cases}
  (\#eq:agelengthtransition)
\end{equation}


where \(\Phi(x|\mu,\sigma)\) is the cumulative normal (but could be generalised to any probability distribution). If growth varies by attributes such as sex, stock or region then this will need to be calculated for each growth model.

At the point a tag cohort is released, the model can derive the length composition of the vulnerable population using the transition matrix derived in Equation \ref(eq:agelengthtransition). Given the number of estimable parameters that govern the age-stricture at a point in time and growth model, an exploitation rate is calculated so that if there are not enough numbers in a length bin to be tagged, a penalty can be added to the objective function to dissuade the combination of parameters that generated this situation. Tag-release by length is a known quantity and so the model must allow for a minimum vulnerable length composition to that released. To enforce this, an exploitation rate by length \((u_l)\) is calculated as follows,
\begin{equation}
u_l = \frac{N^k_{l}}{\sum_a}N_{y_k,a,r_k} P_{l|a}
  (\#eq:lengthexploit)
\end{equation}

During parameter estimation there are no constraints within the model to trial a set of parameters that will allow \(u_l > 1\) i.e., more observed tag-releases than in the available population. To stop negative numbers at age, \(u_l\) is set at a level less than 1 and a penalty added to the objective function to discourage parameters from allowing this condition. Finally, tag-release at age is calculated as follows,
\begin{equation}
N^k_{a} = N_{y_k,a,r_k}  P_{l|a} u_l
  (\#eq:tagatagerelease)
\end{equation}

Once the tag cohort are created in the model it is assumed that tagged fish are exposed to the same dynamics as un-tagged fish. **Revisit this** we will want to explore mixing behaviour/assumptions.


#### External method (Age-length key method) {-}
Once a tag-release event has occurred for tag group \(k\), the only known knowledge is the length distribution \(N^k_l\). Assuming there is an accessible forward age-length key which describes the proportion of ages for a given length bin \(\left(P_{a|l}\right)\) that is representative of the vulnerable population to tagging for the same area and time, then the "forward" or "classic" key method can be used @ailloud2019general. If tag-release coincide with a fishing season you could use fishery-dependent derived age length information, assuming the selectivity curves within a length bin are parallel. If only a subset of fish from each haul are released, it would be better to construct an age-length key from a representative sample of fish that were caught but not-tagged. This would be relevant for single vessel survey release events.
\begin{equation}
	N^k_a = \sum\limits_{l = 1}P_{a|l}N^k_l
\end{equation}
where, \(N^k_a\) is used as an known input into the model with no error. **Revist** can we account for uncertainty here?


### Things to consider for tag release {-}

- we don't have much information regarding sex of tagged fish (36.5\% of recovered tags have sex information). Does this lean towards the internal method? can allocate sex ratio based on vulnerable population which may be important, if there is quite a difference in sex disaggregated selectivities.


## Tag recovery observations {-}
There are two types of tag-recovery observations that we will consider in this work, tag-release conditioned and tag-recapture conditioned. This conditioning relates to whether we relate recoveries to the release event or whether we only look at the recoveries relative to other recoveries within a time period [@vincent2020parameter;@mcgarvey2002estimating]. This conditioning relates more to the log-likelihood formulation rather than the model expected values. We should also explore two different approaches regarding the scanned fish of which tag-recoveries are a subset. The first is recoveries were entirely from a fishery which requires some understanding or assumptions on reporting rates. The other method assumes scanned fish are known, which is the approach used in the Casal2 [@doonan2016casal2] stock assessment program. This information can be recorded if the scanning is done either by observers who are recording LF's for the catch or scientific/trained staff when shed sampling landed catch. Both these scanning approaches will require an assuming or information on detection rates.










<!--chapter:end:06-TaggingData.Rmd-->

# Initial model setup {#InitialModelSetup}
Use @punt2019spatial to outline the key considerations

- When did we choose to start the model and why. was it assumed to be in an equilbruim state
- What spatial regions did we start with and why
- How many fisheries
- What observations were included? how were they initially weighted


## Model start time {-}
The start year for the current stock assessment is 1960. This is the earlest year in which catch is available. There is early sex-aggregated length composition from the Japanese longline fleet along with a CPUE index for that same fleet in the 60's/70's. The survey data doesn't kick in until mid to late 70's. Unfortunately catch is the most limiting data input, where we can only go back to 1977 before we loose information on regional catch be gear method (pers comms kari Fenske). For this reason 1977 was the start year for the spatial assessment model.



## Model spatial resolution of the model {-}
This section describes the decision making process we took in configuring an initial spatial model. The first consideration was whether to include areas outside of the Gulf of Alaska (GOA) and the Bering sea Aleutian islands region, which is the spatial extent for the current stock assessment [@goethel2021assessment]. Sablefish inhabit a broad spatial distribution from the northeastern Pacific Ocean from northern Mexico to the Gulf of Alaska. It is currently assessed by three assessment bodies, the first is the area of focus, being the Bering sea and gulf of Alaska [@goethel2021assessment], one off the west coast of Canada **(Cox et al. 2011)** and the west coast from California to Oregan **(Stewart et al. 2011)**. Given the complex juvenile life history coupled with long range movement capabilities of adults, the stock structure of sablefish has been a topic of extensive research.


A recent genetic study by @jasonowicz2017love, found no significant difference between samples off the west coast of US, GOA and bering sea (there were no samples off the west coast of Canada in this study), providing evidence for a single panmictic population. A recent morphometric study [@kapur2020oceanographic], found significant differences in fish traits such as length at age between samples from the west coast of US and GOA. These results were consistent with the findings from @tripp2012population. Tagging is another important information source, with extensive tag releases along the entire North Pacific. Tag recovery data provide evidence for a two stock hypothesis [@kimura1998tagged]. With a stock north of Vancouver Island and a stock to the south. 


For the purposes of this research, we will assume GOA and BS, hereinafter refereed to as the GOABS complex, is isolated (i.e., no movement in or out of this region) from Canada and the west coast of the US. Although there is evidence to include part of Canada (Figure \@ref(fig:TagRecoveriesOutsideRegion)). Due to practical considerations regarding data sharing and the time-frame of this research Canada was excluded. The focus of this research is to build and explore a spatially explicit model for the GOABS complex.

```{r TagRecoveriesOutsideRegion, out.width = '100%', fig.height= 6, fig.cap="Tag recoveries outside of stock boundaries, with release locations (bottom panel).", echo = F, eval = T}
include_graphics(file.path("Figures", "out_of_region_recoveries.png"))
```



The general approach taken, was to build an initial spatial model that had the finest spatial resolution possible and aggregate regions as data limitations and model convergence issues arose. This finest model spatial resolution was initially based on the input data set with the coarsest reported spatial resolution. Data sets have been reported at different spatial resolutions over time. In general, data are reproted at three levels of spatial resolution: latitude and longitude (high spatial resolution), statistical area (Figure \@ref(fig:spatialstatArea)) and larger fishery management boundaries (Figure \@ref(fig:spatialmanagementArea)). Input data sets and the spatial resolution available are described in the following table.


```{r spatialstatArea, out.width = '100%', fig.height= 6, fig.cap="Statistical and reporting areas for GOABS complex", echo = F, eval = T}
include_graphics(file.path("Figures", "alaska-fisheries-boundaries-map.jpg"))
```


```{r spatialmanagementArea, out.width = '100%', fig.height= 5, fig.cap="Fishery management plan boundaries. The Eastern Gulf is often reported at subareas split up be East/West Yakutat and sometimes Southeast.", echo = F, eval = T}
include_graphics(file.path("Figures", "NFMP_with_EGOA.png"))
```

+------------------+---------------------------------------------------------------------------------------+
| Data source      | Description and spatial resolution                            
+==================+=======================================================================================+
| Catch pre 1990   | Available at FMP spatial resolution
|                  | 
+------------------+---------------------------------------------------------------------------------------+
| Catch post 1990  | Available at FMP spatial resolution 
|                  | 
+------------------+---------------------------------------------------------------------------------------+
| Observer data    | Latitude and Longitude positions
|  (Age,Length     | 
|   and  catch)    |
+------------------+---------------------------------------------------------------------------------------+
| Survey data      | Latitude and Longitude positions
|  (Age, Length    | 
|and  catch)       |
+------------------+---------------------------------------------------------------------------------------+
| Tagging data     | Latitude and Longitude for releases, 
|                  | Latitude and longitude for approximately half the recoveries
+------------------+---------------------------------------------------------------------------------------+

Reported catch from commercial fishers seems to be the most limiting data set which has the coarsest reported spatial resolution (**check this statement**). For this reason, the finest spatial resolution we will consider is the fishery management plan boundaries (FMP) being, Bering Sea, Aleutian Islands, Western Gulf, Central Gulf and Eastern Gulf i.e., five area model (Figure \@ref(fig:LLRegions)). Although this was determined by a reporting constraint, this is also the spatial resolution that ABC's are allocated for


In addition to the reported spatial resolution of the data, we need to consider if there is any spatial sampling stratification within a data set. Sampling designs that have spatially varying stratification and thus spatially varying sampling intensity, could be inappropriate to use for alternative spatial stratifications. Often post-stratified estimators are not as efficient or accurate as design based estimators that are consistent with the sampling design. Fortunately in this case, the independent survey design is systematic. This results in consistent spatial sampling with respect to space, and removes this concern when considering alternative regional boundaries.

```{r LLRegions, out.width = '100%', fig.height= 5, fig.cap="The finest spatial resolution that we are considering for Sablefish assessment", echo = F, eval = T}
include_graphics(file.path("Figures", "Longline_areas.png"))
```


## Fishery structure {-}
I used the approach from @lennert2010exploratory to analysis length frequency samples to identify changes in length frequency that may suggest alternative fleet or stock structures. My understanding of the method described in @lennert2010exploratory is that it uses regression trees to identify splits/nodes in the covariates latitude, longitude and season that generate groups/clusters of length frequency distributions that are similar or somewhat homogenous. "Assuming sampling coverage is adequate, comparison of pooled and individual-year tree results can identify spatial structure that is strongly indicated in every year, and that which is only present in select years, perhaps as a result of strong recruitment or changes in catchability, or if sampling coverage is not adequate, sampling variability." 


<!-- Run this exploration and see where the breaks are. This may lead us to reconsider spatial boundaries. **If we assume catchabilities for the fishery are constant over time and space** this exploration may indicate spatial structure in the population. However, that is a strong assumption... We could repeat this for the survey length frequency and see if the pattern is consistent. Just keep in mind that the survey data is from a smaller intra year window. -->


<!-- How can we disentangle whether change are due to changes in fishing practice or change in population dynamics? i.e., recruitment or movement  -->


A limitation for this method for our application is latitude and longitude grids contain unequal areas due to the shrinking of longitudes towards the poles. Also with the unusual shape of the coastline binary splits by latitude and longitude may also be a problem.


```{r observerLFSplitAnalysis, echo = F, eval = 'asis'}
split_table = readRDS(file.path("Data", "LL_LF_split_algorithm.RDS"))
split_table$Var_explained = round(split_table$Var_explained, 2) * 100
colnames(split_table) = c("Variable", "Split value", "Cell", "Variance explained (%)")
kable(split_table, caption = "Longitude splits based on regression tree analysis")
```


```{r observerLFSplit, out.width = '100%',fig.height= 5, fig.cap = "Longitude splits based on regression tree analysis from Table \\@ref(tab:observerLFSplitAnalysis)", echo = F, eval = T}
include_graphics(file.path("Figures", "observer_split_analysis.png"))
```



## Observations {-}

```{r initialspatialObservations, out.width = '100%', fig.height= 6, fig.cap="Observation type and frequency for the initial model.", echo = F, eval = T}
include_graphics(file.path("Figures", "Observation_Frequency.png"))
```


```{r initialcatch, out.width = '100%', fig.height= 5, fig.cap="Catch (kilo tonnes) by gear and area.", echo = F, eval = T}
include_graphics(file.path("Figures", "InputCatches.png"))
```

## First model run {-}


<!--chapter:end:07-SpatialModelConfiguration.Rmd-->

# Tag likelihoods {#taglike}
This section explores a range of tag-recovery likelihoods that have been used in the literature. I simple tag recapture model is used in a simulation to explore how well different likelihoods estimate movement parameters under a range of dispersion conditions. The tag-recapture model ignores age and models number of tagged fish from a release event denoted by \(k\) over three regions indexed by \(r\) for \(n_t\) discrete time-steps. \(T^k_{r,t}\) denotes the number of tagged fish from release event \(k\) in region \(r\) and time step \(t\). The model applies the following processes between time-steps.

\begin{align*}
T^k_{r,t} & = T^k, \quad t = 0\\
T^k_{r,t + 1} & = T^k_{r,t} e^{-Z_t}, \quad t > 0\\
Z_t & = M + F_t \\
\boldsymbol{T}^k_{t + 1} & = \boldsymbol{T}^k_{t + 1} \boldsymbol{M}
\end{align*}
where, \(T^k\) is the initial nunmber of tag releases for release event \(k\), \(\boldsymbol{T}^k_{t + 1} =  (T^k_{1,t + 1},\dots, T^k_{n_r,t + 1})\) is a vector of numbers of tagged fish across all regions and \(\boldsymbol{M}\) is a \(n_r \times n_r\) movement matrix. This assumes each region has the same fishing mortality in each time-step.

## Likelihoods {-}

For all possible tag-recovery events indexed by \(m\) (where \(m = \{r,t\}\) i.e., has an implied region and time-step dimension) the tag-recapture model derives expected tag-recoveries. In this case, any time or region that has any fishing mortality rate is considered a possible tag-recovery event. All the likelihoods in the following sections use the same derivation for expected tag-recoveries, which follows

\[
N^k_{m} = T^k_{r,t} \left(1 - e^{-Z_t}\right) \frac{F_t}{Z_t}, \quad m = \{r,t\} \ .
\]


### Poisson {-}
The Poisson is the simplest likelihood, which was explored by @hilborn1990determination. It assumes

\[
N^k_{m} \sim \mathcal{Poisson}(\widehat{N}^k_{m}) \,
\]

with the total log-likelihood contribution evaluating the Poisson log-likelihood over all possible release and recovery events (\(\sum\limits_k\sum\limits_m \))

### Negative Binomial {-}
A similar likelihood to the Poisson is the Negative Binomial which was explored by @hanselman2015move. This is a more flexible likelihood as it allows for over dispersion, through an additional estimable parameter denoted by \(\phi\)

\[
N^k_{m} \sim \mathcal{NB}(\widehat{N}^k_{m}, \widehat{\phi}) \,
\]

with the total log-likelihood contribution evaluating the Negative binomial log-likelihood over all possible release and recovery events (\(\sum\limits_k\sum\limits_m \))


### Multinomial:release conditioned {-}

The release conditioned multinomial likelihood was explored by @polacheck2006integrating and subsequently by @vincent2020parameter \& @goethel2014demonstration. This treats all recovery events plus an additional not-recovered event as a multinomial distributed event.

\[
\boldsymbol{N}^k \sim \mathcal{Multinomial}\left(\widehat{\boldsymbol{\theta}}^k\right)
\]
where, \(\boldsymbol{N}^k\) is the vector of tag-recoveries for all possible recovery events (\(n_m\)) plus an additional not-recovered event.
\[
\boldsymbol{N}^k = (N^k_1, N^k_2, \dots, N^k_{n_m}, N^k_{NR})
\]
where, \(N^k_{NR}\) represents the not recovered category and is \(N^k_{NR} = T^k - \sum\limits_m^{n_m} N^k_m\), where \(T^k\) is the initial number of tags released for event \(k\). \(\widehat{\boldsymbol{\theta}}^k\) is a vector of proportions that sum to one, with the same dimensions as \(\boldsymbol{N}^k\) and is calculated as,

\[
\widehat{\theta}^k_m = \frac{\widehat{N}^k_{m}}{T^k}, \quad \forall \ m \in (1, \dots,n_m)
\]
where, again \(T^k\) is the initial number of tags released for event \(k\). The proportions for the not recaptured group is calculated as,
\[
\widehat{\theta}^k_{NR} = 1 - \sum_m^{n_m} \widehat{\theta}^k_m 
\]


### Recapture conditioned {-}

The multinomial which is recapture conditioned was described by @vincent2020parameter but based on the original work of @mcgarvey2002estimating (described in the next section). This likelihood evaluates the probability of recapturing a tagged fish in a certain region among all possible regions for a given release event and time-period.


\[
\boldsymbol{N}^k_t \sim \mathcal{Multinomial}\left(\widehat{\boldsymbol{\theta}}_t^k\right)
\]
where, \(\boldsymbol{N}^k_t\) is the vector of tag-recoveries in all regions (\(n_m\)) for release event \(k\) and time period \(t\)
\[
\boldsymbol{N}^k_t = (N^k_{1,t}, N^k_{2,t}, \dots, N^k_{n_r,t})
\]
and model predicted proportions
\[
\widehat{\boldsymbol{\theta}}_t^k = (\widehat{\theta}^k_{1,t}, \widehat{\theta}^k_{2,t}, \dots, \widehat{\theta}^k_{n_r,t})
\]
and,

\[
\widehat{\theta}^k_{r,t} = \frac{\widehat{\theta}^k_{r,t}}{\sum\limits_r \widehat{\theta}^k_{r,t}}
\]


The literature [@mcgarvey2002estimating;@vincent2020parameter] often describes the log likelihood as

\[
ll = \sum_k\sum_y\sum_r log(\widehat{\theta}^k_{r,t}) \times N^k_{r,t}
\]



## Simple simulation {-}
To explore these likelihoods are very simple simulation was setup to see how well each likelihood could estimate elements of \(\boldsymbol{M}\) under different levels of over dispersion. The simulation assumes all tags encountered where reported (100\% reporting rate) and both natural mortality and fishing mortality are known without error. The only estimated parameters were \(\boldsymbol{M}\) and \(\phi\) when the negative binomial likelihood was explored.


The following is some R-code that was used to generate operating model values for 
```{r simulate_tag_data}
set.seed(123)
n_y = 5
n_regions = 3
M = 0.13
F_y = rlnorm(n = n_y, log(0.2), 0.6)
Z_y = F_y + M
S_y = exp(-Z_y)
# make up some random movement matrix
move_matrix = matrix(0, ncol = n_regions, nrow = n_regions)
diag(move_matrix) = c(0.7, 0.5, 0.6)
move_matrix = move_matrix + rnorm(n = n_regions * n_regions, 0.1,0.05)
move_matrix = sweep(move_matrix, MARGIN = 1, STATS = rowSums(move_matrix), FUN = "/")
## seed tag releases
tag_release_by_region = rep(1000, n_regions)
n_release_events = n_regions
## Calculate tag-partition
tags_partition_by_release_event = array(0, dim = c(n_regions, n_y + 1, n_release_events))
expected_tag_recoveries  = tag_recovery_obs = array(0, dim = c(n_regions, n_y, n_release_events))
for(r in 1:n_release_events)
  tags_partition_by_release_event[r,1,r] = tag_release_by_region[r]
for(y in 1:n_y) {
  for(rel_event in 1:n_release_events) {
    ## ageing and F
    tags_partition_by_release_event[,y + 1, rel_event] = 
      tags_partition_by_release_event[,y,rel_event] * S_y[y]
    ## Movement
    tags_partition_by_release_event[,y + 1, rel_event] = 
      tags_partition_by_release_event[,y + 1, rel_event] %*% move_matrix
  }
}

## Create tag-recovery expected values
for(rel_event in 1:n_regions) {
  expected_tag_recoveries[,,rel_event] = 
    sweep(tags_partition_by_release_event[ ,2:(n_y + 1),rel_event], MARGIN = 2, STATS =  F_y / Z_y * (1 - S_y), FUN = "*")
}
```


The following code chunk assumes `tags_partition_by_release_event` is the same among simulations, but uses the negative binomial with a range of over-dispersion parameters to simulate tag-recoveries for `n_sim` times. We explore Relative error (RE) and mean square error (MSE) for the different likelihood choices.

```{r loadtagTMBmodels, echo = F, eval = T, results = 'hide', warning=FALSE, message=FALSE, error = FALSE}
compile(file.path("TMB","SimpleTagEstimator.cpp"), flags = "-Wignored-attributes -O3",DLLFLAGS="");
dyn.load(dynlib(file.path("TMB","SimpleTagEstimator")))
```

```{r OM_EM}
n_sim = 500
## Populate TMB objects
data = list()
data$n_y = n_y
data$n_regions = n_regions
data$M = M
data$F_y = F_y
data$tag_likelihood = 0 ## Poisson
data$tag_recovery_obs = tag_recovery_obs
data$number_of_tags_released = tag_release_by_region
data$movement_transformation = 0 ## simplex, can use the logistic transformation as well
## starting values
est_parameters = list()
start_matrix = matrix(0, ncol = n_regions, nrow = n_regions)
diag(start_matrix) = c(0.95, 0.95, 0.95)
start_matrix = start_matrix + rnorm(n = n_regions * n_regions, 0.02,0.0001)
## force to sum = 1
start_matrix = sweep(start_matrix, MARGIN = 1, STATS = rowSums(start_matrix), FUN = "/")

est_parameters$transformed_movement_pars = matrix(NA, nrow = n_regions - 1, ncol = n_regions)
for(i in 1:n_regions)
  est_parameters$transformed_movement_pars[,i] = simplex(start_matrix[i,])
est_parameters$ln_phi = log(1)
## don't estimate phi parameter unless Negative binomial
na_map = fix_pars(par_list = est_parameters, pars_to_exclude = "ln_phi")

OM_mat = melt(move_matrix)
colnames(OM_mat) = c("From", "To","proportion")
OM_mat$type = "OM"
```



```{r runsimulationTagLike, warning=FALSE, message=FALSE, error = FALSE, cache=TRUE}
############################################
## over_diserpsion_investication
## what happens when we simualte from
## negative binomial with different 
## over-dispersion parameters
############################################

over_dispersion_params = c(0.1, 0.2, 0.3, 0.5, 1, 10)
n_sim = 500
# use the simplex
data$movement_transformation = 0
sim_data_info = NULL
complete_df = NULL
for(over_ndx in 1:length(over_dispersion_params)) {
  this_dispersion = over_dispersion_params[over_ndx]
  full_move_df = NULL;
  temp_proportion_zeros_sim_data = temp_variance_sim_data = c()
  for(sim in 1:n_sim) {
    ## keep expected values the same among simulations
    for(rel_event in 1:n_release_events) {
      for(y in 1:n_y) {
        for(r in 1:n_regions) {
          tag_recovery_obs[r,y,rel_event] = rnbinom(n = 1, mu = expected_tag_recoveries[ r,y,rel_event], size = this_dispersion)
        }
      }
    }
    ## summarise number of zeros and variance of observed data
    temp_proportion_zeros_sim_data = c(temp_proportion_zeros_sim_data, sum(tag_recovery_obs == 0));
    temp_variance_sim_data = c(temp_variance_sim_data, var(tag_recovery_obs));
    ## save data
    data$tag_recovery_obs = tag_recovery_obs

    ## Poisson
    data$tag_likelihood = 0
    est_obj <- MakeADFun(data, est_parameters, map = na_map, DLL="SimpleTagEstimator", silent = T)
    est_obj$env$tracepar = F
    opt_poisson = nlminb(est_obj$par, est_obj$fn, est_obj$gr, control = list(iter.max = 10000, eval.max = 10000))
    poisson_rep = est_obj$report(opt_poisson$par)
    sd_poisson = sdreport(est_obj)

    ## Multinomial release conditioned
    data$tag_likelihood = 1
    est_obj <- MakeADFun(data, est_parameters, map = na_map, DLL="SimpleTagEstimator", silent =T)
    opt_multi_release = nlminb(est_obj$par, est_obj$fn, est_obj$gr, control = list(iter.max = 10000, eval.max = 10000))
    multi_release_rep = est_obj$report(opt_multi_release$par)
    sd_multi_release = sdreport(est_obj)

    ## Multinomial recapture conditioned
    data$tag_likelihood = 2
    est_obj <- MakeADFun(data, est_parameters,map = na_map,  DLL="SimpleTagEstimator", silent = T)
    opt_multi_recap = nlminb(est_obj$par, est_obj$fn, est_obj$gr, control = list(iter.max = 10000, eval.max = 10000))
    multi_recap_rep = est_obj$report(opt_multi_recap$par)
    sd_multi_recap = sdreport(est_obj)

    ## recapture conditioned
    data$tag_likelihood = 3
    est_obj <- MakeADFun(data, est_parameters,map = na_map,  DLL="SimpleTagEstimator", silent = T)
    opt_recap = nlminb(est_obj$par, est_obj$fn, est_obj$gr, control = list(iter.max = 10000, eval.max = 10000))
    recap_rep = est_obj$report(opt_recap$par)
    sd_recap = sdreport(est_obj)
    ## Negative binomial
    data$tag_likelihood = 4
    est_obj <- MakeADFun(data, est_parameters,  DLL="SimpleTagEstimator", silent = T)
    opt_nb = nlminb(est_obj$par, est_obj$fn, est_obj$gr, control = list(iter.max = 10000, eval.max = 10000))
    nb_rep = est_obj$report(opt_nb$par)
    sd_nb = sdreport(est_obj)
    ## reformat movement estimates
    pois_mat = melt(poisson_rep$movement_matrix)
    pois_mat$RE = (pois_mat$value - OM_mat$proportion) / OM_mat$proportion * 100
    pois_mat$SE = (OM_mat$proportion - pois_mat$value)^2
    pois_mat$type = "Poisson"
    pois_mat$sim = sim
    recap_mat = melt(recap_rep$movement_matrix)
    recap_mat$RE = (recap_mat$value - OM_mat$proportion) / OM_mat$proportion * 100
    recap_mat$SE = (OM_mat$proportion - recap_mat$value)^2
    recap_mat$type = "Recapture"
    recap_mat$sim = sim
    multi_release_mat = melt(multi_release_rep$movement_matrix)
    multi_release_mat$RE = (multi_release_mat$value - OM_mat$proportion) / OM_mat$proportion * 100
    multi_release_mat$SE = (OM_mat$proportion - multi_release_mat$value)^2
    multi_release_mat$type = "Multinomial Release"
    multi_release_mat$sim = sim
    nb_mat = melt(nb_rep$movement_matrix)
    nb_mat$RE = (nb_mat$value - OM_mat$proportion) / OM_mat$proportion * 100
    nb_mat$SE = (OM_mat$proportion - nb_mat$value)^2
    nb_mat$type = "Negative Binomial"
    nb_mat$sim = sim
    full_move_df = rbind(full_move_df, rbind(pois_mat, recap_mat, multi_release_mat, nb_mat))
  }
  prop_obs_zero = temp_proportion_zeros_sim_data / (dim(data$tag_recovery_obs)[1] * dim(data$tag_recovery_obs)[2] * dim(data$tag_recovery_obs)[3])
  tmp_df = data.frame(avg_proportion_zero = mean(prop_obs_zero), avg_var = mean(temp_variance_sim_data), over_dispersion_param = this_dispersion)
  sim_data_info = rbind(sim_data_info, tmp_df)
  full_move_df$over_dispersion_param = this_dispersion
  complete_df = rbind(complete_df, full_move_df)
}
colnames(complete_df) = c("From", "To","proportion", "RE", "SE", "type", "sim", "over_dispersion_param")
```


```{r overdispersiontable, echo = F, eval = T, warning=FALSE, message=FALSE, error = FALSE}
kable(sim_data_info, caption = "Average proportion of zeros and variance of simulated data for different overdispersion settings.", digits  = 3)
```


```{r taglikesummaryplotsRE, echo = F, eval = T, warning=FALSE, message=FALSE, error = FALSE, fig.cap="Relative error in movement parameters. Columns are overdispersion values for simulated data. GGplot facets are movement matrix elemen combinatons", fig.height=10}
ggplot(complete_df, aes(y = RE, col = type, x = type)) +
  geom_boxplot() +
  ylim(-100, 150) +
  labs(x = "", y= "Relative error (%)", col = "Likelihood") +
  geom_hline(yintercept = 0, col = "red", linetype = "dashed") +
  theme_bw() +
  facet_grid(From~To~over_dispersion_param) +
  theme(axis.text.x = element_blank())
```


```{r taglikesummaryplotsEst, echo = F, eval = T, warning=FALSE, message=FALSE, error = FALSE, fig.cap="Estimated movement parameters, red line is true value. Columns are overdispersion values for simulated data. GGplot facets are movement matrix elemen combinatons", fig.height=10}

ggplot(complete_df) +
  geom_boxplot(aes(y = proportion, col = type, x = type)) +
  geom_hline(data =OM_mat, aes(yintercept = proportion), col = "red", linetype = "dashed", linewidth = 1.1) +
  ylim(0, 1) +
  labs(x = "", y= "Estimated Movement parameters", col = "Likelihood") +
  theme_bw() +
  facet_grid(From~To~over_dispersion_param) +
  theme(axis.text.x = element_blank())


```


```{r taglikesummaryplotsMSE, echo = F, eval = T, warning=FALSE, message=FALSE, error = FALSE, fig.cap="Mean squared error (MSE) over all simulations and estimated parameters."}

ggplot(complete_df %>% group_by(type, over_dispersion_param) %>% summarise(MSE = sum(SE) / n_sim)) +
  geom_col(aes(y = MSE, col = type, fill = type, x= type)) +
  facet_wrap(~over_dispersion_param) +
  theme_bw() +
  labs(x = "", col = "Likelihood", fill = "Likelihood", y = "MSE") +
  theme(axis.text.x = element_blank())
```

<!--chapter:end:14-TagLikelihoods.Rmd-->

# Growth {#growth}

Growth is an important input in stock assessments. Although we are not considering spatial varying growth in the spatial stock assessments due to computational reasons. We still want to look at the data to see if growth is significantly different between the spatial regions under consideration.



## Growth estimation using the Laslett–Eveson–Polacheck (LEP) method  {-}
Another use of tag-recovery data is for estimating growth models which are important inputs for stock assessment models. The section explores the “Laslett–Eveson–Polacheck (LEP)” approach, based on @laslett2002flexible \& @eveson2004integrated and described in @aires2015improved.


The idea is to use a single growth model that fits to two observational data sets (ideally within the assessment, but for now this analysis is independent of the assessment). The two data sets are are age and length observations and tag-increment observations from tagging experiments. A benefit of this method, other than estimating growth is that estimates of ages of fish at release are a derivative that can then be used as inputs for tag releases in an age-structured stock assessment model (what to do with observations with no sex information?).


### Age-at-length growth model {-}

The Richards growth curve was assumed as was done in @aires2015improved (but this could be extended). The Richards growth formulation follows

\begin{equation} 
  \bar{l}_{a} = L_{\infty} \left( 1 + \frac{1}{p} \exp \{-K(a - a_0)\}\right)^{-p}
  (\#eq:richardsgrowth)
\end{equation} 
where, \( \bar{l}_{a}\) is the mean length at age \(a\), \(L_{\infty}\) is the asymptotic length, \(K\) is the growth coefficient and \(p\) is a shape parameter that is related to the ratio \(\bar{l}_{a} / L_{\infty}\) at the inflexion point.

### Tag recapture growth data {-}
+----------------------------+-----------------------------------------------------------------------------+
| Symbol                     | Description                        
+============================+=============================================================================+
| \(l_{1,i}\)                | length of individual \(i\) at release
+----------------------------+-----------------------------------------------------------------------------+
| \(l_{2,i}\)                | length of individual \(i\) at recapture
+----------------------------+-----------------------------------------------------------------------------+
| \(a_{1,i}\)                | age of individual \(i\) at release. Denoted as \(A\) in @aires2015improved
+----------------------------+-----------------------------------------------------------------------------+
| \(a_{2,i}\)                | age of individual \(i\) at recapture
+----------------------------+-----------------------------------------------------------------------------+
| \(\Delta_t\)               | Time at liberty \(\Delta_t = a_{2,i} - a_{1,i}\)      
+----------------------------+-----------------------------------------------------------------------------+
**Just one comment on notation!!!** in most all the papers that use this method, they ignore the individual notation of \(A\). That is not an issue in general however it confuses me when they describe the prior on this.


The sub-models for the release and recapture lengths follow,
\begin{equation} 
  l_{1,i} = L_{\infty} \left( 1 + \frac{1}{p} \exp \{-K(a_{1,i} - a_0)\}\right)^{-p}
  (\#eq:lengthatrelease)
\end{equation} 

and,
\begin{equation} 
  l_{2,i} = L_{\infty} \left( 1 + \frac{1}{p} \exp \{-K(a_{1,i} + \Delta_t - a_0)\}\right)^{-p}
  (\#eq:lengthatrecapture)
\end{equation} 


The above growth model assumes that we know the age at recovery \(a_{2,i}\). The problem we have is, we have 22 569 tag recoveries with length information but only a handful of these have been aged. This is dealt with by modelling \(a_{1,i}\) as a random effect i.e., \(a_{1,i} \sim LN \left(\mu, \sigma^2\right)\).

What confuses is me here is how to assign a hyper distribution like the one above for the random effect variables \(a_{1,i}\). \(a_{1,i}\) is expected to vary quite a bit because tagged fish at release have a broad length frequency and thus is expected to have a broad age at release? (ask someone about this because I may be misunderstanding something).

## Simulation test the "LEP" method {-}

```{r, simulate_data, echo = T, eval = T}
## going to use parameters from Aires-da-Silva et al. (2015) Table 1 integrated analysis
L_inf = 200.8
k = 0.44
t_0 = 1.26
p = -4.27
cv = 0.15 ## cv of length around length at age

## selectivity paraemters
sel_a50 = 1.3
sel_ato95 = 0.8
## sample sizes for simulation
n_sample_release = 1000
n_sample_recoveries = 1000 * 0.1 ## about the recovery rate from sablefish data
n_sample_age_length = 1000

## generate a pseudo age structure
set.seed(123)
R_0 = 200000
M = 0.29
ages = 1:20
n_ages = length(ages)
#plot(ages, mean_length_at_age, type = "l", lwd = 3, xlab = "Age", ylab = "Length (cm)")
sel_at_age = logis(ages, sel_a50, sel_ato95)

## numbers at age
N_age = vector(length = n_ages, "numeric")
for (age_ndx in 1:n_ages) 
  N_age[age_ndx] = R_0 * exp(-ages[age_ndx] * M) * exp(rnorm(1,0,0.7))

N_age = N_age * sel_at_age ## vulnerable numbers at age

plot(ages, N_age, ylab = "Numbers", xlab = "Age", main = "Population age-structure", type = "o")

## plot growth
mean_length_at_age = richards_growth(ages, p, k, t_0, L_inf)
## randomly sample 1000 individuals with replacement for otolithing no ageing error!!
## 
individual_age_length_df = NULL
for(i in 1:n_sample_age_length) {
  age_i = sample(1:n_ages, size = 1, prob = N_age)
  mean_length_i = richards_growth(age_i, p, k, t_0, L_inf)
  length_i = rnorm(1, mean_length_i, mean_length_i * cv)
  temp_df = data.frame(age = age_i, length = length_i)
  individual_age_length_df = rbind(individual_age_length_df, temp_df)
}

## Simulate a tag-recapture experiment 
# releases
release_ages = sample(1:n_ages, size = n_sample_release, prob = N_age, replace = T)
release_mean_lengths = richards_growth(release_ages, p, k, t_0, L_inf)
release_lengths = rnorm(n_sample_release, release_mean_lengths, release_mean_lengths * cv)
individual_release_df = data.frame(release_age = release_ages, release_length = release_lengths, release_mean_length = release_mean_lengths)
individual_release_df$fish_id = 1:nrow(individual_release_df)

# recaptures sample uniformly without replacement
fish_ndx = sample(1:nrow(individual_release_df), size = n_sample_recoveries, replace = F) 
individual_recovery_df = subset(individual_release_df, subset = individual_release_df$fish_id %in% fish_ndx)
## time-at liberty days randomly recovered on average between 100-600 days
individual_recovery_df$time_at_liberty = rpois(n = n_sample_recoveries, lambda = runif(n_sample_recoveries,100,600))
individual_recovery_df$recovery_age = individual_recovery_df$release_age + individual_recovery_df$time_at_liberty/365
## how to add the length increment between release and recovery?
individual_recovery_df$recovery_mean_length = richards_growth(individual_recovery_df$recovery_age, p, k, t_0, L_inf)
individual_recovery_df$recovery_mean_length_increment = with(individual_recovery_df, recovery_mean_length - release_mean_length)
individual_recovery_df$recovery_length = with(individual_recovery_df, release_length + rlnorm(n_sample_recoveries, log(recovery_mean_length_increment), cv))
individual_recovery_df$growth_change = individual_recovery_df$recovery_length - individual_recovery_df$release_length 

## visualise length at age samples
ggplot(individual_age_length_df, aes(x = age, y = length)) +
  geom_point() +
  geom_line(data= data.frame(length = mean_length_at_age, age = ages), aes(x = age, y = length), col = "red", linewidth = 1.2, inherit.aes = F) +
  labs(x = "Age", y = "Length") +
  ylim(0,NA)
```

Assumptions in the above OM follow.

\[
l_{1,i} \sim \mathcal{N} \left(\bar{l}_{1,a}, \sigma = \bar{l}_{1,a} \times cv\right)
\]
where the mean length at age release (\(\bar{l}_{1,a}\)) follows the Richards growth curve defined in Equation \@ref(eq:richardsgrowth). The age used to derive the mean length at age was a random sample with replacement from the population in shown in the earlier figure. Time at liberty was drawn from a Poisson distribution with a rate parameter randomly drawn from a uniform distribution between 100-600 days. The age at recovery \(a_{2,i} = a_{1,i} + \Delta_t\). An approximation was made when calculating the length at recovery. The length increment (\(l_{\Delta_t}\)) was simulated using a Log Normal distribution with the median set based on the difference between mean length at release age and mean length at recovery age. This was to ensure all recovered fish positively grew at a rate expected by the growth model (less than ideal but will do for now).
\[
l_{i,\Delta_t} \sim \mathcal{LN} \left(\ln (\bar{l}_{a_{2,i}} - \bar{l}_{a_{1,i}}), \sigma = cv\right)
\]

\[
l_{2,i} = l_{1,i} + l_{i, \Delta_t}
\]
In theory we can now pass this data to our LEP model to back estimate growth parameters.


```{r,compile_TMB_model, echo = T, results = 'hide', eval = T}
setwd(file.path("TMB"))
#sink(file = "compile_output.txt")
compile(file = "LEPgrowth_model.cpp", flags = "-Wignored-attributes -O3")
#sink()
dyn.load(dynlib("LEPgrowth_model"))
#setwd(DIR$book)
```


```{r, build_TMB_model, echo = T, eval = T, results = 'hide'}
# data
data = list()
data$ages_from_age_length = individual_age_length_df$age
data$lengths_from_age_length = individual_age_length_df$length
data$lengths_at_release = individual_recovery_df$release_length
data$lengths_at_recovery = individual_recovery_df$recovery_length
data$time_at_liberty = individual_recovery_df$time_at_liberty / 365
data$ages_for_report = ages;

data$p_bounds = c(-20, 20)
data$t0_bounds = c(-6, 4)

# parameters
parameters = list()
parameters$ln_cv_length_at_age = log(cv)
parameters$ln_k = log(k)
parameters$ln_L_inf = log(L_inf)
parameters$logit_p =  logit_general(p, data$p_bounds[1],data$p_bounds[2])
parameters$logit_t0 =  logit_general(t_0, data$t0_bounds[1],data$t0_bounds[2])
parameters$ln_cv_length_release = log(0.1)
parameters$ln_cv_length_recovery = log(0.1)
parameters$ln_age_at_release = log(individual_recovery_df$release_age)

parameters$ln_mu_age_release = log(3)
parameters$ln_sd_age_release = log(1)

obj_mixed_all <- MakeADFun(data, parameters, random = "ln_age_at_release", DLL="LEPgrowth_model")
```


```{r, optimise_TMB_model, echo = T, eval = T, results = 'hide'}
MLE_mixed_all = nlminb(start = obj_mixed_all$par, objective = obj_mixed_all$fn, gradient  = obj_mixed_all$gr)
MLE_mixed_all$convergence
MLE_mixed_all_rep = obj_mixed_all$report(obj_mixed_all$env$last.par.best)
MLE_mixed_all_sd_rep = sdreport(obj_mixed_all)
plot(ages, MLE_mixed_all_rep$mean_length_at_age, type = "l", lwd = 3, col = "red", xlab = "Age", ylab = "Length (cm)", ylim = c(0, 240))
lines(ages, mean_length_at_age, col = "blue", lty = 3, lwd = 3)
legend('bottomright', legend = c("LEP obs error","True"), col = c("red", "blue"), lty = c(1,2), lwd = 3)

```


## Next steps {-}

- Check sensitivity to the model to starting parameters 
- Repeat with different sample sizes
- Repeat with a truncated age-structure for the age-length data using a selectivity
- Look at Sablefish data and see if the LEP approach can be used to it
- Explore alternative growth models i.e., @schnute1981versatile





<!--chapter:end:15-Growth.Rmd-->

# Review/summarise reference points {#refpoints}

Most of my experience with reference points are spawning biomass related i.e., \(SSB_y/SSB_0\). However, many of the reference points outside of New Zealand are \(F\) based i.e., \(F_{35\%}\) which I don't really understand. The purpose of this section is to define them mainly to help me understand, but also as a reference for when I forget in the future.


+----------------------------+-----------------------------------------------------------------------------+
| Symbol                     | Description/Calculation                            
+============================+=============================================================================+
| \(SSB^{\%B_0}_{y}\)        | *Percent \(B_0\)* \(= \frac{SSB_y}{B_0}\)
+----------------------------+-----------------------------------------------------------------------------+
| \(SPR\)                    | *Spawner per recruit* a measure/proxy of population fecundity
+----------------------------+-----------------------------------------------------------------------------+
| \(F_{35\%spr}\)            | *\(F\) 35\%* The Fishing mortality that results in a 35\% SPR
+----------------------------+-----------------------------------------------------------------------------+
| \(SPR_{msy}\)              | *SPR at MSY*
+----------------------------+-----------------------------------------------------------------------------+





<!--chapter:end:16-ReferencePoints.Rmd-->

# Sex ratios in age and length composition data {#sexratios}

An immediate improvement in the current stock assessment (Chapter \@ref(modeldescription)) relates to how sexually disaggregated compositional data are handled. Currently LF's are separated by sex (needed for the different growths) and age compositional data are aggregated over both sexes. Although LFs are disaggregated by sex and should (in theory) be sufficient to estimate sex specific selectivities. It is not ideal to use LFs to estimate age-based selectivities because older cohorts "smush" into single modes and age information is lost.


Before developing a spatially explicit model stock assessment model, I wanted to tidy some loose ends that may come back and bite me in the proverbial butt once we explore the spatially sex disaggregated stock assessment model in anger. These things are exponentially more easier to deal with in "simpler" models. My general intention is to drop length data when we have well sampled age data, currently they both go into the model together. For age composition data I want to structure the observations so that there is potential information on sex ratio. The current assessment can be given information on sex ratio for generating model predicted values which is similar to the approach in @ward2019assessing. Howver, I personally think this should be dealt within the model.


I am aware of two approaches for supplying observations that in theory should provide information on sex ratio. The first ("Approach 1") was taken from Casal2 [@doonan2016casal2]. This treats sexed composition data for a year as a single proportion i.e., proportions across all ages and sexes sum to one for each year,

\[
\boldsymbol{P}^k_{y} = \frac{(C^k_{a,y,1},C^k_{a,y,2})}{\sum_a \sum_s C^k_{a,y,s}}, \quad \sum \boldsymbol{P}^k_{y} = 1
\]
where, \(C^k_{a,y,1}\) is the catch at age (numbers) for males in year \(y\), age \(a\) and survey \(k\). \(\boldsymbol{P}^k_{y}\) is a proportion vector that sums to one that covers both sexes and corresponding ages. The likelihood contribution for this approach follows  
\[
\boldsymbol{P}^k_{y} \sim Multinomial(\mathbb{E}[\boldsymbol{P}^k_{y}], N^{eff, k}_{y}) \ .
\]
where, \(N^{eff, k}_{y}\) is the effective sample size for this survey and year.


The second approach ("Approach 2") is to treat composition for each sex seperately that is proportions at age or length for a sex will sum to one, but also provide a specific sex ratio observation over all ages or lengths as done in the New Zealand rock lobster stock assessment [@webber2021rapid]. 

\[
R^k_{y,s} = \frac{\sum_a C^k_{a,y,s}}{\sum_a \sum_s C^k_{a,y,s}}, \quad  \sum_s R^k_{y,s} = 1
\]
and,
\[
P^k_{a, y,s} = \frac{C^k_{a,y,s}}{\sum_a C^k_{a,y,s}}, \quad  \sum_a P^k_{a, y,s} = 1 \ .
\]
The likelihood assumptions for this model are,
\[
R^k_{y,s} \sim Binomial(\mathbb{E}[R^k_{y,s}], \sum_s N^{eff, k}_{y,s})
\]
and,
\[
\boldsymbol{P}^k_{y,s} \sim Multinomial(\mathbb{E}[\boldsymbol{P}^k_{a, y,s}], N^{eff, k}_{y,s}) \ .
\]
where, \(N^{eff, k}_{y,s}\) is the effective sample size.


## A simple simulation {-}
To explore the utility of these two approaches we conducted a simple simulation using a sexually disaggregated age-structured model (see simulation section below). The Operating Model (OM) used in the simulation assumed a 50:50 sex ratio during the recruitment process however the OM did assume different growth and selectivity among the sexes.

### Results {-}
In summary both approaches resulted in similar estimates in selectivities and SSBs. I prefer Approach 1 as it is a single observation compared with Approach 2 which has two observation types (need to consider how to avoid double counting of samples), but this simulation showed at least within the very small assumptions explored here they resulted in similar performance. This was all I was after in order to move forward to the spatial model.


Another consideration about "Approach 2" is if the selectivity shape differs between sexes i.e., lower age at 50\% retention. Then due to the sex ratio being derived by summing numbers over all ages, and the natural exponential decay in successive age-cohorts, selectivities that capture more younger fish will result in higher sex ratios. "Approach 1"

### Future research that was not considered in this simple simulation {-}
- LF observations with sexual dimorphism in growth. A possible reason LF's could be influencing assessment output is due to mis-specified growth à la @minte2017get 
- Sample size between the two approaches
- Actual sex ratio skewed in the recruitment process, rather than just a selectivity effect which was of focus in the following simulation
- Look at the effect on \(F\)'s I only summarised SSB and selectivities
- Look at some residuals to see if that is a way to discredit some of these models
### Operating Model (OM) Parameters  {-}

```{r setup}
set.seed(123)
n_sims = 5

bio_params = list(
  ages = 1:20,
  L_inf_m = 58,
  K_m = 0.133,
  t0_m = 0,
  L_inf_f = 62,
  K_f = 0.143,
  t0_f = 0,
  M = 0.15,
  a_m = 2.18e-9, ## tonnes
  b_m = 3.2,
  a_f = 2.08e-9, ## tonnes
  b_f = 3.3,
  m_a50 = 2.3,
  m_ato95 = 1.2,
  sigma = 0.6,
  h = 0.85,
  sigma_r = 0.6,
  R0 = 8234132,
  plus_group = 1 # 0 = No, 1 = Yes
)

other_params = list(
  s_a50_m = 3.6,
  s_ato95_m = 2,
  s_a50_f = 4.6,
  s_ato95_f = 1.4,
  s_q = 0.2,
  s_alpha = 1,
  f_a50_m = 4.2,
  f_ato95_m = 1.17,
  f_a50_f = 4.7,
  f_ato95_f = 1.76,
  f_alpha = 1,
  ssb_prop_Z = 0.5,
  survey_prop_Z = 0.5,
  survey_age_error = c(0.5, 0.4),  ## sd, rho (ignored if iid)
  fishery_age_error = c(0.5, 0.4),  ## sd, rho (ignored if iid)
  survey_bio_cv = c(0.1)
)

ages = bio_params$ages
max_age = max(bio_params$ages)
n_years = 30
years = (2020 - n_years + 1):2020
n_ages = length(ages)
## annual fishing mortality
start_F = c(rlnorm(10, log(seq(from = 0.02, to = 0.1, length = 10)), 0.1), rlnorm(10, log(0.1), 0.1), rlnorm(10, log(0.07), 0.1))
recruit_devs = log(rlnorm(n_years, -0.5 * bio_params$sigma_r * bio_params$sigma_r, bio_params$sigma_r))
```


```{r Buildtmpsex, echo = F, eval = T, results = 'hide', warning=FALSE, message=FALSE, error = FALSE}
##################
## Plot Biology
##################
length_at_age_m = vonbert(bio_params$ages, bio_params$K_m, bio_params$L_inf_m, bio_params$t0_m)
length_at_age_f = vonbert(bio_params$ages, bio_params$K_f, bio_params$L_inf_f, bio_params$t0_f)
fishery_ogive_m = logis(bio_params$ages, other_params$f_a50_m, other_params$f_ato95_m)
fishery_ogive_f = logis(bio_params$ages, other_params$f_a50_f, other_params$f_ato95_f)

survey_ogive_m = logis(bio_params$ages, other_params$s_a50_m, other_params$s_ato95_m)
survey_ogive_f = logis(bio_params$ages, other_params$s_a50_f, other_params$s_ato95_f)
mat_age = logis(bio_params$ages, bio_params$m_a50, bio_params$m_ato95)
weight_at_age_m = bio_params$a_m * length_at_age_m^bio_params$b_m
weight_at_age_f = bio_params$a_m * length_at_age_f^bio_params$b_f

## can change these if we want biennial surveys, or truncated observatons.
## Using the ALN method doesn't allow zeros so consider truncating tail ages.
survey_year_obs = seq(from = min(years), to = max(years), by = 2)
survey_ages = ages
fishery_year_obs = seq(from = min(years) + 1, to = max(years), by = 2)
fishery_ages = ages

############
## Build a multinomial model to double check estimability of all parameters
## In this case we have 'good' data, annual data, no ageing error.
##  survey index cv = 0.05
##  year effective sample size = 1000
############
TMB_data = list()
TMB_data$ages = ages
TMB_data$maxAgePlusGroup = 1;
TMB_data$years = years
TMB_data$nyears = length(TMB_data$years)
TMB_data$nages = length(TMB_data$ages)

## No ageing error
TMB_data$ageing_error_matrix = matrix(0, nrow = TMB_data$nages, ncol = TMB_data$nages)
diag(TMB_data$ageing_error_matrix) = 1;

TMB_data$survey_year_indicator = as.integer(TMB_data$years %in% survey_year_obs)
TMB_data$survey_obs = rnorm(sum(TMB_data$survey_year_indicator), 100, 4)
TMB_data$survey_cv = rep(0.15,sum(TMB_data$survey_year_indicator))
TMB_data$survey_sample_time = rep(0.5,sum(TMB_data$survey_year_indicator))
TMB_data$survey_AF_obs = array(2, dim = c(n_ages * 2, sum(TMB_data$survey_year_indicator)))
TMB_data$survey_AF_type = 0;
TMB_data$survey_numbers_male = colSums(TMB_data$survey_AF_obs)/ 2
TMB_data$fishery_year_indicator = as.integer(TMB_data$years %in% fishery_year_obs)
TMB_data$fishery_AF_obs = array(2, dim = c(n_ages * 2, sum(TMB_data$fishery_year_indicator)))
TMB_data$fishery_AF_type = 0;
TMB_data$fishery_numbers_male = colSums(TMB_data$fishery_AF_obs)/ 2
TMB_data$catches = rep(1000, n_years)# this will be overriden in the simulate() call
TMB_data$ycs_estimated = rep(1, n_years)
TMB_data$standardise_ycs = 0;

TMB_data$catchMeanLength = TMB_data$stockMeanLength = array(0, dim = c(TMB_data$nages, TMB_data$nyears,2))
TMB_data$catchMeanLength[,,1] = TMB_data$stockMeanLength[,,1] = matrix(length_at_age_m, byrow = F, ncol = TMB_data$nyears, nrow = TMB_data$nages)
TMB_data$catchMeanLength[,,2] = TMB_data$stockMeanLength[,,2] =  matrix(length_at_age_f, byrow = F, ncol = TMB_data$nyears, nrow = TMB_data$nages)

TMB_data$prop_mature = matrix(mat_age, byrow = F, ncol = TMB_data$nyears, nrow = TMB_data$nages)
TMB_data$natMor = bio_params$M
TMB_data$steepness = bio_params$h
TMB_data$stockRecruitmentModelCode = 2 ## BH
TMB_data$propZ_ssb = rep(other_params$ssb_prop_Z, TMB_data$nyears)
TMB_data$propZ_survey = rep(other_params$survey_prop_Z, TMB_data$nyears)
TMB_data$sel_ato95_bounds = c(0.1,20)
TMB_data$sel_a50_bounds = c(0.1,20)
TMB_data$sel_alpha_bounds = c(0.5, 2)

TMB_data$mean_weight_a = c(bio_params$a_m,bio_params$a_f) 
TMB_data$mean_weight_b = c(bio_params$b_m,bio_params$b_f)


## The same parameters as OM, to check for consistency
true_pars = list(
  ln_R0 = log(bio_params$R0),
  ln_ycs_est =  log(exp(recruit_devs[TMB_data$ycs_estimated == 1] - 0.5*bio_params$sigma_r^2)),
  ln_sigma_r = log( bio_params$sigma_r),
  ln_extra_survey_cv = log(0.0001),
  logit_f_a50 = logit_general(c(other_params$f_a50_m, other_params$f_a50_f), TMB_data$sel_a50_bounds[1], TMB_data$sel_a50_bounds[2]),
  logit_f_ato95 = logit_general(c(other_params$f_ato95_m, other_params$f_ato95_f), TMB_data$sel_ato95_bounds[1], TMB_data$sel_ato95_bounds[2]),
  logit_f_alpha_f = logit_general(other_params$f_alpha, TMB_data$sel_alpha_bounds[1], TMB_data$sel_alpha_bounds[2]),
  logit_survey_a50 = logit_general(c(other_params$s_a50_m, other_params$s_a50_f), TMB_data$sel_a50_bounds[1], TMB_data$sel_a50_bounds[2]),
  logit_survey_ato95 = logit_general(c(other_params$s_ato95_m, other_params$s_ato95_f), TMB_data$sel_ato95_bounds[1], TMB_data$sel_ato95_bounds[2]),
  logit_surveyQ = qlogis(other_params$s_q),
  logit_survey_alpha_f = logit_general(other_params$s_alpha, TMB_data$sel_alpha_bounds[1], TMB_data$sel_alpha_bounds[2]),
  ln_F = log(start_F),
  logit_proportion_male = rep(0, TMB_data$nyears),
  ln_catch_sd = log(0.02)
)

ran_start_vals = function() {
  start_params = list()
  start_params$ln_R0 = ran_start(n = 1, LB = log(bio_params$R0 * 0.2), UB = log(bio_params$R0 * 2))
  start_params$ln_ycs_est = ran_start(n = sum(TMB_data$ycs_estimated), LB = -1, UB = 1)
  start_params$ln_sigma_r = log(ran_start(n = 1,LB = 0.2, UB = 1))
  start_params$ln_extra_survey_cv = log(0.01)
  start_params$logit_f_a50 = logit_general(ran_start(n = 2, LB = 3, UB = 8),TMB_data$sel_a50_bounds[1], TMB_data$sel_a50_bounds[2])
  start_params$logit_f_ato95 = logit_general(ran_start(n = 2, LB = 3, UB = 8), TMB_data$sel_ato95_bounds[1], TMB_data$sel_ato95_bounds[2])
  start_params$logit_survey_a50 = logit_general(ran_start(n = 2, LB = 3 , UB = 8),TMB_data$sel_a50_bounds[1], TMB_data$sel_a50_bounds[2])
  start_params$logit_survey_ato95 = logit_general(ran_start(n = 2, LB = 3, UB = 8), TMB_data$sel_ato95_bounds[1], TMB_data$sel_ato95_bounds[2])
  
  start_params$logit_survey_alpha_f = logit_general(ran_start(n = 1,LB = 0.8, UB = 1.2),TMB_data$sel_alpha_bounds[1], TMB_data$sel_alpha_bounds[2])
  start_params$logit_f_alpha_f = logit_general(ran_start(n = 1,LB = 0.8, UB = 1.2),TMB_data$sel_alpha_bounds[1], TMB_data$sel_alpha_bounds[2])
  
  start_params$logit_surveyQ = qlogis(ran_start(n = 1, LB = 0.04, UB = 0.3))
  start_params$ln_F = log(ran_start(n = TMB_data$nyears, LB = 0.02, UB = 0.4))
  start_params$logit_proportion_male = rep(0, TMB_data$nyears)

  start_params$ln_catch_sd = log(0.02)
  return(start_params)
}
##################################
### Build TMB OM with Multinomial
##################################
#dyn.unload(dynlib(file.path(DIR$book,"TMB","SexDisaggregatedAgeStructuredModel")))
compile(file.path("TMB","SexDisaggregatedAgeStructuredModel.cpp"), flags = "-Wignored-attributes -O3",DLLFLAGS="");
dyn.load(dynlib(file.path("TMB","SexDisaggregatedAgeStructuredModel")))
## tolerance form model convergence, all gradients need to be less than this.
grad_tol = 0.001
# these parameters we are not estimating.
na_map = fix_pars(par_list = true_pars, pars_to_exclude = c("ln_catch_sd","ln_extra_survey_cv","ln_sigma_r", "logit_proportion_male"))
na_map_fix_alphas = fix_pars(par_list = true_pars, pars_to_exclude = c("logit_survey_alpha_f","logit_f_alpha_f","ln_catch_sd","ln_extra_survey_cv","ln_sigma_r", "logit_proportion_male"))
ASM_obj <- MakeADFun(TMB_data, true_pars, DLL= "SexDisaggregatedAgeStructuredModel", map = na_map,checkParameterOrder=T)

#invlogit_general(true_pars$logit_survey_alpha_f, TMB_data$sel_alpha_bounds[1], TMB_data$sel_alpha_bounds[2])

TMB_data_alt = TMB_data
TMB_data_alt$survey_AF_type = 1
TMB_data_alt$fishery_AF_type = 1
ASM_obj_alt_sex <- MakeADFun(TMB_data_alt, true_pars, DLL= "SexDisaggregatedAgeStructuredModel", map = na_map,checkParameterOrder=T)
true_report = ASM_obj$report()
##################################
### Generate predictions from both methods to compare
##################################
sim_data = ASM_obj$simulate(complete = T)
sim_data_alt = ASM_obj_alt_sex$simulate(complete = T)
```




```{r compilesexplot, warning=F, echo = F, eval = T, fig.cap="Examples of the two approaches for the survey (top row) and fishery (bottom row). The black and red line in approach 1 will sum to one, where as the black line and red line in approach will each sum to one. Approach 2 has an extra observation which is the proportion male (not shown)."}
par(mfrow = c(2,2))
plot(survey_ages, sim_data_alt$survey_comp_fitted[1:n_ages,9], type = "l", lwd = 3, ylab = "Survey predicted values", main = "Approach 1", xlab = "", ylim = c(0,0.2))
lines(survey_ages, sim_data_alt$survey_comp_fitted[(n_ages + 1):(n_ages*2),9], type = "l", lwd = 3, lty = 2, col = "red")
plot(survey_ages, sim_data$survey_comp_fitted[1:n_ages,9], type = "l", lwd = 3, ylab = "", main = "Approach 2", xlab = "", ylim = c(0,0.2))
lines(survey_ages, sim_data$survey_comp_fitted[(n_ages + 1):(n_ages*2),9], type = "l", lwd = 3, lty = 2, col = "red")

plot(fishery_ages, sim_data_alt$fishery_comp_fitted[1:n_ages,9], type = "l", lwd = 3, ylab = "Fishery predicted values", main = "", xlab = "", ylim = c(0,0.3))
lines(fishery_ages, sim_data_alt$fishery_comp_fitted[(n_ages + 1):(n_ages*2),9], type = "l", lwd = 3, lty = 2, col = "red")

plot(fishery_ages, sim_data$fishery_comp_fitted[1:n_ages,9], type = "l", lwd = 3, ylab = "", main = "", xlab = "", ylim = c(0,0.3))
lines(fishery_ages, sim_data$fishery_comp_fitted[(n_ages + 1):(n_ages*2),9], type = "l", lwd = 3, lty = 2, col = "red")
legend('topright', legend = c("Male","Female"), col = c("black", "red"), lty = c(1,2), lwd = 3)
```


### Scenario 1  {-}
The first simulation assumed the OM had 50:50 males and female sex ratio at recruitment, and selectivities were as described in the above OM section. We simulated 100 data sets for each of the two approach's for sexually disaggregated compositional data outlined in the introduction. These were then fitted to in two EM's, where the EM's differed in the approach they used. Both EM's estimated a scalar on the female selectivity that allowed females to be more or less selected compared to males for both the fishery and survey. This was done be introducing an estimable \(\alpha\) parameter into the selectivity. Males selectivities were constrained to have a max value = 1 using,
\begin{equation}
S^{male}_a = 1/(1+19^{(a_{50}-a)/a_{to95})})
	  (\#eq:maleselectivity)

\end{equation}
where as the female was
\begin{equation}
S^{female}_a = \alpha/(1+19^{(a_{50}-a)/a_{to95})}) 
	  (\#eq:femaleselectivity)
\end{equation}


Although this scenario assumed both male and female had the same max selectivity I wanted to see the effect of estimating this additional parameter.

A third EM (EM3) was also explored, this structured compositional data using Approach 1 but fixed the female \(\alpha\) = 1 in Equation \@ref(eq:femaleselectivity). This is often the default approach. For scenario 1 this should be the best performing EM as it has the \(\alpha\) set at the values of the OM.


#### EM 1 (using approach 1) {-}
```{r run_scenario_1_EM_1, warning=F, echo = F, eval = T, cache = TRUE}
##################################
### Simulate 100 data sets for Scenario 1
##################################
est_pars_alt = NULL
ssbs_alt = NULL
convergence_alt = NULL
max_grad_alt = NULL
est_list_alt = list()
for(sim_iter in 1:n_sims) {
  #if(sim_iter %% 10 == 0)
  #  cat("iter = ", sim_iter, "\n")
  sim_data = ASM_obj_alt_sex$simulate(complete = T)
  start_pars = ran_start_vals()
  est_model_alt = MakeADFun(sim_data, start_pars, DLL= "SexDisaggregatedAgeStructuredModel", map = na_map, silent = T)
  opt_model_alt = nlminb(est_model_alt$par, est_model_alt$fn, est_model_alt$gr, control = list(iter.max = 10000, eval.max = 10000))
  ## Do two newton steps to try get parameters closer to the solution
  for(i in 1:2) {
    g = as.numeric(est_model_alt$gr(opt_model_alt$par))
    h = optimHess(opt_model_alt$par, fn = est_model_alt$fn, gr = est_model_alt$gr)
    opt_model_alt$par = opt_model_alt$par - solve(h,g)
    opt_model_alt$objective = est_model_alt$fn(opt_model_alt$par)
  }
  convergence_alt[sim_iter] = opt_model_alt$convergence
  max_grad_alt[sim_iter] = max(abs(est_model_alt$gr(est_model_alt$env$last.par.best)))
  est_pars_alt = rbind(est_pars_alt, opt_model_alt$par)
  est_rep_alt = est_model_alt$report(opt_model_alt$par)
  ssbs_alt = rbind(ssbs_alt, est_rep_alt$ssb)
  est_list_alt[[sim_iter]] = est_rep_alt
}
cat("max gradient from 100 simualtions = ", max(max_grad_alt), "\n")
R0s = get_list_obj(est_ls = est_list_alt, object_label = "R0")
sel_survey = get_list_obj(est_ls = est_list_alt, object_label = "survey_selectivity")
sel_survey_df = data.frame(sim = factor(sel_survey[,1]), male = sel_survey[,2], female = sel_survey[,3], age = ages)
sel_fishery = get_list_obj(est_ls = est_list_alt, object_label = "fishery_selectivity")
sel_fishery_df = data.frame(sim = factor(sel_fishery[,1]), male = sel_fishery[,2], female = sel_fishery[,3], age = ages)
annual_fs = get_list_obj(est_ls = est_list_alt, object_label = "annual_F")
colnames(annual_fs) = c("Sim", as.character(TMB_data$years))
molten_Fs = melt(as.data.frame(annual_fs), id.vars = "Sim", variable.name = "year", value.name = "F")
## plot selectivities
male_s_sel = ggplot(data  = sel_survey_df) +
  geom_line(aes(x = age, y = male, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, male = true_report$survey_selectivity[,1]), aes(x = age, y = male, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Survey - Male", col = "")+
  theme_bw()

female_s_sel = ggplot(data  = sel_survey_df) +
  geom_line(aes(x = age, y = female, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, female = true_report$survey_selectivity[,2]), aes(x = age, y = female, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Survey - Female", col = "")+
  theme_bw()
male_f_sel = ggplot(data  = sel_fishery_df) +
  geom_line(aes(x = age, y = male, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, male = true_report$fishery_selectivity[,1]), aes(x = age, y = male, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Fishery - Male", col = "")+
  theme_bw()

female_f_sel = ggplot(data  = sel_fishery_df) +
  geom_line(aes(x = age, y = female, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, female = true_report$fishery_selectivity[,2]), aes(x = age, y = female, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Fishery - Female", col = "")+
  theme_bw()

grid.arrange(grobs = list(male_s_sel, female_s_sel, male_f_sel, female_f_sel), ncol= 2)
```

```{r runscenario1EM1ssb, warning=F, echo = F, eval = T, fig.cap="Absolute SSBs, red line OM (truth), black lines are the estimated values form EMs"}
## plpt SSBs
molten_ssbs = melt(ssbs_alt)
colnames(molten_ssbs) = c("sim", "year", "value")
ggplot() +
  geom_line(data =molten_ssbs, aes(x = year, y = value, group = sim)) +
  geom_line(data = data.frame(year = 1:31, value = true_report$ssb),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "SSB (t)") +
  theme_bw()
```

```{r runscenario1EM1Fs, warning=F, echo = F, eval = T, fig.cap="Annual fishing mortality, red line OM (truth), black lines are the estimated values form EMs"}
## plpt SSBs
ggplot() +
  geom_line(data = molten_Fs, aes(x = as.numeric(as.character(year)), y = F, group = Sim)) +
  geom_line(data = data.frame(year = TMB_data$years, value = true_report$annual_F),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "F") +
  theme_bw()
```

#### EM 2 (using approach 2) {-}
```{r run_scenario_1_EM_2, warning=F, echo = F, eval = T, cache = TRUE}
##################################
### Simulate 100 data sets for Scenario 1
##################################
est_pars = NULL
ssbs = NULL
convergence = NULL
max_grad = NULL
est_list = list()
for(sim_iter in 1:n_sims) {
  #if(sim_iter %% 10 == 0)
  #  cat("iter = ", sim_iter, "\n")
  sim_data = ASM_obj$simulate(complete = T)
  start_pars = ran_start_vals()
  est_model = MakeADFun(sim_data, start_pars, DLL= "SexDisaggregatedAgeStructuredModel", map = na_map, silent = T)
  opt_modelc = nlminb(est_model$par, est_model$fn, est_model$gr, control = list(iter.max = 10000, eval.max = 10000))
  ## Do two newton steps to try get parameters closer to the solution
  for(i in 1:2) {
    g = as.numeric(est_model$gr(opt_modelc$par))
    h = optimHess(opt_modelc$par, fn = est_model$fn, gr = est_model$gr)
    opt_modelc$par = opt_modelc$par - solve(h,g)
    opt_modelc$objective = est_model$fn(opt_modelc$par)
  }
  convergence[sim_iter] = opt_modelc$convergence
  max_grad[sim_iter] = max(abs(est_model$gr(est_model$env$last.par.best)))
  est_pars = rbind(est_pars, opt_modelc$par)
  est_rep = est_model$report(opt_modelc$par)
  ssbs = rbind(ssbs, est_rep$ssb)
  est_list[[sim_iter]] = est_rep
}
cat("max gradient from 100 simualtions = ", max(max_grad), "\n")
R0s = get_list_obj(est_ls = est_list, object_label = "R0")
sel_survey = get_list_obj(est_ls = est_list, object_label = "survey_selectivity")
sel_survey_df = data.frame(sim = factor(sel_survey[,1]), male = sel_survey[,2], female = sel_survey[,3], age = ages)
sel_fishery = get_list_obj(est_ls = est_list, object_label = "fishery_selectivity")
sel_fishery_df = data.frame(sim = factor(sel_fishery[,1]), male = sel_fishery[,2], female = sel_fishery[,3], age = ages)
annual_fs = get_list_obj(est_ls = est_list, object_label = "annual_F")
colnames(annual_fs) = c("Sim", as.character(TMB_data$years))
molten_Fs = melt(as.data.frame(annual_fs), id.vars = "Sim", variable.name = "year", value.name = "F")

## plot selectivities
male_s_sel = ggplot(data  = sel_survey_df) +
  geom_line(aes(x = age, y = male, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, male = true_report$survey_selectivity[,1]), aes(x = age, y = male, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Survey - Male", col = "")+
  theme_bw()

female_s_sel = ggplot(data  = sel_survey_df) +
  geom_line(aes(x = age, y = female, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, female = true_report$survey_selectivity[,2]), aes(x = age, y = female, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Survey - Female", col = "")+
  theme_bw()
male_f_sel = ggplot(data  = sel_fishery_df) +
  geom_line(aes(x = age, y = male, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, male = true_report$fishery_selectivity[,1]), aes(x = age, y = male, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Fishery - Male", col = "")+
  theme_bw()

female_f_sel = ggplot(data  = sel_fishery_df) +
  geom_line(aes(x = age, y = female, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, female = true_report$fishery_selectivity[,2]), aes(x = age, y = female, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Fishery - Female", col = "")+
  theme_bw()

grid.arrange(grobs = list(male_s_sel, female_s_sel, male_f_sel, female_f_sel), ncol= 2)
```

```{r runscenario1EM2ssb, warning=F, echo = F, eval = T, fig.cap="Absolute SSBs, red line OM (truth), black lines are the estimated values form EMs"}
## plpt SSBs
molten_ssbs = melt(ssbs)
colnames(molten_ssbs) = c("sim", "year", "value")
ggplot() +
  geom_line(data =molten_ssbs, aes(x = year, y = value, group = sim)) +
  geom_line(data = data.frame(year = 1:31, value = true_report$ssb),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "SSB (t)") +
  theme_bw()
```

```{r runscenario1EM2Fs, warning=F, echo = F, eval = T, fig.cap="Annual fishing mortality, red line OM (truth), black lines are the estimated values form EMs"}
## plpt SSBs
ggplot() +
  geom_line(data = molten_Fs, aes(x = as.numeric(as.character(year)), y = F, group = Sim)) +
  geom_line(data = data.frame(year = TMB_data$years, value = true_report$annual_F),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "F") +
  theme_bw()
```


#### EM 3 (using approach 1 with female fixed at \(\alpha = 1\)) {-}
```{r run_scenario_1_EM_3, warning=F, echo = F, eval = T, cache = TRUE}
##################################
### Simulate 100 data sets for Scenario 1
##################################
est_pars = NULL
ssbs = NULL
convergence = NULL
max_grad = NULL
est_list = list()
for(sim_iter in 1:n_sims) {
  #if(sim_iter %% 10 == 0)
  #  cat("iter = ", sim_iter, "\n")
  sim_data = ASM_obj_alt_sex$simulate(complete = T)
  start_pars = ran_start_vals()
  start_pars$logit_survey_alpha_f = logit_general(1, TMB_data$sel_alpha_bounds[1], TMB_data$sel_alpha_bounds[2])
  start_pars$logit_f_alpha_f = logit_general(1, TMB_data$sel_alpha_bounds[1], TMB_data$sel_alpha_bounds[2])

  est_model = MakeADFun(sim_data, start_pars, DLL= "SexDisaggregatedAgeStructuredModel", map = na_map_fix_alphas, silent = T)
  opt_modelc = nlminb(est_model$par, est_model$fn, est_model$gr, control = list(iter.max = 10000, eval.max = 10000))
  ## Do two newton steps to try get parameters closer to the solution
  for(i in 1:2) {
    g = as.numeric(est_model$gr(opt_modelc$par))
    h = optimHess(opt_modelc$par, fn = est_model$fn, gr = est_model$gr)
    opt_modelc$par = opt_modelc$par - solve(h,g)
    opt_modelc$objective = est_model$fn(opt_modelc$par)
  }
  convergence[sim_iter] = opt_modelc$convergence
  max_grad[sim_iter] = max(abs(est_model$gr(est_model$env$last.par.best)))
  est_pars = rbind(est_pars, opt_modelc$par)
  est_rep = est_model$report(opt_modelc$par)
  ssbs = rbind(ssbs, est_rep$ssb)
  est_list[[sim_iter]] = est_rep
}
cat("max gradient from 100 simualtions = ", max(max_grad), "\n")
R0s = get_list_obj(est_ls = est_list, object_label = "R0")
sel_survey = get_list_obj(est_ls = est_list, object_label = "survey_selectivity")
sel_survey_df = data.frame(sim = factor(sel_survey[,1]), male = sel_survey[,2], female = sel_survey[,3], age = ages)
sel_fishery = get_list_obj(est_ls = est_list, object_label = "fishery_selectivity")
sel_fishery_df = data.frame(sim = factor(sel_fishery[,1]), male = sel_fishery[,2], female = sel_fishery[,3], age = ages)
annual_fs = get_list_obj(est_ls = est_list, object_label = "annual_F")
colnames(annual_fs) = c("Sim", as.character(TMB_data$years))
molten_Fs = melt(as.data.frame(annual_fs), id.vars = "Sim", variable.name = "year", value.name = "F")

## plot selectivities
male_s_sel = ggplot(data  = sel_survey_df) +
  geom_line(aes(x = age, y = male, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, male = true_report$survey_selectivity[,1]), aes(x = age, y = male, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Survey - Male", col = "")+
  theme_bw()

female_s_sel = ggplot(data  = sel_survey_df) +
  geom_line(aes(x = age, y = female, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, female = true_report$survey_selectivity[,2]), aes(x = age, y = female, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Survey - Female", col = "")+
  theme_bw()
male_f_sel = ggplot(data  = sel_fishery_df) +
  geom_line(aes(x = age, y = male, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, male = true_report$fishery_selectivity[,1]), aes(x = age, y = male, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Fishery - Male", col = "")+
  theme_bw()

female_f_sel = ggplot(data  = sel_fishery_df) +
  geom_line(aes(x = age, y = female, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, female = true_report$fishery_selectivity[,2]), aes(x = age, y = female, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Fishery - Female", col = "")+
  theme_bw()

grid.arrange(grobs = list(male_s_sel, female_s_sel, male_f_sel, female_f_sel), ncol= 2)
```

```{r runscenario1EM3ssb, warning=F, echo = F, eval = T, fig.cap="Absolute SSBs, red line OM (truth), black lines are the estimated values form EMs"}
## plpt SSBs
molten_ssbs = melt(ssbs)
colnames(molten_ssbs) = c("sim", "year", "value")
ggplot() +
  geom_line(data =molten_ssbs, aes(x = year, y = value, group = sim)) +
  geom_line(data = data.frame(year = 1:31, value = true_report$ssb),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "SSB (t)") +
  theme_bw()
```


```{r runscenario1EM3Fs, warning=F, echo = F, eval = T, fig.cap="Annual fishing mortality, red line OM (truth), black lines are the estimated values form EMs"}
## plpt SSBs
ggplot() +
  geom_line(data = molten_Fs, aes(x = as.numeric(as.character(year)), y = F, group = Sim)) +
  geom_line(data = data.frame(year = TMB_data$years, value = true_report$annual_F),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "F") +
  theme_bw()
```



### Scenario 2  {-}
The second scenario assumed sex ratio was 50:50 at recruitment and females were more selective than males to the fishery. This was done by setting \(\alpha\) = 1.2 in Equation \@ref(eq:femaleselectivity). 

```{r ChangeOMSceneario2, warning=F, echo = T, eval = T}
true_pars$logit_f_alpha_f = logit_general(1.2, TMB_data$sel_alpha_bounds[1], TMB_data$sel_alpha_bounds[2])
```

```{r RecompileTMBForScenario2, echo = F, eval = T, results = 'hide', warning=FALSE, message=FALSE, error = FALSE}
ASM_obj <- MakeADFun(TMB_data, true_pars, DLL= "SexDisaggregatedAgeStructuredModel", map = na_map_fix_alphas, checkParameterOrder=T)
ASM_obj_alt_sex <- MakeADFun(TMB_data_alt, true_pars, DLL= "SexDisaggregatedAgeStructuredModel", map = na_map_fix_alphas, checkParameterOrder=T)
true_report = ASM_obj$report()
```

#### EM 1 (using approach 1) {-}
```{r run_scenario_2_EM_1, warning=F, echo = F, eval = T, cache = TRUE}
##################################
### Simulate 100 data sets for Scenario 1
##################################
est_pars_alt = NULL
ssbs_alt = NULL
convergence_alt = NULL
max_grad_alt = NULL
est_list_alt = list()
for(sim_iter in 1:n_sims) {
  #if(sim_iter %% 10 == 0)
  #  cat("iter = ", sim_iter, "\n")
  sim_data = ASM_obj_alt_sex$simulate(complete = T)
  start_pars = ran_start_vals()
  est_model_alt = MakeADFun(sim_data, start_pars, DLL= "SexDisaggregatedAgeStructuredModel", map = na_map, silent = T)
  opt_model_alt = nlminb(est_model_alt$par, est_model_alt$fn, est_model_alt$gr, control = list(iter.max = 10000, eval.max = 10000))
  ## Do two newton steps to try get parameters closer to the solution
  for(i in 1:2) {
    g = as.numeric(est_model_alt$gr(opt_model_alt$par))
    h = optimHess(opt_model_alt$par, fn = est_model_alt$fn, gr = est_model_alt$gr)
    opt_model_alt$par = opt_model_alt$par - solve(h,g)
    opt_model_alt$objective = est_model_alt$fn(opt_model_alt$par)
  }
  convergence_alt[sim_iter] = opt_model_alt$convergence
  max_grad_alt[sim_iter] = max(abs(est_model_alt$gr(est_model_alt$env$last.par.best)))
  est_pars_alt = rbind(est_pars_alt, opt_model_alt$par)
  est_rep_alt = est_model_alt$report(opt_model_alt$par)
  ssbs_alt = rbind(ssbs_alt, est_rep_alt$ssb)
  est_list_alt[[sim_iter]] = est_rep_alt
}
cat("max gradient from 100 simualtions = ", max(max_grad_alt), "\n")
R0s = get_list_obj(est_ls = est_list_alt, object_label = "R0")
sel_survey = get_list_obj(est_ls = est_list_alt, object_label = "survey_selectivity")
sel_survey_df = data.frame(sim = factor(sel_survey[,1]), male = sel_survey[,2], female = sel_survey[,3], age = ages)
sel_fishery = get_list_obj(est_ls = est_list_alt, object_label = "fishery_selectivity")
sel_fishery_df = data.frame(sim = factor(sel_fishery[,1]), male = sel_fishery[,2], female = sel_fishery[,3], age = ages)
annual_fs = get_list_obj(est_ls = est_list_alt, object_label = "annual_F")
colnames(annual_fs) = c("Sim", as.character(TMB_data$years))
molten_Fs = melt(as.data.frame(annual_fs), id.vars = "Sim", variable.name = "year", value.name = "F")

## plot selectivities
male_s_sel = ggplot(data  = sel_survey_df) +
  geom_line(aes(x = age, y = male, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, male = true_report$survey_selectivity[,1]), aes(x = age, y = male, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Survey - Male", col = "")+
  theme_bw()

female_s_sel = ggplot(data  = sel_survey_df) +
  geom_line(aes(x = age, y = female, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, female = true_report$survey_selectivity[,2]), aes(x = age, y = female, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Survey - Female", col = "")+
  theme_bw()
male_f_sel = ggplot(data  = sel_fishery_df) +
  geom_line(aes(x = age, y = male, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, male = true_report$fishery_selectivity[,1]), aes(x = age, y = male, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Fishery - Male", col = "")+
  theme_bw()

female_f_sel = ggplot(data  = sel_fishery_df) +
  geom_line(aes(x = age, y = female, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, female = true_report$fishery_selectivity[,2]), aes(x = age, y = female, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Fishery - Female", col = "")+
  theme_bw()

grid.arrange(grobs = list(male_s_sel, female_s_sel, male_f_sel, female_f_sel), ncol= 2)
```

```{r runscenario2EM1ssb, warning=F, echo = F, eval = T, fig.cap="Absolute SSBs, red line OM (truth), black lines are the estimated values form EMs"}
## plpt SSBs
molten_ssbs = melt(ssbs_alt)
colnames(molten_ssbs) = c("sim", "year", "value")
ggplot() +
  geom_line(data =molten_ssbs, aes(x = year, y = value, group = sim)) +
  geom_line(data = data.frame(year = 1:31, value = true_report$ssb),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "SSB (t)") +
  theme_bw()
```


```{r runscenario2EM1Fs, warning=F, echo = F, eval = T, fig.cap="Annual fishing mortality, red line OM (truth), black lines are the estimated values form EMs"}
## plpt SSBs
ggplot() +
  geom_line(data = molten_Fs, aes(x = as.numeric(as.character(year)), y = F, group = Sim)) +
  geom_line(data = data.frame(year = TMB_data$years, value = true_report$annual_F),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "F") +
  theme_bw()
```

#### EM 2 (using approach 2) {-}
```{r run_scenario_2_EM_2, warning=F, echo = F, eval = T, cache = TRUE}
##################################
### Simulate 100 data sets for Scenario 1
##################################
est_pars = NULL
ssbs = NULL
convergence = NULL
max_grad = NULL
est_list = list()
for(sim_iter in 1:n_sims) {
  #if(sim_iter %% 10 == 0)
  #  cat("iter = ", sim_iter, "\n")
  sim_data = ASM_obj$simulate(complete = T)
  start_pars = ran_start_vals()
  est_model = MakeADFun(sim_data, start_pars, DLL= "SexDisaggregatedAgeStructuredModel", map = na_map, silent = T)
  opt_modelc = nlminb(est_model$par, est_model$fn, est_model$gr, control = list(iter.max = 10000, eval.max = 10000))
  ## Do two newton steps to try get parameters closer to the solution
  for(i in 1:2) {
    g = as.numeric(est_model$gr(opt_modelc$par))
    h = optimHess(opt_modelc$par, fn = est_model$fn, gr = est_model$gr)
    opt_modelc$par = opt_modelc$par - solve(h,g)
    opt_modelc$objective = est_model$fn(opt_modelc$par)
  }
  convergence[sim_iter] = opt_modelc$convergence
  max_grad[sim_iter] = max(abs(est_model$gr(est_model$env$last.par.best)))
  est_pars = rbind(est_pars, opt_modelc$par)
  est_rep = est_model$report(opt_modelc$par)
  ssbs = rbind(ssbs, est_rep$ssb)
  est_list[[sim_iter]] = est_rep
}
cat("max gradient from 100 simualtions = ", max(max_grad), "\n")
R0s = get_list_obj(est_ls = est_list, object_label = "R0")
sel_survey = get_list_obj(est_ls = est_list, object_label = "survey_selectivity")
sel_survey_df = data.frame(sim = factor(sel_survey[,1]), male = sel_survey[,2], female = sel_survey[,3], age = ages)
sel_fishery = get_list_obj(est_ls = est_list, object_label = "fishery_selectivity")
sel_fishery_df = data.frame(sim = factor(sel_fishery[,1]), male = sel_fishery[,2], female = sel_fishery[,3], age = ages)
annual_fs = get_list_obj(est_ls = est_list, object_label = "annual_F")
colnames(annual_fs) = c("Sim", as.character(TMB_data$years))
molten_Fs = melt(as.data.frame(annual_fs), id.vars = "Sim", variable.name = "year", value.name = "F")

## plot selectivities
male_s_sel = ggplot(data  = sel_survey_df) +
  geom_line(aes(x = age, y = male, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, male = true_report$survey_selectivity[,1]), aes(x = age, y = male, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Survey - Male", col = "")+
  theme_bw()

female_s_sel = ggplot(data  = sel_survey_df) +
  geom_line(aes(x = age, y = female, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, female = true_report$survey_selectivity[,2]), aes(x = age, y = female, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Survey - Female", col = "")+
  theme_bw()
male_f_sel = ggplot(data  = sel_fishery_df) +
  geom_line(aes(x = age, y = male, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, male = true_report$fishery_selectivity[,1]), aes(x = age, y = male, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Fishery - Male", col = "")+
  theme_bw()

female_f_sel = ggplot(data  = sel_fishery_df) +
  geom_line(aes(x = age, y = female, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, female = true_report$fishery_selectivity[,2]), aes(x = age, y = female, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Fishery - Female", col = "")+
  theme_bw()

grid.arrange(grobs = list(male_s_sel, female_s_sel, male_f_sel, female_f_sel), ncol= 2)
```

```{r runscenario2EM2ssb, warning=F, echo = F, eval = T, fig.cap="Absolute SSBs, red line OM (truth), black lines are the estimated values form EMs"}
## plpt SSBs
molten_ssbs = melt(ssbs)
colnames(molten_ssbs) = c("sim", "year", "value")
ggplot() +
  geom_line(data =molten_ssbs, aes(x = year, y = value, group = sim)) +
  geom_line(data = data.frame(year = 1:31, value = true_report$ssb),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "SSB (t)") +
  theme_bw()
```


```{r runscenario2EM2Fs, warning=F, echo = F, eval = T, fig.cap="Annual fishing mortality, red line OM (truth), black lines are the estimated values form EMs"}
## plpt SSBs
ggplot() +
  geom_line(data = molten_Fs, aes(x = as.numeric(as.character(year)), y = F, group = Sim)) +
  geom_line(data = data.frame(year = TMB_data$years, value = true_report$annual_F),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "F") +
  theme_bw()
```
#### EM 3 (using approach 1 with female fixed at \(\alpha = 1\)) {-}
```{r run_scenario_2_EM_3, warning=F, echo = F, eval = T, cache = TRUE}
##################################
### Simulate 100 data sets for Scenario 1
##################################
est_pars = NULL
ssbs = NULL
convergence = NULL
max_grad = NULL
est_list = list()
for(sim_iter in 1:n_sims) {
  #if(sim_iter %% 10 == 0)
  #  cat("iter = ", sim_iter, "\n")
  sim_data = ASM_obj_alt_sex$simulate(complete = T)
  start_pars = ran_start_vals()
  start_pars$logit_survey_alpha_f = logit_general(1, TMB_data$sel_alpha_bounds[1], TMB_data$sel_alpha_bounds[2])
  start_pars$logit_f_alpha_f = logit_general(1, TMB_data$sel_alpha_bounds[1], TMB_data$sel_alpha_bounds[2])

  est_model = MakeADFun(sim_data, start_pars, DLL= "SexDisaggregatedAgeStructuredModel", map = na_map_fix_alphas, silent = T)
  opt_modelc = nlminb(est_model$par, est_model$fn, est_model$gr, control = list(iter.max = 10000, eval.max = 10000))
  ## Do two newton steps to try get parameters closer to the solution
  for(i in 1:2) {
    g = as.numeric(est_model$gr(opt_modelc$par))
    h = optimHess(opt_modelc$par, fn = est_model$fn, gr = est_model$gr)
    opt_modelc$par = opt_modelc$par - solve(h,g)
    opt_modelc$objective = est_model$fn(opt_modelc$par)
  }
  convergence[sim_iter] = opt_modelc$convergence
  max_grad[sim_iter] = max(abs(est_model$gr(est_model$env$last.par.best)))
  est_pars = rbind(est_pars, opt_modelc$par)
  est_rep = est_model$report(opt_modelc$par)
  ssbs = rbind(ssbs, est_rep$ssb)
  est_list[[sim_iter]] = est_rep
}
cat("max gradient from 100 simualtions = ", max(max_grad), "\n")
R0s = get_list_obj(est_ls = est_list, object_label = "R0")
sel_survey = get_list_obj(est_ls = est_list, object_label = "survey_selectivity")
sel_survey_df = data.frame(sim = factor(sel_survey[,1]), male = sel_survey[,2], female = sel_survey[,3], age = ages)
sel_fishery = get_list_obj(est_ls = est_list, object_label = "fishery_selectivity")
sel_fishery_df = data.frame(sim = factor(sel_fishery[,1]), male = sel_fishery[,2], female = sel_fishery[,3], age = ages)
annual_fs = get_list_obj(est_ls = est_list, object_label = "annual_F")
colnames(annual_fs) = c("Sim", as.character(TMB_data$years))
molten_Fs = melt(as.data.frame(annual_fs), id.vars = "Sim", variable.name = "year", value.name = "F")

## plot selectivities
male_s_sel = ggplot(data  = sel_survey_df) +
  geom_line(aes(x = age, y = male, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, male = true_report$survey_selectivity[,1]), aes(x = age, y = male, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Survey - Male", col = "")+
  theme_bw()

female_s_sel = ggplot(data  = sel_survey_df) +
  geom_line(aes(x = age, y = female, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, female = true_report$survey_selectivity[,2]), aes(x = age, y = female, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Survey - Female", col = "")+
  theme_bw()
male_f_sel = ggplot(data  = sel_fishery_df) +
  geom_line(aes(x = age, y = male, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, male = true_report$fishery_selectivity[,1]), aes(x = age, y = male, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Fishery - Male", col = "")+
  theme_bw()

female_f_sel = ggplot(data  = sel_fishery_df) +
  geom_line(aes(x = age, y = female, group = factor(sim), alpha = 0.3, col = "EMs"), linewidth = 1.3) +
  geom_line(data = data.frame(age = ages, female = true_report$fishery_selectivity[,2]), aes(x = age, y = female, col = "OM"), linewidth= 1.2, linetype = "dashed")+
  guides(alpha = "none") +
  labs(x = "Age", y = "Selectivity", title = "Fishery - Female", col = "")+
  theme_bw()

grid.arrange(grobs = list(male_s_sel, female_s_sel, male_f_sel, female_f_sel), ncol= 2)
```

```{r runscenario2EM3ssb, warning=F, echo = F, eval = T, fig.cap="Absolute SSBs, red line OM (truth), black lines are the estimated values form EMs"}
## plpt SSBs
molten_ssbs = melt(ssbs)
colnames(molten_ssbs) = c("sim", "year", "value")
ggplot() +
  geom_line(data =molten_ssbs, aes(x = year, y = value, group = sim)) +
  geom_line(data = data.frame(year = 1:31, value = true_report$ssb),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "SSB (t)") +
  theme_bw()
```


```{r runscenario2EM3Fs, warning=F, echo = F, eval = T, fig.cap="Annual fishing mortality, red line OM (truth), black lines are the estimated values form EMs"}
## plpt SSBs
ggplot() +
  geom_line(data = molten_Fs, aes(x = as.numeric(as.character(year)), y = F, group = Sim)) +
  geom_line(data = data.frame(year = TMB_data$years, value = true_report$annual_F),aes(x = year, y = value), col = "red", linetype = "dashed", linewidth = 1.2) +
  ylim(0,NA) +
  labs(x = "Time", y = "F") +
  theme_bw()
```




<!--chapter:end:17-DealWithSexRatios.Rmd-->

# Fishing mortality approaches {#Fexplore}

The current Alaskan sablefish stock assessment (Chapter \@ref(modeldescription)) estimates annual fishing mortality values for each gear \(g\) denoted by \(F^g_{y}\). This parametersation poses to potential problems when considering future assessment models and spatial models. The first is, the number of parameters will increase as the number of gears increase. The fishery is currently going through a transformation whereby there is a switch from longline to pots. The second consideration is how to set this up in a spatially explicit model where catch have an added spatial dimension. There are two alternative approaches to the current approach which treat \(F\) as a derived quantity rather than an estimable parameter. The first is to use Newton Raphson method to solve for \(F^g_{y}\). This is the recommended approach in Stock Synthesis [@methot2013stock], termed the "hybrid" approach. The previous two methods assume the Baranov catch equation for mortality [@baranov1918question]. An alternative is to assume Popes discrete formulation [@pope1972investigation] which uses exploitation proportions (sometimes called harvest rates or fishing pressure) and has a closed form solution.

A good general overview on these methods can be found in @branch2009differences. They describe and compared the continuous Baranov catch equation [@baranov1918question] with Pope's discrete formulation [@pope1972investigation]. Arguments for using the continuous case is that $M$ and $F$ occur simultaneously, also with the continuous case, $F$ allows for multiple event encounters, this is assuming a fleet has the same selectivity and availability, that a fish that escapes one net can be caught in another. In contrast, the discrete formulation only allows a fish to be caught or escape from an instantaneous event. I have tried to summarize the benefits of the continuous equation in the following list,

*  Allows the entire population to be caught (not sure this is that relevant)
*  Allows simultaneous $M$ and $F$, no need to worry about order of operations. From a coding/practical perspective this is quite attractive. Once you have an F and M you can easily derive all mid-mortality quantities. Where as using a \(U\) approach you need save the population before and after to interpolate to derive mid-mortality quantities.
*  The magnitude of $F$ effects composition data, where as in the discrete case, composition is independent of the magniture of $U$.
*  Allows for multiple catch events of an individual
*  Can fit to catch observations thus allows for uncertainty in catches. In practice the uncertainty/variance on catch is very small i.e., coeffecient of variations ranging from 0.01 to 0.1. This essentially states catch is observed with high confidence and in my opinion isn't that much different to saying catch is known exactly. Note often this high precision on observed catch is needed in order to make the \(F\)'s identifiable. This high precision also muddies the "degrees of freedom" for the model. Although the \(F\)'s look like independent and free parameters they are heavily constrained by the assumptiosn on observed catch variance.


The arguments for the discrete approximation is that there is an analytical solution for \(U\) and so is fast to calculate expected catch, where as $F$ has to be either solved numerically or estimated as a free parameter (as mentioned earlier). 
 
Chris Francis's wrote a response to this paper [@francisCommentBranch] where he argues the discrete formulation does not preclude the multiple encounters and that only the data can truly tell us which catch equation is the best one to use.

- Need to make a point about how there may be Automatic differentiation issues with the \(U\) approach. Because there is an `if(U > 0.99)` which can cause a fork in the chain rule which can equal a coding nightmare.

The relationship between $F$ (Instantaneous fishing mortality) and $U$ exploitation rate for a simple scenario (single fishery) is illustrated in the following R code.


```{r illustrate_F_vs_U_sim}
exploitation_rates = seq(0,0.8,by = 0.02)
## calculate F given a U
fishing_mortalites = -log(1 - exploitation_rates)
## back calculate U given a F
# 1 - exp(-fishing_mortalites) 
```



The objective of this simulation

- Is the method efficient i.e., no loss of speed.
- Is the method numerically stable (No NaNs during optimization), particularly under high fishing pressure




```{r illustrateFvsU, echo = F, eval = T, fig.fullwidth=T, fig.cap="Illustration of how mortality is applied to an age cohort continuously over time (y)."}
## the application throught time.
N_1 = 100
par(mfrow = c(2,2), mar = c(2,2,2,1), oma = c(3,2,2,0))
Fs = c(0.2,0.6,1,1.4)
for(i in 1:length(Fs)) {
  F_t = Fs[i]
  U_t = 1 - exp(-F_t)
  M = 0.5
  time_ = seq(0,1, by = 0.001)
  change_over_time = N_1 * exp(-(F_t+M)*time_)
  change_over_time_alt = N_1 * exp(-M*time_[time_<0.5]) 
  change_over_time_alt = c(change_over_time_alt, change_over_time_alt[length(change_over_time_alt)] * (1 - U_t))
  change_over_time_alt = c(change_over_time_alt, change_over_time_alt[length(change_over_time_alt)] * exp(-M*time_[time_ < 0.5]) )
  
  plot(1,1, type = "n", xlab = "", ylab = "", ylim = c(0,100), xlim = c(0,1), xaxt = "n", yaxt = "n", cex.main = 1.5,cex.lab = 1.5, main = substitute(paste(F[y], " = ", this_F, " M = ", M), list(this_F = F_t, M= M)))#paste0(, " = ", F_t))
  lines(time_, change_over_time, lwd = 4)
  lines(time_, change_over_time_alt, lwd = 4, col = "red")
  if (i > 2)
    axis(side = 1, tick = T, at = c(0,1), labels = c("y", "y+1"), cex.axis = 2)
  if (i == 1)
    legend('bottomleft', legend = c("F","U"), lwd = 3, col = c("black","red"), cex = 0.8)
}
mtext(side = 1, text = "Time", outer = T, line = 0.7, cex = 1.3)
mtext(side = 2, text = "N", outer = T, line = -1, cex = 1.3)
```



## Set up a simulation {-}
To explore the above described methods a simple simulation was conducted using a simple age-structured stock assessment operating model. The model assumed 15 seperate fisheries all with a common selectivity. The purpose was to check that the derived methods were reliable (provided unbiased stock quantities) and that they are computationally efficient.


```{r setupfirstsim}
bio_params = list(
  ages = 1:20,
  L_inf = 58,
  K = 0.133,
  t0 = 0,
  M = 0.15,
  a = 2.08e-9, ## tonnes
  b = 3.5,
  m_a50 = 6.3,
  m_ato95 = 1.2,
  sigma = 0.6,
  h = 0.85,
  sigma_r = 0.6,
  R0 = 8234132,
  plus_group = 1 # 0 = No, 1 = Yes
)

other_params = list(
  s_a50 = 3.6,
  s_ato95 = 2,
  s_q = 0.2,
  f_a50 = 5,
  f_ato95 = 2,
  ssb_prop_Z = 0.5,
  survey_prop_Z = 0.5,
  survey_age_error = c(0.5, 0.4),  ## sd, rho (ignored if iid)
  fishery_age_error = c(0.5, 0.4),  ## sd, rho (ignored if iid)
  survey_bio_cv = c(0.1)
)

ages = bio_params$ages
max_age = max(bio_params$ages)
n_years = 30
years = (2020 - n_years + 1):2020
n_ages = length(ages)
## annual fishing mortality
start_F = c(rlnorm(10, log(seq(from = 0.05, to = 0.2, length = 10)), 0.1), rlnorm(10, log(0.13), 0.1), rlnorm(10, log(0.07), 0.1))
recruit_devs = log(rlnorm(n_years, -0.5 * bio_params$sigma_r * bio_params$sigma_r, bio_params$sigma_r))

length_at_age = vonbert(bio_params$ages, bio_params$K, bio_params$L_inf, bio_params$t0)
fishery_ogive = logis(bio_params$ages, other_params$f_a50, other_params$f_ato95)
survey_ogive = logis(bio_params$ages, other_params$s_a50, other_params$s_ato95)
mat_age = logis(bio_params$ages, bio_params$m_a50, bio_params$m_ato95)
weight_at_age = bio_params$a * length_at_age^bio_params$b

## observation temporal frequency
survey_year_obs = years
survey_ages = 1:20
fishery_year_obs = years
fishery_ages = 1:20
```

```{r poptmbobjects, echo = F, eval = T, results = 'hide', warning=FALSE, message=FALSE, error = FALSE}
############
## Build a multinomial model to double check estimability of all parameters
## In this case we have 'good' data, annual data, no ageing error.
##  survey index cv = 0.05
##  year effective sample size = 1000
############
TMB_data = list()
TMB_data$ages = ages
TMB_data$maxAgePlusGroup = bio_params$plus_group
TMB_data$years = years
TMB_data$n_years = length(TMB_data$years)
TMB_data$n_ages = length(TMB_data$ages)
TMB_data$n_fisheries = 15
## No ageing error
TMB_data$ageing_error_matrix = matrix(0, nrow = TMB_data$n_ages, ncol = TMB_data$n_ages)
diag(TMB_data$ageing_error_matrix) = 1;

TMB_data$survey_year_indicator = as.integer(TMB_data$years %in% survey_year_obs)
TMB_data$survey_obs = rnorm(sum(TMB_data$survey_year_indicator), 100, 4)
TMB_data$survey_cv = rep(0.15,sum(TMB_data$survey_year_indicator))
TMB_data$survey_sample_time = rep(0.5,sum(TMB_data$survey_year_indicator))
TMB_data$survey_AF_obs = matrix(5, nrow = TMB_data$n_ages, ncol = sum(TMB_data$survey_year_indicator))

TMB_data$fishery_year_indicator = array(as.integer(TMB_data$years %in% fishery_year_obs), dim = c(length(fishery_year_obs), TMB_data$n_fisheries))
TMB_data$fishery_AF_obs = array(5, dim = c(TMB_data$n_ages, length(fishery_year_obs), TMB_data$n_fisheries))

TMB_data$catches = array(1000, dim = c(TMB_data$n_years, TMB_data$n_fisheries))# this will be overriden in the simulate() call
TMB_data$F_method = 0
TMB_data$F_iterations = 4
TMB_data$F_max = 3

TMB_data$catch_indicator = array(1, dim = c(TMB_data$n_years, TMB_data$n_fisheries))
TMB_data$ycs_estimated = rep(1, n_years)
TMB_data$standardise_ycs = 0;

TMB_data$catchMeanLength = TMB_data$stockMeanLength = matrix(length_at_age, byrow = F, ncol = TMB_data$n_years, nrow = TMB_data$n_ages)
TMB_data$propMat = matrix(mat_age, byrow = F, ncol = TMB_data$n_years, nrow = TMB_data$n_ages)
TMB_data$natMor = bio_params$M
TMB_data$steepness = bio_params$h
TMB_data$stockRecruitmentModelCode = 2 ## BH
TMB_data$propZ_ssb = rep(other_params$ssb_prop_Z, TMB_data$n_years)
TMB_data$propZ_survey = rep(other_params$survey_prop_Z, TMB_data$n_years)
TMB_data$sel_ato95_bounds = c(0.1,20)
TMB_data$sel_a50_bounds = c(0.1,20)
TMB_data$mean_weight_a = bio_params$a
TMB_data$mean_weight_b = bio_params$b


## fishery_probs
fishery_probs = rlnorm(TMB_data$n_fisheries, meanlog = log(1), 0.6)
fishery_probs = fishery_probs / sum(fishery_probs)
prob_F = rmultinom(n = TMB_data$n_years, size = 500, prob = fishery_probs)
prob_F = (sweep(prob_F, MARGIN = 2, FUN = "/", STATS = colSums(prob_F)))
F_by_fishery = sweep(prob_F, MARGIN = 2, FUN = "*", STATS = start_F)

## The same parameters as OM, to check for consistency
true_pars = list(
  ln_R0 = log(bio_params$R0),
  ln_ycs_est =  log(exp(recruit_devs[TMB_data$ycs_estimated == 1] - 0.5*bio_params$sigma_r^2)),
  ln_sigma_r = log( bio_params$sigma_r),
  ln_extra_survey_cv = log(0.0001),
  logit_f_a50 = logit_general(rep(other_params$f_a50, TMB_data$n_fisheries), TMB_data$sel_a50_bounds[1], TMB_data$sel_a50_bounds[2]),
  logit_f_ato95 = logit_general(rep(other_params$f_ato95, TMB_data$n_fisheries), TMB_data$sel_ato95_bounds[1], TMB_data$sel_ato95_bounds[2]),
  logit_survey_a50 = logit_general(other_params$s_a50, TMB_data$sel_a50_bounds[1], TMB_data$sel_a50_bounds[2]),
  logit_survey_ato95 = logit_general(other_params$s_ato95, TMB_data$sel_ato95_bounds[1], TMB_data$sel_ato95_bounds[2]),
  logit_surveyQ = qlogis(other_params$s_q),
  ln_F = array(log(F_by_fishery), dim = c(TMB_data$n_fisheries,TMB_data$n_years)),
  ln_catch_sd = log(0.02)
)

ran_start_vals = function() {
  start_params = list()
  start_params$ln_R0 = ran_start(n = 1, LB = log(bio_params$R0 * 0.2), UB = log(bio_params$R0 * 2))
  start_params$ln_ycs_est = ran_start(n = sum(TMB_data$ycs_estimated), LB = -1, UB = 1)
  start_params$ln_sigma_r = log(ran_start(n = 1,LB = 0.2, UB = 1))
  start_params$ln_extra_survey_cv = log(0.01)
  start_params$logit_f_a50 = logit_general(rep(ran_start(n = 1, LB = 3, UB = 8), TMB_data$n_fisheries),TMB_data$sel_a50_bounds[1], TMB_data$sel_a50_bounds[2])
  start_params$logit_f_ato95 = logit_general(rep(ran_start(n = 1, LB = 3, UB = 8), TMB_data$n_fisheries), TMB_data$sel_ato95_bounds[1], TMB_data$sel_ato95_bounds[2])
  start_params$logit_survey_a50 = logit_general(ran_start(n = 1, LB = 3 , UB = 8),TMB_data$sel_a50_bounds[1], TMB_data$sel_a50_bounds[2])
  start_params$logit_survey_ato95 = logit_general(ran_start(n = 1, LB = 3, UB = 8), TMB_data$sel_ato95_bounds[1], TMB_data$sel_ato95_bounds[2])
  start_params$logit_surveyQ = qlogis(ran_start(n = 1, LB = 0.01, UB = 0.3))
  start_params$ln_F = array(log(ran_start(n = TMB_data$n_years * TMB_data$n_fisheries, LB = 0.02, UB = 0.4)), dim = c(TMB_data$n_fisheries,TMB_data$n_years))
  start_params$ln_catch_sd = log(0.02)
  return(start_params)
}
##################################
### Build TMB OM with Multinomial
##################################
#dyn.unload(dynlib(file.path("TMB","SimpleAgestructuredModelMultiFs")))
compile(file.path("TMB","SimpleAgestructuredModelMultiFs.cpp"), flags = "-Wignored-attributes -O3",DLLFLAGS="");
dyn.load(dynlib(file.path("TMB","SimpleAgestructuredModelMultiFs")))
## tolerance form model convergence, all gradients need to be less than this.
grad_tol = 0.001
# these parameters we are not estimating.
na_map = fix_pars(par_list = true_pars, pars_to_exclude = c("ln_catch_sd", "ln_extra_survey_cv","ln_sigma_r", "logit_f_a50", "logit_f_ato95"),
                  vec_elements_to_exclude = list(logit_f_a50 = 2:length(true_pars$logit_f_a50), logit_f_ato95 = 2:length(true_pars$logit_f_ato95)))
na_map = set_pars_to_be_the_same(par_list = true_pars, map = na_map,
                                 base_parameters = append(rep(list(logit_f_a50 = c(1)), TMB_data$n_fisheries - 1),rep(list(logit_f_ato95 = c(1)), TMB_data$n_fisheries - 1)),
                                 copy_parameters = list(
                                   logit_f_a50 = 2,
                                   logit_f_a50 = 3,
                                   logit_f_a50 = 4,
                                   logit_f_a50 = 5,
                                   logit_f_a50 = 6,
                                   logit_f_a50 = 7,
                                   logit_f_a50 = 8,
                                   logit_f_a50 = 9,
                                   logit_f_a50 = 10,
                                   logit_f_a50 = 11,
                                   logit_f_a50 = 12,
                                   logit_f_a50 = 13,
                                   logit_f_a50 = 14,
                                   logit_f_a50 = 15,
                                   logit_f_ato95 = 2,
                                   logit_f_ato95 = 3,
                                   logit_f_ato95 = 4,
                                   logit_f_ato95 = 5,
                                   logit_f_ato95 = 6,
                                   logit_f_ato95 = 7,
                                   logit_f_ato95 = 8,
                                   logit_f_ato95 = 9,
                                   logit_f_ato95 = 10,
                                   logit_f_ato95 = 11,
                                   logit_f_ato95 = 12,
                                   logit_f_ato95 = 13,
                                   logit_f_ato95 = 14,
                                   logit_f_ato95 = 15
                                   )
                                 )
na_map$logit_f_a50
na_map$logit_f_ato95


na_map_hybrid = fix_pars(par_list = true_pars, pars_to_exclude = c("ln_catch_sd", "ln_extra_survey_cv","ln_sigma_r", "ln_F", "logit_f_a50", "logit_f_ato95"),
                         vec_elements_to_exclude = list(logit_f_a50 = 2:length(true_pars$logit_f_a50), logit_f_ato95 = 2:length(true_pars$logit_f_ato95)))
na_map_hybrid = set_pars_to_be_the_same(par_list = true_pars, map = na_map_hybrid,
                                 base_parameters = append(rep(list(logit_f_a50 = c(1)), TMB_data$n_fisheries - 1),rep(list(logit_f_ato95 = c(1)), TMB_data$n_fisheries - 1)),
                                 copy_parameters = list(
                                   logit_f_a50 = 2,
                                   logit_f_a50 = 3,
                                   logit_f_a50 = 4,
                                   logit_f_a50 = 5,
                                   logit_f_a50 = 6,
                                   logit_f_a50 = 7,
                                   logit_f_a50 = 8,
                                   logit_f_a50 = 9,
                                   logit_f_a50 = 10,
                                   logit_f_a50 = 11,
                                   logit_f_a50 = 12,
                                   logit_f_a50 = 13,
                                   logit_f_a50 = 14,
                                   logit_f_a50 = 15,
                                   logit_f_ato95 = 2,
                                   logit_f_ato95 = 3,
                                   logit_f_ato95 = 4,
                                   logit_f_ato95 = 5,
                                   logit_f_ato95 = 6,
                                   logit_f_ato95 = 7,
                                   logit_f_ato95 = 8,
                                   logit_f_ato95 = 9,
                                   logit_f_ato95 = 10,
                                   logit_f_ato95 = 11,
                                   logit_f_ato95 = 12,
                                   logit_f_ato95 = 13,
                                   logit_f_ato95 = 14,
                                   logit_f_ato95 = 15
                                   )
)
ASM_obj <- MakeADFun(TMB_data, true_pars, DLL= "SimpleAgestructuredModelMultiFs", map = na_map, checkParameterOrder = T)
true_report = ASM_obj$report()
TMB_data_alt = TMB_data
TMB_data_alt$F_method = 1
ASM_obj_hybrid_F <- MakeADFun(TMB_data_alt, true_pars, DLL= "SimpleAgestructuredModelMultiFs", map = na_map, checkParameterOrder = T)
```

```{r self_test, echo = T, eval = T}
## simulate data
set.seed(123)
sim_data = ASM_obj$simulate(complete = T)

## Build AD TMB functions
start_pars = ran_start_vals()
est_model_est_F = MakeADFun(sim_data, true_pars, DLL= "SimpleAgestructuredModelMultiFs", map = na_map, silent = T)
sim_data$F_method = 1
sim_data$F_iterations = 4
est_model_hybrid_F = MakeADFun(sim_data, true_pars, DLL= "SimpleAgestructuredModelMultiFs", map = na_map_hybrid, silent = T)
## optimise
opt_model_est_F = nlminb(est_model_est_F$par, est_model_est_F$fn, est_model_est_F$gr, control = list(iter.max = 10000, eval.max = 10000))
opt_model_hybrid_F = nlminb(est_model_hybrid_F$par, est_model_hybrid_F$fn, est_model_hybrid_F$gr, control = list(iter.max = 10000, eval.max = 10000))

## look at the number of iterations used to solve
opt_model_hybrid_F$iterations
opt_model_est_F$iterations

## get reports = 
rep_est_hybrid = est_model_hybrid_F$report(est_model_hybrid_F$env$last.par.best)
rep_est_F = est_model_est_F$report(est_model_est_F$env$last.par.best)

plot(TMB_data$years, rep_est_hybrid$ssb[-1], type = "l", lwd = 3, xlab = "Year", ylab = "SSB", ylim = c(0,46000))
lines(TMB_data$years, rep_est_F$ssb[-1], type = "l", lwd = 3, lty = 2, col = "purple")
lines(TMB_data$years, sim_data$ssb[-1], type = "l", lwd = 4, lty = 3, col = "red")
legend("topright", col = c("black", "purple", "red"), legend = c("Hybrid", "Est F", "OM"), lwd = 3)
```


```{r firstbenchmarks, echo = T, eval = T}
est_F_bench = benchmark(obj = est_model_est_F, n = 1000)
hybrid_F_bench = benchmark(obj = est_model_hybrid_F, n = 1000)
est_F_bench
hybrid_F_bench
```



```{r secondbenchmarks, echo = T, eval = T}
ben_est_F <- benchmark(obj = est_model_est_F, n=20,expr=expression(do.call("optim",obj)))
ben_hybrid_F <- benchmark(obj = est_model_hybrid_F, n=20,expr=expression(do.call("optim",obj)))
ben_est_F
ben_hybrid_F
```


These results have highlighted the following

- Both the Free \(F\) and hybrid estimate very similar model quantities i.e., SSB's and F's
- The Free \(F\) method is much faster on average for a single gradient calculation and function call compared to the hybrid method
- However, the hybrid method requires less iterations due to there being less estimated parameters. For this simulation where we assumed 15 fisheries both optimised in similar amounts of time


## Appendix - Hybrid approach {-}
The hybrid fishing mortality process uses the methods and algorithms applied in Stock Synthesis [@methot2013stock]. The descriptions below are heavily based on the text describing this approach in the Appendix of [@methot2013stock].

This process begins by calculating popes discrete approximation, and then converts this to Baranov fishing mortality coefficients. A tuning algorithm is then done to tune these coefficients to match input catch nearly exactly, rather than the full Baranov approach.

Total mortality, denoted by \(Z_{a,y,s,r}\) for sex \(s\), region \(r\), age \(a\) and year \(y\) will hereby be denoted by \(Z_{a,y,s}\) i.e., drop the region index. This is because mortality rates are calculated independently (in isolation) among regions (**NOTE** consider parallelising this in the model).

\begin{equation*}
	Z_{a,y,s} = M_{a,y,s} + \sum\limits_{f} S^g_{y,s} F^g_y
\end{equation*}

where, \( M_{a,s}\) is the natural mortality rate, \( F^g_y\) is fishing mortality and \(S^g_{a,s}\) is the selectivity.

The hybrid fishing mortality method allows the \(F\) values to be "tuned" to match input catch nearly exactly, rather than estimating them as free model parameters. The process begins by calculating mid year exploitation rate using Pope’s approximation. This exploitation rate is then converted to an approximation of the Baranov continuous \(F\). The \(F\) values for all fisheries operating in that year and region are then tuned over a set number of iterations (`f_iterations`) to match the observed catch for each fishery with its corresponding \(F\). Differentiability is achieved by the use of Pope's approximation to obtain the starting value for each \(F\) and then the use of a fixed number of tuning iterations, typically 4. Tests from Stock Synthesis have shown that modelling \(F\) as hybrid versus \(F\) as a parameter has trivial impact on the estimates of the variances of other model derived quantities. 

The hybrid method calculates the harvest rate using the Pope's approximation then converts to an approximation of the corresponding F as:

\begin{align}
	V^g_{y} &= \sum\limits_s\sum\limits_a N_{a,y,s} \exp\left(-\delta_t M_{a,s}\right) \nonumber \\
	\tilde{U}^g_{y} &= \frac{C^g_{y}}{V^g_{y} + 0.1 C^g_{y}}\\
	j^g_{y} &= \left(1 + \exp \left(30 (\tilde{U}^g_{y} - 0.95) \right)\right)^{-1}\\
	U^g_{y} &= 	j^g_{y} \tilde{U}^g_{y} + 0.95 (1 - j^g_{y} )\\
	\tilde{F}^g_{y} &= \frac{-\log\left(1 - U^g_{y}\right)}{\delta_t}
\end{align}

where, \(C^{g}_y\) is the observed catch, \(\delta_t\) is the duration of the period of observation within the year. In most situations where the entire catch has been observed in a time-step. This should be one. \(V^g_{y}\) is partway vulnerable biomass and \(\tilde{F}^g_{y}\) is the initial \(F\).

The formulation above is designed so that high exploitation rates (above 0.95) are converted into an F that corresponds to a harvest rate of close to 0.95, thus providing a more robust starting point for subsequent iterative adjustment of this F. The logistic joiner, \(j\), is used at other places in Stock Synthesis to link across discontinuities.

The tuning algorithm begins by setting \(F^g_{y} = \tilde{F}^g_{y}\) and repeating the following algorithm `f_iteration` times.

\[
\widehat{C}_{a,y,s}  = \sum\limits_g {F}^g_{y}S^g_{a,s} N_{a,y,s}\bar{w}_{a,y,s} \lambda^*_{a,y,s}
\]

where, \( \lambda^*_{a,y,s}\) denotes the survivorship and is calculated as:

\begin{equation}
 \lambda^*_{a,y,s} = \frac{1 - \exp\left(-\delta_t Z_{a,y,s}  \right) }{Z_{a,y,s}}
   (\#eq:survival)
\end{equation}

Total fishing mortality is then adjusted over several fixed number of iterations (typically four, but more in high F and multiple fishery situations). The first step is to calculate the ratio of the total observed catch over all fleets to the predicted total catch according to the current F estimates. This ratio provides an overall adjustment factor to bring the total mortality closer to what it will be after adjusting the individual \(F\) values.

\[
\widehat{C}_{y}  =  \sum\limits_g \sum\limits_s\sum\limits_a {F}^g_{y}\left(S^g_{a,s} N_{a,y,s}\right) \lambda^*_{a,y,s}
\]

This is different from Equation A.1.25 in the Appendix of [@methot2013stock]. They include \(Z_{a,y,s}\) in the denominator when describing \({F}^g_{y}\), I think this is a typo error because \(Z_{a,y,s}\) is already included in the denominator when calculating \(\lambda^*_{a,y,s}\) (see Equation \@ref(eq:survival)).

\[
Z^{adj}_y = \frac{\sum\limits_g C^g_{y}}{\widehat{C}_{y}}
\]

The total mortality if this adjuster was applied to all the Fs is then calculated:

\[
Z^*_{a,y,s} = M_{a,s} + Z^{adj}_y \left(Z_{a,y,s} -M_{a,s} \right)
\]

\[
\lambda^*_{a,y,s} = \frac{1 - \exp\left(-\delta_t Z^*_{a,y,s}  \right) }{Z^*_{a,y,s}}
\]

The adjusted mortality rate is used to calculate the total removals for each fishery, and then the new \(F\) estimate is calculated by the ratio of observed catch to total removals, with a constraint to prevent unreasonably high \(F\) calculations (`max_f`):

\begin{align*}
	\tilde{V}^g_{y} &= \sum\limits_s\sum\limits_a \left(N_{a,y,s} \bar{w}_{a,y,s}S^g_{a,s} \right)\lambda^*_{a,y,s} \\
	F^{g*}_{y} &= \frac{C^g_{y}}{\tilde{V}^g_{y} + 0.0001}\\
	j^{g*}_{y} &= \left(1 + \exp \left(30 (F^{g*}_{y} - 0.95 F_{max}) \right)\right)^{-1}\\
\end{align*}

where, \(F_{max}\) is a user defined maximum fishing mortality `f_max`. The \(F\) at the end of each tuning iteration follows: 

\[
F^g_{y} = j^{g*}_{y} F^{g*}_{y} + \left(1 - j^{g*}_{y}\right)F_{max}
\]

After the tuning algorithm removals at age, and other derived quantities are recorded. The final total mortality is updated
\[
	Z_{a,y,s} = M_{a,s} + \sum\limits_{g} S^g_{a,s} F^g_{y}
\]

This process generates catch at age and sex for each year (and region) (\(\widehat{C}^g_{a,y,s}\)) which can be accessed by `process_removals` observations. Numbers at age are calculated as

\[
\widehat{C}^g_{a,y,s} = \frac{F^g_{a,s,y}}{Z_{a,y,s}} N_{a,y,s} \exp\left(-Z_{a,y,s}\right)
\]

Total catch is the summed over all sexes and age 

\[
\widehat{C}^g_{y} = \sum\limits_s\sum\limits_a \widehat{C}^g_{a,y,s}  \bar{w}_{a,y,s}
\]

where \(\bar{w}_{a,y,s}\) is the mean weight


<!--chapter:end:18-ExploreFParameterisations.Rmd-->

# Initialising the plus group in spatial models {#spatialInit}

In single area age-structured models, the plus age group is calculated using a solution to an infinite geometric series. This solution for an age-structured model is used to derive initial (equilibrium) state of the partition. The partition consists of a vector of numbers at age for each category denoted by \(\textbf{N}\);
\[
\textbf{N} = (N_1, N_2, ... , N_{a_+})^T
\]

where \(N_{a_+}\) denotes the numbers in the plus group. The numbers at age for a single area population with no density-dependent processes is derived as,

\[
N_a = \left\{ \begin{array}{lcl}
	R_0 & \mbox{for} & a = 1
	\\ 
	R_0e^{-aM}  & \mbox{for} & 1 < a < a_+
    \\
	R_0e^{\sum_{i = a_+}^\infty-iM} & \mbox{for} & a = a_+
\end{array}\right.
\]

Initialization for ages \(1 \leq a < a_+\) is easy, all you need to do is iterate the model \(a_+ - 1\) times and the age classes will be populated with there respected initial numbers at age. However, for the plus group (\(N_{a_+}\)) the solution is needed.

Although the plus group may start at let say 30 years old, it actually represents 30 and 31 and 32 up to some biological plausible value lets say 130, but mathematically can be thought of as \(\infty\). The plus group is modeled for practical reasons and care should be given when choosing the cut-off maximum age. The plus group is an infinite geometric series that is defined by the common ratio \(r_{a_+} = M\).

\begin{align}
N_{a_+} &= R_0e^{\sum_{a = a_+}^\infty-aM}\\
        &= R_0 \frac{e^{M - a_+M}}{1 - e^{-M}}
\end{align}
```{r illustrateplusGroup}
M = 0.1
R0 = 4e6
plus_group_age = 30
N_age = R0 * exp(-M * 1:plus_group_age)
## numerically calculate it
N_plus_group =  tail(N_age, n = 1)
for(i in 1:10000) {
  N_plus_group = N_plus_group * exp(-M) + tail(N_age, n = 1) * exp(-M)
}
N_plus_group
## using the geometric series calculation
R0 * exp(-M - plus_group_age*M) / (1 - exp(-M))
```

The question becomes, how do we calculate this plus group when there is markovian movement? This code shows how the same infinite geometric series solution works for the plus group with movement. Instead of the common ratio being just it also need to account for the movement process. Let \(c^r_{a+}\) denote the accumulation of individuals into the plus group in region \(r\). This will be the result of ageing, natural mortality and movement. Then the plus group for that region can be 

\[
N_{a+, r} = N_{a+ - 1, r} \frac{1}{1 - c^r_{a+}}
\]
where, \(N_{a+ - 1, r}\) is the numbers at age for the second to last age cohort in the plus group. The following R code shows how this is equivalent to running the annual cycle for 1000 years.

```{r spatialcase}
M = 0.1
R0 = 4e6
n_regions = 3
movement_matrix = matrix(0, nrow = n_regions, ncol = n_regions);
movement_matrix[1,] = c(0.7, 0.2, 0.1)
movement_matrix[2,] = c(0.1, 0.6, 0.3)
movement_matrix[3,] = c(0.15, 0.05, 0.8)
## rows are "from" cols are "to"
plus_group_age = 30
## partition
N_age = matrix(0, nrow = n_regions, ncol = plus_group_age)
## initialise with no movement initially
N_age[1,] = R0 * exp(-M * 1:plus_group_age) ##
N_age[2,] = R0 * exp(-M * 1:plus_group_age) ## 
N_age[3,] = R0 * exp(-M * 1:plus_group_age) ##

## apply an annual cycle 10000 times to see what the "initial conditions should be"
N_next_year_age = N_age
for(i in 1:1000) {
  ## recruitment
  N_next_year_age[,1] = R0 * exp(-M)
  ## ageing and mortality
  N_next_year_age[,2:plus_group_age] = N_age[,1:(plus_group_age - 1)] * exp(-M)
  ## plus group
  N_next_year_age[,plus_group_age] = N_next_year_age[,plus_group_age] * exp(-M) + N_age[,plus_group_age] * exp(-M)
  ## movement
  N_age = t(movement_matrix) %*% N_next_year_age
}
iterative_N_age = N_age
plot(1:plus_group_age, N_age[3,], xlab = "Age", ylab= "Initial numbers", ylim = c(0,R0* 1.4), type = "l", lwd = 3)
lines(1:plus_group_age, N_age[2,], lwd = 3, col = "red", lty = 2)
lines(1:plus_group_age, N_age[1,], lwd = 3, col = "blue", lty = 2)
legend('topright', col = c("black","red","blue"), legend = c("Region 3", "Region 2", "Region 1"), lwd = 3)

```

```{r spatialcase_alt}
N_age = matrix(0, nrow = n_regions, ncol = plus_group_age)
update_N_age = N_age
for(i in 1:(plus_group_age)) {
  # recruitment
  update_N_age[,1] = R0 * exp(-M)
  # ageing and mortality
  update_N_age[,2:plus_group_age] = N_age[,1:(plus_group_age - 1)] * exp(-M)
  # plus group
  update_N_age[,plus_group_age] = update_N_age[,plus_group_age] * exp(-M) + N_age[,plus_group_age] * exp(-M)
  # movement
  N_age = t(movement_matrix) %*% update_N_age
}
## calculate one more annual cycle
# recruitment
update_N_age[,1] = R0 * exp(-M)
# ageing and mortality
update_N_age[,2:plus_group_age] = N_age[,1:(plus_group_age - 1)] * exp(-M)
# plus group
update_N_age[,plus_group_age] = update_N_age[,plus_group_age] * exp(-M) + N_age[,plus_group_age] * exp(-M)
# movement
update_N_age = t(movement_matrix) %*% update_N_age
## approximate!
c = update_N_age[,plus_group_age] / N_age[,plus_group_age] - 1
update_N_age[,plus_group_age] = N_age[,plus_group_age] * 1 / (1 - c)

iterative_N_age[,plus_group_age]
update_N_age[,plus_group_age]

plot(1:plus_group_age, iterative_N_age[3,], xlab = "Age", ylab= "Initial numbers", ylim = c(0,R0* 1.4), type = "l", lwd = 3)
lines(1:plus_group_age, update_N_age[3,], lwd = 3, col = "red", lty = 2)

lines(1:plus_group_age, iterative_N_age[2,], lwd = 3, col = "black", lty = 1)
lines(1:plus_group_age, update_N_age[2,], lwd = 3, col = "red", lty = 2)
lines(1:plus_group_age, iterative_N_age[1,], lwd = 3, col = "black", lty = 1)
lines(1:plus_group_age, update_N_age[1,], lwd = 3, col = "red", lty = 2)
legend('topright', col = c("black","red"), legend = c("Numerical", "Analytical"), lwd = 3, lty = c(1,2))

```




<!--chapter:end:19-InitialisingTheSpatialModel.Rmd-->

# Glossary of terms {#glossary}


+----------------------------+-----------------------------------------------------------------------------+
| Term                       | Description                            
+============================+=============================================================================+
| ALK                        | Age length key
+----------------------------+-----------------------------------------------------------------------------+
| OM                         | Operating model 
+----------------------------+-----------------------------------------------------------------------------+
| EM                         | Estimation model
+----------------------------+-----------------------------------------------------------------------------+
| AF                         | Age frequency
+----------------------------+-----------------------------------------------------------------------------+
| LF                         | Length frequency
+----------------------------+-----------------------------------------------------------------------------+


<!--chapter:end:20-Glossary.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:22-references.Rmd-->

